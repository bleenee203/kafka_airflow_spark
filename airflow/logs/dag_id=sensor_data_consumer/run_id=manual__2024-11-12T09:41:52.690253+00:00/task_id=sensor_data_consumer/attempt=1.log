[2024-11-12T09:41:55.399+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-11-12T09:41:55.482+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-12T09:41:52.690253+00:00 [queued]>
[2024-11-12T09:41:55.499+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-12T09:41:52.690253+00:00 [queued]>
[2024-11-12T09:41:55.500+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-11-12T09:41:55.811+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): sensor_data_consumer> on 2024-11-12 09:41:52.690253+00:00
[2024-11-12T09:41:55.827+0000] {standard_task_runner.py:64} INFO - Started process 684 to run task
[2024-11-12T09:41:55.832+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sensor_data_consumer', 'sensor_data_consumer', 'manual__2024-11-12T09:41:52.690253+00:00', '--job-id', '589', '--raw', '--subdir', 'DAGS_FOLDER/***_consumer.py', '--cfg-path', '/tmp/tmp_z5bp7qp']
[2024-11-12T09:41:55.834+0000] {standard_task_runner.py:91} INFO - Job 589: Subtask sensor_data_consumer
[2024-11-12T09:41:55.952+0000] {task_command.py:426} INFO - Running <TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-12T09:41:52.690253+00:00 [running]> on host b51985a4b8f1
[2024-11-12T09:41:56.133+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Bich Ly' AIRFLOW_CTX_DAG_ID='sensor_data_consumer' AIRFLOW_CTX_TASK_ID='sensor_data_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-11-12T09:41:52.690253+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-12T09:41:52.690253+00:00'
[2024-11-12T09:41:56.134+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-11-12T09:41:56.182+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2024-11-12T09:41:56.186+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2,org.apache.hadoop:hadoop-client:3.2.1 --name KafkaSparkHDFS /opt/***/dags/spark_streaming_job.py
[2024-11-12T09:41:58.877+0000] {spark_submit.py:495} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-12T09:41:58.991+0000] {spark_submit.py:495} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-12T09:41:58.991+0000] {spark_submit.py:495} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-12T09:41:58.994+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency
[2024-11-12T09:41:58.994+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-11-12T09:41:58.995+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client added as a dependency
[2024-11-12T09:41:58.995+0000] {spark_submit.py:495} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-3b6376a1-dd9e-4f22-a0e4-5c77877700c9;1.0
[2024-11-12T09:41:58.995+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-12T09:41:59.220+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 in central
[2024-11-12T09:41:59.307+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 in central
[2024-11-12T09:41:59.368+0000] {spark_submit.py:495} INFO - found org.apache.kafka#kafka-clients;3.3.2 in central
[2024-11-12T09:41:59.410+0000] {spark_submit.py:495} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-11-12T09:41:59.452+0000] {spark_submit.py:495} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-11-12T09:41:59.490+0000] {spark_submit.py:495} INFO - found org.slf4j#slf4j-api;2.0.6 in central
[2024-11-12T09:41:59.535+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-11-12T09:41:59.588+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-11-12T09:41:59.640+0000] {spark_submit.py:495} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-11-12T09:41:59.677+0000] {spark_submit.py:495} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-11-12T09:41:59.735+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 in central
[2024-11-12T09:41:59.796+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-11-12T09:41:59.843+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client;3.2.1 in central
[2024-11-12T09:41:59.902+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-common;3.2.1 in central
[2024-11-12T09:41:59.975+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-annotations;3.2.1 in central
[2024-11-12T09:42:00.008+0000] {spark_submit.py:495} INFO - found com.google.guava#guava;27.0-jre in central
[2024-11-12T09:42:00.041+0000] {spark_submit.py:495} INFO - found com.google.guava#failureaccess;1.0 in central
[2024-11-12T09:42:00.084+0000] {spark_submit.py:495} INFO - found com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central
[2024-11-12T09:42:00.136+0000] {spark_submit.py:495} INFO - found org.checkerframework#checker-qual;2.5.2 in central
[2024-11-12T09:42:00.181+0000] {spark_submit.py:495} INFO - found com.google.errorprone#error_prone_annotations;2.2.0 in central
[2024-11-12T09:42:00.198+0000] {spark_submit.py:495} INFO - found com.google.j2objc#j2objc-annotations;1.1 in central
[2024-11-12T09:42:00.244+0000] {spark_submit.py:495} INFO - found org.codehaus.mojo#animal-sniffer-annotations;1.17 in central
[2024-11-12T09:42:00.267+0000] {spark_submit.py:495} INFO - found commons-cli#commons-cli;1.2 in central
[2024-11-12T09:42:00.292+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-math3;3.1.1 in central
[2024-11-12T09:42:00.322+0000] {spark_submit.py:495} INFO - found org.apache.httpcomponents#httpclient;4.5.6 in central
[2024-11-12T09:42:00.372+0000] {spark_submit.py:495} INFO - found org.apache.httpcomponents#httpcore;4.4.10 in central
[2024-11-12T09:42:00.414+0000] {spark_submit.py:495} INFO - found commons-codec#commons-codec;1.11 in central
[2024-11-12T09:42:00.445+0000] {spark_submit.py:495} INFO - found commons-io#commons-io;2.5 in central
[2024-11-12T09:42:00.482+0000] {spark_submit.py:495} INFO - found commons-net#commons-net;3.6 in central
[2024-11-12T09:42:00.518+0000] {spark_submit.py:495} INFO - found commons-collections#commons-collections;3.2.2 in central
[2024-11-12T09:42:00.552+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-servlet;9.3.24.v20180605 in central
[2024-11-12T09:42:00.612+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-security;9.3.24.v20180605 in central
[2024-11-12T09:42:00.634+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-webapp;9.3.24.v20180605 in central
[2024-11-12T09:42:00.667+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-xml;9.3.24.v20180605 in central
[2024-11-12T09:42:00.707+0000] {spark_submit.py:495} INFO - found com.sun.jersey#jersey-servlet;1.19 in central
[2024-11-12T09:42:00.733+0000] {spark_submit.py:495} INFO - found log4j#log4j;1.2.17 in central
[2024-11-12T09:42:00.758+0000] {spark_submit.py:495} INFO - found commons-beanutils#commons-beanutils;1.9.3 in central
[2024-11-12T09:42:00.790+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-configuration2;2.1.1 in central
[2024-11-12T09:42:00.819+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-lang3;3.7 in central
[2024-11-12T09:42:00.844+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-text;1.4 in central
[2024-11-12T09:42:00.900+0000] {spark_submit.py:495} INFO - found org.apache.avro#avro;1.7.7 in central
[2024-11-12T09:42:00.928+0000] {spark_submit.py:495} INFO - found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
[2024-11-12T09:42:00.964+0000] {spark_submit.py:495} INFO - found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
[2024-11-12T09:42:00.998+0000] {spark_submit.py:495} INFO - found com.thoughtworks.paranamer#paranamer;2.3 in central
[2024-11-12T09:42:01.059+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-compress;1.18 in central
[2024-11-12T09:42:01.088+0000] {spark_submit.py:495} INFO - found com.google.re2j#re2j;1.1 in central
[2024-11-12T09:42:01.124+0000] {spark_submit.py:495} INFO - found com.google.protobuf#protobuf-java;2.5.0 in central
[2024-11-12T09:42:01.152+0000] {spark_submit.py:495} INFO - found com.google.code.gson#gson;2.2.4 in central
[2024-11-12T09:42:01.187+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-auth;3.2.1 in central
[2024-11-12T09:42:01.219+0000] {spark_submit.py:495} INFO - found com.nimbusds#nimbus-jose-jwt;4.41.1 in central
[2024-11-12T09:42:01.252+0000] {spark_submit.py:495} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-11-12T09:42:01.275+0000] {spark_submit.py:495} INFO - found net.minidev#json-smart;2.3 in central
[2024-11-12T09:42:01.308+0000] {spark_submit.py:495} INFO - found net.minidev#accessors-smart;1.2 in central
[2024-11-12T09:42:01.331+0000] {spark_submit.py:495} INFO - found org.ow2.asm#asm;5.0.4 in central
[2024-11-12T09:42:01.359+0000] {spark_submit.py:495} INFO - found org.apache.curator#curator-framework;2.13.0 in central
[2024-11-12T09:42:01.389+0000] {spark_submit.py:495} INFO - found org.apache.curator#curator-client;2.13.0 in central
[2024-11-12T09:42:01.418+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-simplekdc;1.0.1 in central
[2024-11-12T09:42:01.443+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-client;1.0.1 in central
[2024-11-12T09:42:01.461+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-config;1.0.1 in central
[2024-11-12T09:42:01.489+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-core;1.0.1 in central
[2024-11-12T09:42:01.516+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-pkix;1.0.1 in central
[2024-11-12T09:42:01.546+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-asn1;1.0.1 in central
[2024-11-12T09:42:01.576+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-util;1.0.1 in central
[2024-11-12T09:42:01.605+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-common;1.0.1 in central
[2024-11-12T09:42:01.625+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-crypto;1.0.1 in central
[2024-11-12T09:42:01.658+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-util;1.0.1 in central
[2024-11-12T09:42:01.695+0000] {spark_submit.py:495} INFO - found org.apache.kerby#token-provider;1.0.1 in central
[2024-11-12T09:42:01.738+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-admin;1.0.1 in central
[2024-11-12T09:42:01.773+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-server;1.0.1 in central
[2024-11-12T09:42:01.818+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-identity;1.0.1 in central
[2024-11-12T09:42:01.858+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-xdr;1.0.1 in central
[2024-11-12T09:42:01.892+0000] {spark_submit.py:495} INFO - found org.apache.curator#curator-recipes;2.13.0 in central
[2024-11-12T09:42:01.929+0000] {spark_submit.py:495} INFO - found org.apache.htrace#htrace-core4;4.1.0-incubating in central
[2024-11-12T09:42:01.967+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.core#jackson-databind;2.9.8 in central
[2024-11-12T09:42:01.996+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.core#jackson-annotations;2.9.8 in central
[2024-11-12T09:42:02.040+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.core#jackson-core;2.9.8 in central
[2024-11-12T09:42:02.070+0000] {spark_submit.py:495} INFO - found org.codehaus.woodstox#stax2-api;3.1.4 in central
[2024-11-12T09:42:02.098+0000] {spark_submit.py:495} INFO - found com.fasterxml.woodstox#woodstox-core;5.0.3 in central
[2024-11-12T09:42:02.122+0000] {spark_submit.py:495} INFO - found dnsjava#dnsjava;2.1.7 in central
[2024-11-12T09:42:02.170+0000] {spark_submit.py:495} INFO - found javax.servlet.jsp#jsp-api;2.1 in central
[2024-11-12T09:42:02.271+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-hdfs-client;3.2.1 in central
[2024-11-12T09:42:02.290+0000] {spark_submit.py:495} INFO - found com.squareup.okhttp#okhttp;2.7.5 in central
[2024-11-12T09:42:02.312+0000] {spark_submit.py:495} INFO - found com.squareup.okio#okio;1.6.0 in central
[2024-11-12T09:42:02.353+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-yarn-api;3.2.1 in central
[2024-11-12T09:42:02.380+0000] {spark_submit.py:495} INFO - found javax.xml.bind#jaxb-api;2.2.11 in central
[2024-11-12T09:42:02.428+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-yarn-client;3.2.1 in central
[2024-11-12T09:42:02.494+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-mapreduce-client-core;3.2.1 in central
[2024-11-12T09:42:02.536+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-yarn-common;3.2.1 in central
[2024-11-12T09:42:02.580+0000] {spark_submit.py:495} INFO - found javax.servlet#javax.servlet-api;3.1.0 in central
[2024-11-12T09:42:02.608+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-util;9.3.24.v20180605 in central
[2024-11-12T09:42:02.643+0000] {spark_submit.py:495} INFO - found com.sun.jersey#jersey-core;1.19 in central
[2024-11-12T09:42:02.674+0000] {spark_submit.py:495} INFO - found javax.ws.rs#jsr311-api;1.1.1 in central
[2024-11-12T09:42:02.698+0000] {spark_submit.py:495} INFO - found com.sun.jersey#jersey-client;1.19 in central
[2024-11-12T09:42:02.805+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.8 in central
[2024-11-12T09:42:02.844+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.8 in central
[2024-11-12T09:42:02.884+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.8 in central
[2024-11-12T09:42:02.934+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.1 in central
[2024-11-12T09:42:02.980+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-mapreduce-client-common;3.2.1 in central
[2024-11-12T09:42:03.207+0000] {spark_submit.py:495} INFO - :: resolution report :: resolve 4075ms :: artifacts dl 129ms
[2024-11-12T09:42:03.209+0000] {spark_submit.py:495} INFO - :: modules in use:
[2024-11-12T09:42:03.209+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.core#jackson-annotations;2.9.8 from central in [default]
[2024-11-12T09:42:03.209+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.core#jackson-core;2.9.8 from central in [default]
[2024-11-12T09:42:03.211+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.core#jackson-databind;2.9.8 from central in [default]
[2024-11-12T09:42:03.211+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.8 from central in [default]
[2024-11-12T09:42:03.213+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.8 from central in [default]
[2024-11-12T09:42:03.213+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.8 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.google.code.gson#gson;2.2.4 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.google.errorprone#error_prone_annotations;2.2.0 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.google.guava#failureaccess;1.0 from central in [default]
[2024-11-12T09:42:03.214+0000] {spark_submit.py:495} INFO - com.google.guava#guava;27.0-jre from central in [default]
[2024-11-12T09:42:03.215+0000] {spark_submit.py:495} INFO - com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]
[2024-11-12T09:42:03.215+0000] {spark_submit.py:495} INFO - com.google.j2objc#j2objc-annotations;1.1 from central in [default]
[2024-11-12T09:42:03.215+0000] {spark_submit.py:495} INFO - com.google.protobuf#protobuf-java;2.5.0 from central in [default]
[2024-11-12T09:42:03.215+0000] {spark_submit.py:495} INFO - com.google.re2j#re2j;1.1 from central in [default]
[2024-11-12T09:42:03.215+0000] {spark_submit.py:495} INFO - com.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]
[2024-11-12T09:42:03.216+0000] {spark_submit.py:495} INFO - com.squareup.okhttp#okhttp;2.7.5 from central in [default]
[2024-11-12T09:42:03.216+0000] {spark_submit.py:495} INFO - com.squareup.okio#okio;1.6.0 from central in [default]
[2024-11-12T09:42:03.216+0000] {spark_submit.py:495} INFO - com.sun.jersey#jersey-client;1.19 from central in [default]
[2024-11-12T09:42:03.216+0000] {spark_submit.py:495} INFO - com.sun.jersey#jersey-core;1.19 from central in [default]
[2024-11-12T09:42:03.217+0000] {spark_submit.py:495} INFO - com.sun.jersey#jersey-servlet;1.19 from central in [default]
[2024-11-12T09:42:03.217+0000] {spark_submit.py:495} INFO - com.thoughtworks.paranamer#paranamer;2.3 from central in [default]
[2024-11-12T09:42:03.217+0000] {spark_submit.py:495} INFO - commons-beanutils#commons-beanutils;1.9.3 from central in [default]
[2024-11-12T09:42:03.217+0000] {spark_submit.py:495} INFO - commons-cli#commons-cli;1.2 from central in [default]
[2024-11-12T09:42:03.218+0000] {spark_submit.py:495} INFO - commons-codec#commons-codec;1.11 from central in [default]
[2024-11-12T09:42:03.218+0000] {spark_submit.py:495} INFO - commons-collections#commons-collections;3.2.2 from central in [default]
[2024-11-12T09:42:03.218+0000] {spark_submit.py:495} INFO - commons-io#commons-io;2.5 from central in [default]
[2024-11-12T09:42:03.218+0000] {spark_submit.py:495} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2024-11-12T09:42:03.218+0000] {spark_submit.py:495} INFO - commons-net#commons-net;3.6 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - dnsjava#dnsjava;2.1.7 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - javax.servlet#javax.servlet-api;3.1.0 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - javax.servlet.jsp#jsp-api;2.1 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - javax.ws.rs#jsr311-api;1.1.1 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - javax.xml.bind#jaxb-api;2.2.11 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - log4j#log4j;1.2.17 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - net.minidev#accessors-smart;1.2 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - net.minidev#json-smart;2.3 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - org.apache.avro#avro;1.7.7 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-compress;1.18 from central in [default]
[2024-11-12T09:42:03.219+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-configuration2;2.1.1 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-lang3;3.7 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-math3;3.1.1 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-text;1.4 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.curator#curator-client;2.13.0 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.curator#curator-framework;2.13.0 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.curator#curator-recipes;2.13.0 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-annotations;3.2.1 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-auth;3.2.1 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client;3.2.1 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-11-12T09:42:03.220+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-common;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-hdfs-client;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-mapreduce-client-common;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-mapreduce-client-core;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-yarn-api;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-yarn-client;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-yarn-common;3.2.1 from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]
[2024-11-12T09:42:03.221+0000] {spark_submit.py:495} INFO - org.apache.httpcomponents#httpclient;4.5.6 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.httpcomponents#httpcore;4.4.10 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kafka#kafka-clients;3.3.2 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-admin;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-client;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-common;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-core;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-crypto;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-identity;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-server;1.0.1 from central in [default]
[2024-11-12T09:42:03.222+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]
[2024-11-12T09:42:03.226+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-util;1.0.1 from central in [default]
[2024-11-12T09:42:03.226+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-asn1;1.0.1 from central in [default]
[2024-11-12T09:42:03.227+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-config;1.0.1 from central in [default]
[2024-11-12T09:42:03.227+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-pkix;1.0.1 from central in [default]
[2024-11-12T09:42:03.227+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-util;1.0.1 from central in [default]
[2024-11-12T09:42:03.227+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-xdr;1.0.1 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.apache.kerby#token-provider;1.0.1 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.checkerframework#checker-qual;2.5.2 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.codehaus.woodstox#stax2-api;3.1.4 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-security;9.3.24.v20180605 from central in [default]
[2024-11-12T09:42:03.228+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-servlet;9.3.24.v20180605 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-util;9.3.24.v20180605 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-webapp;9.3.24.v20180605 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-xml;9.3.24.v20180605 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.ow2.asm#asm;5.0.4 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;2.0.6 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - :: evicted modules:
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;1.7.25 by [org.slf4j#slf4j-api;2.0.6] in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.0.5 by [org.xerial.snappy#snappy-java;1.1.10.3] in [default]
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-12T09:42:03.229+0000] {spark_submit.py:495} INFO - |                  |            modules            ||   artifacts   |
[2024-11-12T09:42:03.230+0000] {spark_submit.py:495} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-12T09:42:03.230+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-12T09:42:03.230+0000] {spark_submit.py:495} INFO - |      default     |  100  |   0   |   0   |   2   ||   98  |   0   |
[2024-11-12T09:42:03.230+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-12T09:42:03.266+0000] {spark_submit.py:495} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-3b6376a1-dd9e-4f22-a0e4-5c77877700c9
[2024-11-12T09:42:03.267+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-12T09:42:03.316+0000] {spark_submit.py:495} INFO - 0 artifacts copied, 98 already retrieved (0kB/54ms)
[2024-11-12T09:42:03.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-12T09:42:06.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SparkContext: Running Spark version 3.4.2
[2024-11-12T09:42:06.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO ResourceUtils: ==============================================================
[2024-11-12T09:42:06.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-12T09:42:06.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO ResourceUtils: ==============================================================
[2024-11-12T09:42:06.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SparkContext: Submitted application: KafkaSparkStreaming
[2024-11-12T09:42:06.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-12T09:42:06.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO ResourceProfile: Limiting resource is cpu
[2024-11-12T09:42:06.274+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-12T09:42:06.550+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SecurityManager: Changing view acls to: ***
[2024-11-12T09:42:06.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SecurityManager: Changing modify acls to: ***
[2024-11-12T09:42:06.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SecurityManager: Changing view acls groups to:
[2024-11-12T09:42:06.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SecurityManager: Changing modify acls groups to:
[2024-11-12T09:42:06.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-12T09:42:07.489+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO Utils: Successfully started service 'sparkDriver' on port 46451.
[2024-11-12T09:42:07.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO SparkEnv: Registering MapOutputTracker
[2024-11-12T09:42:07.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-12T09:42:07.815+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-12T09:42:07.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-12T09:42:07.830+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-12T09:42:07.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-45692ad3-a1f5-4269-8887-e6cdf13520c6
[2024-11-12T09:42:07.939+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-12T09:42:07.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-12T09:42:08.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-12T09:42:08.529+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-12T09:42:08.621+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://b51985a4b8f1:46451/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://b51985a4b8f1:46451/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-client-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://b51985a4b8f1:46451/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://b51985a4b8f1:46451/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://b51985a4b8f1:46451/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://b51985a4b8f1:46451/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://b51985a4b8f1:46451/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://b51985a4b8f1:46451/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://b51985a4b8f1:46451/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-common-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-common-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.guava_guava-27.0-jre.jar at spark://b51985a4b8f1:46451/jars/com.google.guava_guava-27.0-jre.jar with timestamp 1731404526068
[2024-11-12T09:42:08.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-cli_commons-cli-1.2.jar at spark://b51985a4b8f1:46451/jars/commons-cli_commons-cli-1.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.commons_commons-math3-3.1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar at spark://b51985a4b8f1:46451/jars/org.apache.httpcomponents_httpclient-4.5.6.jar with timestamp 1731404526068
[2024-11-12T09:42:08.640+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.11.jar at spark://b51985a4b8f1:46451/jars/commons-codec_commons-codec-1.11.jar with timestamp 1731404526068
[2024-11-12T09:42:08.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-io_commons-io-2.5.jar at spark://b51985a4b8f1:46451/jars/commons-io_commons-io-2.5.jar with timestamp 1731404526068
[2024-11-12T09:42:08.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-net_commons-net-3.6.jar at spark://b51985a4b8f1:46451/jars/commons-net_commons-net-3.6.jar with timestamp 1731404526068
[2024-11-12T09:42:08.645+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar at spark://b51985a4b8f1:46451/jars/commons-collections_commons-collections-3.2.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:08.651+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:08.651+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.sun.jersey_jersey-servlet-1.19.jar at spark://b51985a4b8f1:46451/jars/com.sun.jersey_jersey-servlet-1.19.jar with timestamp 1731404526068
[2024-11-12T09:42:08.652+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://b51985a4b8f1:46451/jars/log4j_log4j-1.2.17.jar with timestamp 1731404526068
[2024-11-12T09:42:08.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar at spark://b51985a4b8f1:46451/jars/commons-beanutils_commons-beanutils-1.9.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-configuration2-2.1.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.commons_commons-configuration2-2.1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.7.jar at spark://b51985a4b8f1:46451/jars/org.apache.commons_commons-lang3-3.7.jar with timestamp 1731404526068
[2024-11-12T09:42:08.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-text-1.4.jar at spark://b51985a4b8f1:46451/jars/org.apache.commons_commons-text-1.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.avro_avro-1.7.7.jar at spark://b51985a4b8f1:46451/jars/org.apache.avro_avro-1.7.7.jar with timestamp 1731404526068
[2024-11-12T09:42:08.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.re2j_re2j-1.1.jar at spark://b51985a4b8f1:46451/jars/com.google.re2j_re2j-1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar at spark://b51985a4b8f1:46451/jars/com.google.protobuf_protobuf-java-2.5.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.666+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar at spark://b51985a4b8f1:46451/jars/com.google.code.gson_gson-2.2.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.667+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.668+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.curator_curator-client-2.13.0.jar at spark://b51985a4b8f1:46451/jars/org.apache.curator_curator-client-2.13.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.668+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.curator_curator-recipes-2.13.0.jar at spark://b51985a4b8f1:46451/jars/org.apache.curator_curator-recipes-2.13.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at spark://b51985a4b8f1:46451/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1731404526068
[2024-11-12T09:42:08.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-compress-1.18.jar at spark://b51985a4b8f1:46451/jars/org.apache.commons_commons-compress-1.18.jar with timestamp 1731404526068
[2024-11-12T09:42:08.675+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.676+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:08.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar at spark://b51985a4b8f1:46451/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/dnsjava_dnsjava-2.1.7.jar at spark://b51985a4b8f1:46451/jars/dnsjava_dnsjava-2.1.7.jar with timestamp 1731404526068
[2024-11-12T09:42:08.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.guava_failureaccess-1.0.jar at spark://b51985a4b8f1:46451/jars/com.google.guava_failureaccess-1.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar at spark://b51985a4b8f1:46451/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar with timestamp 1731404526068
[2024-11-12T09:42:08.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.checkerframework_checker-qual-2.5.2.jar at spark://b51985a4b8f1:46451/jars/org.checkerframework_checker-qual-2.5.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.680+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar at spark://b51985a4b8f1:46451/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.1.jar at spark://b51985a4b8f1:46451/jars/com.google.j2objc_j2objc-annotations-1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar at spark://b51985a4b8f1:46451/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar with timestamp 1731404526068
[2024-11-12T09:42:08.682+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar at spark://b51985a4b8f1:46451/jars/org.apache.httpcomponents_httpcore-4.4.10.jar with timestamp 1731404526068
[2024-11-12T09:42:08.682+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:08.682+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:08.683+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://b51985a4b8f1:46451/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1731404526068
[2024-11-12T09:42:08.684+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://b51985a4b8f1:46451/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1731404526068
[2024-11-12T09:42:08.685+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar at spark://b51985a4b8f1:46451/jars/com.thoughtworks.paranamer_paranamer-2.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.685+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar at spark://b51985a4b8f1:46451/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/net.minidev_json-smart-2.3.jar at spark://b51985a4b8f1:46451/jars/net.minidev_json-smart-2.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.curator_curator-framework-2.13.0.jar at spark://b51985a4b8f1:46451/jars/org.apache.curator_curator-framework-2.13.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.687+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://b51985a4b8f1:46451/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/net.minidev_accessors-smart-1.2.jar at spark://b51985a4b8f1:46451/jars/net.minidev_accessors-smart-1.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.692+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.ow2.asm_asm-5.0.4.jar at spark://b51985a4b8f1:46451/jars/org.ow2.asm_asm-5.0.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.692+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-client-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-client-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-admin-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-admin-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-config-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerby-config-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-core-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-core-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-common-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-common-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-util-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-util-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.701+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_token-provider-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_token-provider-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-pkix-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerby-pkix-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-asn1-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerby-asn1-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-util-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerby-util-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-crypto-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-crypto-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-server-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-server-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-xdr-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerby-xdr-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-identity-1.0.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.kerby_kerb-identity-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar at spark://b51985a4b8f1:46451/jars/javax.servlet.jsp_jsp-api-2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.squareup.okhttp_okhttp-2.7.5.jar at spark://b51985a4b8f1:46451/jars/com.squareup.okhttp_okhttp-2.7.5.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.squareup.okio_okio-1.6.0.jar at spark://b51985a4b8f1:46451/jars/com.squareup.okio_okio-1.6.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar at spark://b51985a4b8f1:46451/jars/javax.xml.bind_jaxb-api-2.2.11.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.servlet_javax.servlet-api-3.1.0.jar at spark://b51985a4b8f1:46451/jars/javax.servlet_javax.servlet-api-3.1.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:08.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.sun.jersey_jersey-core-1.19.jar at spark://b51985a4b8f1:46451/jars/com.sun.jersey_jersey-core-1.19.jar with timestamp 1731404526068
[2024-11-12T09:42:08.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.sun.jersey_jersey-client-1.19.jar at spark://b51985a4b8f1:46451/jars/com.sun.jersey_jersey-client-1.19.jar with timestamp 1731404526068
[2024-11-12T09:42:08.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:08.705+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:08.706+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.ws.rs_jsr311-api-1.1.1.jar at spark://b51985a4b8f1:46451/jars/javax.ws.rs_jsr311-api-1.1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.706+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar at spark://b51985a4b8f1:46451/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:08.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar at spark://b51985a4b8f1:46451/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://b51985a4b8f1:46451/files/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar
[2024-11-12T09:42:08.723+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://b51985a4b8f1:46451/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.724+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar
[2024-11-12T09:42:08.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-client-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:08.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-client-3.2.1.jar
[2024-11-12T09:42:08.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://b51985a4b8f1:46451/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar
[2024-11-12T09:42:08.772+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://b51985a4b8f1:46451/files/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1731404526068
[2024-11-12T09:42:08.774+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kafka_kafka-clients-3.3.2.jar
[2024-11-12T09:42:08.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1731404526068
[2024-11-12T09:42:08.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-11-12T09:42:08.963+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://b51985a4b8f1:46451/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1731404526068
[2024-11-12T09:42:08.964+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.lz4_lz4-java-1.8.0.jar
[2024-11-12T09:42:08.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://b51985a4b8f1:46451/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1731404526068
[2024-11-12T09:42:08.987+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:08 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-11-12T09:42:09.036+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://b51985a4b8f1:46451/files/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1731404526068
[2024-11-12T09:42:09.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.slf4j_slf4j-api-2.0.6.jar
[2024-11-12T09:42:09.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1731404526068
[2024-11-12T09:42:09.051+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-11-12T09:42:09.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://b51985a4b8f1:46451/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1731404526068
[2024-11-12T09:42:09.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-logging_commons-logging-1.1.3.jar
[2024-11-12T09:42:09.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://b51985a4b8f1:46451/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1731404526068
[2024-11-12T09:42:09.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-11-12T09:42:09.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://b51985a4b8f1:46451/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.179+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.commons_commons-pool2-2.11.1.jar
[2024-11-12T09:42:09.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-common-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-common-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-common-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-common-3.2.1.jar
[2024-11-12T09:42:09.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar
[2024-11-12T09:42:09.265+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar
[2024-11-12T09:42:09.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar
[2024-11-12T09:42:09.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar
[2024-11-12T09:42:09.327+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar
[2024-11-12T09:42:09.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-annotations-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-annotations-3.2.1.jar
[2024-11-12T09:42:09.355+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.guava_guava-27.0-jre.jar at spark://b51985a4b8f1:46451/files/com.google.guava_guava-27.0-jre.jar with timestamp 1731404526068
[2024-11-12T09:42:09.356+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.guava_guava-27.0-jre.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.guava_guava-27.0-jre.jar
[2024-11-12T09:42:09.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-cli_commons-cli-1.2.jar at spark://b51985a4b8f1:46451/files/commons-cli_commons-cli-1.2.jar with timestamp 1731404526068
[2024-11-12T09:42:09.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-cli_commons-cli-1.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-cli_commons-cli-1.2.jar
[2024-11-12T09:42:09.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar at spark://b51985a4b8f1:46451/files/org.apache.commons_commons-math3-3.1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.407+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.commons_commons-math3-3.1.1.jar
[2024-11-12T09:42:09.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar at spark://b51985a4b8f1:46451/files/org.apache.httpcomponents_httpclient-4.5.6.jar with timestamp 1731404526068
[2024-11-12T09:42:09.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.httpcomponents_httpclient-4.5.6.jar
[2024-11-12T09:42:09.445+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.11.jar at spark://b51985a4b8f1:46451/files/commons-codec_commons-codec-1.11.jar with timestamp 1731404526068
[2024-11-12T09:42:09.447+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-codec_commons-codec-1.11.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-codec_commons-codec-1.11.jar
[2024-11-12T09:42:09.457+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-io_commons-io-2.5.jar at spark://b51985a4b8f1:46451/files/commons-io_commons-io-2.5.jar with timestamp 1731404526068
[2024-11-12T09:42:09.457+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-io_commons-io-2.5.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-io_commons-io-2.5.jar
[2024-11-12T09:42:09.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-net_commons-net-3.6.jar at spark://b51985a4b8f1:46451/files/commons-net_commons-net-3.6.jar with timestamp 1731404526068
[2024-11-12T09:42:09.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-net_commons-net-3.6.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-net_commons-net-3.6.jar
[2024-11-12T09:42:09.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar at spark://b51985a4b8f1:46451/files/commons-collections_commons-collections-3.2.2.jar with timestamp 1731404526068
[2024-11-12T09:42:09.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-collections_commons-collections-3.2.2.jar
[2024-11-12T09:42:09.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/files/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:09.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar
[2024-11-12T09:42:09.549+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/files/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:09.550+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar
[2024-11-12T09:42:09.561+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.sun.jersey_jersey-servlet-1.19.jar at spark://b51985a4b8f1:46451/files/com.sun.jersey_jersey-servlet-1.19.jar with timestamp 1731404526068
[2024-11-12T09:42:09.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.sun.jersey_jersey-servlet-1.19.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.sun.jersey_jersey-servlet-1.19.jar
[2024-11-12T09:42:09.586+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://b51985a4b8f1:46451/files/log4j_log4j-1.2.17.jar with timestamp 1731404526068
[2024-11-12T09:42:09.586+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/log4j_log4j-1.2.17.jar
[2024-11-12T09:42:09.597+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar at spark://b51985a4b8f1:46451/files/commons-beanutils_commons-beanutils-1.9.3.jar with timestamp 1731404526068
[2024-11-12T09:42:09.598+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/commons-beanutils_commons-beanutils-1.9.3.jar
[2024-11-12T09:42:09.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-configuration2-2.1.1.jar at spark://b51985a4b8f1:46451/files/org.apache.commons_commons-configuration2-2.1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-configuration2-2.1.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.commons_commons-configuration2-2.1.1.jar
[2024-11-12T09:42:09.645+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.7.jar at spark://b51985a4b8f1:46451/files/org.apache.commons_commons-lang3-3.7.jar with timestamp 1731404526068
[2024-11-12T09:42:09.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.7.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.commons_commons-lang3-3.7.jar
[2024-11-12T09:42:09.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-text-1.4.jar at spark://b51985a4b8f1:46451/files/org.apache.commons_commons-text-1.4.jar with timestamp 1731404526068
[2024-11-12T09:42:09.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-text-1.4.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.commons_commons-text-1.4.jar
[2024-11-12T09:42:09.682+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.avro_avro-1.7.7.jar at spark://b51985a4b8f1:46451/files/org.apache.avro_avro-1.7.7.jar with timestamp 1731404526068
[2024-11-12T09:42:09.683+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.avro_avro-1.7.7.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.avro_avro-1.7.7.jar
[2024-11-12T09:42:09.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.re2j_re2j-1.1.jar at spark://b51985a4b8f1:46451/files/com.google.re2j_re2j-1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.re2j_re2j-1.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.re2j_re2j-1.1.jar
[2024-11-12T09:42:09.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar at spark://b51985a4b8f1:46451/files/com.google.protobuf_protobuf-java-2.5.0.jar with timestamp 1731404526068
[2024-11-12T09:42:09.723+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.protobuf_protobuf-java-2.5.0.jar
[2024-11-12T09:42:09.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar at spark://b51985a4b8f1:46451/files/com.google.code.gson_gson-2.2.4.jar with timestamp 1731404526068
[2024-11-12T09:42:09.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.code.gson_gson-2.2.4.jar
[2024-11-12T09:42:09.768+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-auth-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-auth-3.2.1.jar
[2024-11-12T09:42:09.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.curator_curator-client-2.13.0.jar at spark://b51985a4b8f1:46451/files/org.apache.curator_curator-client-2.13.0.jar with timestamp 1731404526068
[2024-11-12T09:42:09.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.curator_curator-client-2.13.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.curator_curator-client-2.13.0.jar
[2024-11-12T09:42:09.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.curator_curator-recipes-2.13.0.jar at spark://b51985a4b8f1:46451/files/org.apache.curator_curator-recipes-2.13.0.jar with timestamp 1731404526068
[2024-11-12T09:42:09.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.curator_curator-recipes-2.13.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.curator_curator-recipes-2.13.0.jar
[2024-11-12T09:42:09.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at spark://b51985a4b8f1:46451/files/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1731404526068
[2024-11-12T09:42:09.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.htrace_htrace-core4-4.1.0-incubating.jar
[2024-11-12T09:42:09.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-compress-1.18.jar at spark://b51985a4b8f1:46451/files/org.apache.commons_commons-compress-1.18.jar with timestamp 1731404526068
[2024-11-12T09:42:09.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-compress-1.18.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.commons_commons-compress-1.18.jar
[2024-11-12T09:42:09.863+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-simplekdc-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:09.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-simplekdc-1.0.1.jar
[2024-11-12T09:42:09.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:09.879+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar
[2024-11-12T09:42:09.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar at spark://b51985a4b8f1:46451/files/org.codehaus.woodstox_stax2-api-3.1.4.jar with timestamp 1731404526068
[2024-11-12T09:42:09.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.codehaus.woodstox_stax2-api-3.1.4.jar
[2024-11-12T09:42:09.918+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.woodstox_woodstox-core-5.0.3.jar with timestamp 1731404526068
[2024-11-12T09:42:09.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.woodstox_woodstox-core-5.0.3.jar
[2024-11-12T09:42:09.942+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/dnsjava_dnsjava-2.1.7.jar at spark://b51985a4b8f1:46451/files/dnsjava_dnsjava-2.1.7.jar with timestamp 1731404526068
[2024-11-12T09:42:09.943+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/dnsjava_dnsjava-2.1.7.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/dnsjava_dnsjava-2.1.7.jar
[2024-11-12T09:42:09.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.guava_failureaccess-1.0.jar at spark://b51985a4b8f1:46451/files/com.google.guava_failureaccess-1.0.jar with timestamp 1731404526068
[2024-11-12T09:42:09.956+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.guava_failureaccess-1.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.guava_failureaccess-1.0.jar
[2024-11-12T09:42:09.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar at spark://b51985a4b8f1:46451/files/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar with timestamp 1731404526068
[2024-11-12T09:42:09.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar
[2024-11-12T09:42:09.987+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.checkerframework_checker-qual-2.5.2.jar at spark://b51985a4b8f1:46451/files/org.checkerframework_checker-qual-2.5.2.jar with timestamp 1731404526068
[2024-11-12T09:42:09.989+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO Utils: Copying /home/***/.ivy2/jars/org.checkerframework_checker-qual-2.5.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.checkerframework_checker-qual-2.5.2.jar
[2024-11-12T09:42:10.000+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:09 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar at spark://b51985a4b8f1:46451/files/com.google.errorprone_error_prone_annotations-2.2.0.jar with timestamp 1731404526068
[2024-11-12T09:42:10.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.errorprone_error_prone_annotations-2.2.0.jar
[2024-11-12T09:42:10.022+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.1.jar at spark://b51985a4b8f1:46451/files/com.google.j2objc_j2objc-annotations-1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.google.j2objc_j2objc-annotations-1.1.jar
[2024-11-12T09:42:10.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar at spark://b51985a4b8f1:46451/files/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar with timestamp 1731404526068
[2024-11-12T09:42:10.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar
[2024-11-12T09:42:10.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar at spark://b51985a4b8f1:46451/files/org.apache.httpcomponents_httpcore-4.4.10.jar with timestamp 1731404526068
[2024-11-12T09:42:10.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.httpcomponents_httpcore-4.4.10.jar
[2024-11-12T09:42:10.092+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/files/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:10.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar
[2024-11-12T09:42:10.118+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/files/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:10.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar
[2024-11-12T09:42:10.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://b51985a4b8f1:46451/files/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1731404526068
[2024-11-12T09:42:10.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.codehaus.jackson_jackson-core-asl-1.9.13.jar
[2024-11-12T09:42:10.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://b51985a4b8f1:46451/files/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1731404526068
[2024-11-12T09:42:10.151+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar
[2024-11-12T09:42:10.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar at spark://b51985a4b8f1:46451/files/com.thoughtworks.paranamer_paranamer-2.3.jar with timestamp 1731404526068
[2024-11-12T09:42:10.174+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.thoughtworks.paranamer_paranamer-2.3.jar
[2024-11-12T09:42:10.198+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar at spark://b51985a4b8f1:46451/files/com.nimbusds_nimbus-jose-jwt-4.41.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.198+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.nimbusds_nimbus-jose-jwt-4.41.1.jar
[2024-11-12T09:42:10.222+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/net.minidev_json-smart-2.3.jar at spark://b51985a4b8f1:46451/files/net.minidev_json-smart-2.3.jar with timestamp 1731404526068
[2024-11-12T09:42:10.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/net.minidev_json-smart-2.3.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/net.minidev_json-smart-2.3.jar
[2024-11-12T09:42:10.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.curator_curator-framework-2.13.0.jar at spark://b51985a4b8f1:46451/files/org.apache.curator_curator-framework-2.13.0.jar with timestamp 1731404526068
[2024-11-12T09:42:10.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.curator_curator-framework-2.13.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.curator_curator-framework-2.13.0.jar
[2024-11-12T09:42:10.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://b51985a4b8f1:46451/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.260+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-11-12T09:42:10.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/net.minidev_accessors-smart-1.2.jar at spark://b51985a4b8f1:46451/files/net.minidev_accessors-smart-1.2.jar with timestamp 1731404526068
[2024-11-12T09:42:10.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/net.minidev_accessors-smart-1.2.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/net.minidev_accessors-smart-1.2.jar
[2024-11-12T09:42:10.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.ow2.asm_asm-5.0.4.jar at spark://b51985a4b8f1:46451/files/org.ow2.asm_asm-5.0.4.jar with timestamp 1731404526068
[2024-11-12T09:42:10.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.ow2.asm_asm-5.0.4.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.ow2.asm_asm-5.0.4.jar
[2024-11-12T09:42:10.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-client-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-client-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-client-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-client-1.0.1.jar
[2024-11-12T09:42:10.322+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-admin-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-admin-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.323+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-admin-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-admin-1.0.1.jar
[2024-11-12T09:42:10.344+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-config-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerby-config-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.344+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-config-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerby-config-1.0.1.jar
[2024-11-12T09:42:10.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-core-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-core-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-core-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-core-1.0.1.jar
[2024-11-12T09:42:10.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-common-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-common-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-common-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-common-1.0.1.jar
[2024-11-12T09:42:10.391+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-util-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-util-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-util-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-util-1.0.1.jar
[2024-11-12T09:42:10.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_token-provider-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_token-provider-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.409+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_token-provider-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_token-provider-1.0.1.jar
[2024-11-12T09:42:10.429+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-pkix-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerby-pkix-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-pkix-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerby-pkix-1.0.1.jar
[2024-11-12T09:42:10.450+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-asn1-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerby-asn1-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-asn1-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerby-asn1-1.0.1.jar
[2024-11-12T09:42:10.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-util-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerby-util-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-util-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerby-util-1.0.1.jar
[2024-11-12T09:42:10.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-crypto-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-crypto-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-crypto-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-crypto-1.0.1.jar
[2024-11-12T09:42:10.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-server-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-server-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-server-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-server-1.0.1.jar
[2024-11-12T09:42:10.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-xdr-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerby-xdr-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-xdr-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerby-xdr-1.0.1.jar
[2024-11-12T09:42:10.576+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-identity-1.0.1.jar at spark://b51985a4b8f1:46451/files/org.apache.kerby_kerb-identity-1.0.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.577+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-identity-1.0.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.kerby_kerb-identity-1.0.1.jar
[2024-11-12T09:42:10.601+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:10.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar
[2024-11-12T09:42:10.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.jackson.core_jackson-core-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:10.619+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.jackson.core_jackson-core-2.9.8.jar
[2024-11-12T09:42:10.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar at spark://b51985a4b8f1:46451/files/javax.servlet.jsp_jsp-api-2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.655+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/javax.servlet.jsp_jsp-api-2.1.jar
[2024-11-12T09:42:10.667+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.squareup.okhttp_okhttp-2.7.5.jar at spark://b51985a4b8f1:46451/files/com.squareup.okhttp_okhttp-2.7.5.jar with timestamp 1731404526068
[2024-11-12T09:42:10.667+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.squareup.okhttp_okhttp-2.7.5.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.squareup.okhttp_okhttp-2.7.5.jar
[2024-11-12T09:42:10.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.squareup.okio_okio-1.6.0.jar at spark://b51985a4b8f1:46451/files/com.squareup.okio_okio-1.6.0.jar with timestamp 1731404526068
[2024-11-12T09:42:10.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.squareup.okio_okio-1.6.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.squareup.okio_okio-1.6.0.jar
[2024-11-12T09:42:10.697+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar at spark://b51985a4b8f1:46451/files/javax.xml.bind_jaxb-api-2.2.11.jar with timestamp 1731404526068
[2024-11-12T09:42:10.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/javax.xml.bind_jaxb-api-2.2.11.jar
[2024-11-12T09:42:10.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar
[2024-11-12T09:42:10.743+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.servlet_javax.servlet-api-3.1.0.jar at spark://b51985a4b8f1:46451/files/javax.servlet_javax.servlet-api-3.1.0.jar with timestamp 1731404526068
[2024-11-12T09:42:10.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/javax.servlet_javax.servlet-api-3.1.0.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/javax.servlet_javax.servlet-api-3.1.0.jar
[2024-11-12T09:42:10.761+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar at spark://b51985a4b8f1:46451/files/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar with timestamp 1731404526068
[2024-11-12T09:42:10.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar
[2024-11-12T09:42:10.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.sun.jersey_jersey-core-1.19.jar at spark://b51985a4b8f1:46451/files/com.sun.jersey_jersey-core-1.19.jar with timestamp 1731404526068
[2024-11-12T09:42:10.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.sun.jersey_jersey-core-1.19.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.sun.jersey_jersey-core-1.19.jar
[2024-11-12T09:42:10.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.sun.jersey_jersey-client-1.19.jar at spark://b51985a4b8f1:46451/files/com.sun.jersey_jersey-client-1.19.jar with timestamp 1731404526068
[2024-11-12T09:42:10.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.sun.jersey_jersey-client-1.19.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.sun.jersey_jersey-client-1.19.jar
[2024-11-12T09:42:10.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:10.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar
[2024-11-12T09:42:10.839+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:10.840+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar
[2024-11-12T09:42:10.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.ws.rs_jsr311-api-1.1.1.jar at spark://b51985a4b8f1:46451/files/javax.ws.rs_jsr311-api-1.1.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/javax.ws.rs_jsr311-api-1.1.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/javax.ws.rs_jsr311-api-1.1.1.jar
[2024-11-12T09:42:10.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar at spark://b51985a4b8f1:46451/files/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar with timestamp 1731404526068
[2024-11-12T09:42:10.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar
[2024-11-12T09:42:10.879+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar at spark://b51985a4b8f1:46451/files/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar with timestamp 1731404526068
[2024-11-12T09:42:10.880+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:10 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar to /tmp/spark-cbaf153f-9e00-40e3-a4e1-7d48313e5529/userFiles-2e8461a6-ab88-42aa-98bd-f463e32d5260/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar
[2024-11-12T09:42:11.260+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:11 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-12T09:42:11.576+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:11 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.2:7077 after 227 ms (0 ms spent in bootstraps)
[2024-11-12T09:42:11.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:11 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241112094211-0001
[2024-11-12T09:42:11.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241112094211-0001/0 on worker-20241112093831-172.19.0.7-41025 (172.19.0.7:41025) with 1 core(s)
[2024-11-12T09:42:12.004+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20241112094211-0001/0 on hostPort 172.19.0.7:41025 with 1 core(s), 1024.0 MiB RAM
[2024-11-12T09:42:12.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33097.
[2024-11-12T09:42:12.029+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO NettyBlockTransferService: Server created on b51985a4b8f1:33097
[2024-11-12T09:42:12.036+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-12T09:42:12.106+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b51985a4b8f1, 33097, None)
[2024-11-12T09:42:12.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO BlockManagerMasterEndpoint: Registering block manager b51985a4b8f1:33097 with 434.4 MiB RAM, BlockManagerId(driver, b51985a4b8f1, 33097, None)
[2024-11-12T09:42:12.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241112094211-0001/0 is now RUNNING
[2024-11-12T09:42:12.151+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b51985a4b8f1, 33097, None)
[2024-11-12T09:42:12.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b51985a4b8f1, 33097, None)
[2024-11-12T09:42:13.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-12T09:42:14.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-12T09:42:14.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:14 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-12T09:42:22.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:22 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.7:45302) with ID 0,  ResourceProfileId 0
[2024-11-12T09:42:22.759+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.7:42109 with 434.4 MiB RAM, BlockManagerId(0, 172.19.0.7, 42109, None)
[2024-11-12T09:42:23.974+0000] {spark_submit.py:495} INFO - done
[2024-11-12T09:42:26.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:26 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-11-12T09:42:26.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:26 INFO ResolveWriteToStream: Checkpoint root hdfs://namenode:9000/spark_checkpoint resolved to hdfs://namenode:9000/spark_checkpoint.
[2024-11-12T09:42:26.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-11-12T09:42:26.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/metadata using temp file hdfs://namenode:9000/spark_checkpoint/.metadata.0eb50588-8b83-47b8-af3b-2f0252ab0820.tmp
[2024-11-12T09:42:28.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/.metadata.0eb50588-8b83-47b8-af3b-2f0252ab0820.tmp to hdfs://namenode:9000/spark_checkpoint/metadata
[2024-11-12T09:42:28.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:28 INFO MicroBatchExecution: Starting [id = 3481c982-a298-4e8b-aacf-54ed3a142e79, runId = 96b096b9-58d5-4caf-8bcf-39e570060840]. Use hdfs://namenode:9000/spark_checkpoint to store the query checkpoint.
[2024-11-12T09:42:29.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:29 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@47826005] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@721b73e4]
[2024-11-12T09:42:29.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:29 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-12T09:42:29.182+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:29 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-12T09:42:29.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:29 INFO MicroBatchExecution: Starting new streaming query.
[2024-11-12T09:42:29.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:29 INFO MicroBatchExecution: Stream started from {}
[2024-11-12T09:42:30.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:30 INFO AdminClientConfig: AdminClientConfig values:
[2024-11-12T09:42:30.078+0000] {spark_submit.py:495} INFO - bootstrap.servers = [10.0.2.15:9092]
[2024-11-12T09:42:30.080+0000] {spark_submit.py:495} INFO - client.dns.lookup = use_all_dns_ips
[2024-11-12T09:42:30.081+0000] {spark_submit.py:495} INFO - client.id =
[2024-11-12T09:42:30.081+0000] {spark_submit.py:495} INFO - connections.max.idle.ms = 300000
[2024-11-12T09:42:30.081+0000] {spark_submit.py:495} INFO - default.api.timeout.ms = 60000
[2024-11-12T09:42:30.083+0000] {spark_submit.py:495} INFO - metadata.max.age.ms = 300000
[2024-11-12T09:42:30.083+0000] {spark_submit.py:495} INFO - metric.reporters = []
[2024-11-12T09:42:30.083+0000] {spark_submit.py:495} INFO - metrics.num.samples = 2
[2024-11-12T09:42:30.084+0000] {spark_submit.py:495} INFO - metrics.recording.level = INFO
[2024-11-12T09:42:30.084+0000] {spark_submit.py:495} INFO - metrics.sample.window.ms = 30000
[2024-11-12T09:42:30.084+0000] {spark_submit.py:495} INFO - receive.buffer.bytes = 65536
[2024-11-12T09:42:30.084+0000] {spark_submit.py:495} INFO - reconnect.backoff.max.ms = 1000
[2024-11-12T09:42:30.085+0000] {spark_submit.py:495} INFO - reconnect.backoff.ms = 50
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - request.timeout.ms = 30000
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - retries = 2147483647
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - retry.backoff.ms = 100
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - sasl.client.callback.handler.class = null
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - sasl.jaas.config = null
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - sasl.kerberos.min.time.before.relogin = 60000
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - sasl.kerberos.service.name = null
[2024-11-12T09:42:30.086+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.jitter = 0.05
[2024-11-12T09:42:30.087+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.window.factor = 0.8
[2024-11-12T09:42:30.087+0000] {spark_submit.py:495} INFO - sasl.login.callback.handler.class = null
[2024-11-12T09:42:30.088+0000] {spark_submit.py:495} INFO - sasl.login.class = null
[2024-11-12T09:42:30.088+0000] {spark_submit.py:495} INFO - sasl.login.connect.timeout.ms = null
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.read.timeout.ms = null
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.refresh.buffer.seconds = 300
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.refresh.min.period.seconds = 60
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.factor = 0.8
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.jitter = 0.05
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.max.ms = 10000
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.ms = 100
[2024-11-12T09:42:30.089+0000] {spark_submit.py:495} INFO - sasl.mechanism = GSSAPI
[2024-11-12T09:42:30.090+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.clock.skew.seconds = 30
[2024-11-12T09:42:30.090+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.audience = null
[2024-11-12T09:42:30.090+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.issuer = null
[2024-11-12T09:42:30.091+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2024-11-12T09:42:30.091+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2024-11-12T09:42:30.095+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2024-11-12T09:42:30.097+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.url = null
[2024-11-12T09:42:30.097+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.scope.claim.name = scope
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.sub.claim.name = sub
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.token.endpoint.url = null
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - security.protocol = PLAINTEXT
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - security.providers = null
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - send.buffer.bytes = 131072
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.max.ms = 30000
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.ms = 10000
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - ssl.cipher.suites = null
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - ssl.endpoint.identification.algorithm = https
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - ssl.engine.factory.class = null
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - ssl.key.password = null
[2024-11-12T09:42:30.098+0000] {spark_submit.py:495} INFO - ssl.keymanager.algorithm = SunX509
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.keystore.certificate.chain = null
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.keystore.key = null
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.keystore.location = null
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.keystore.password = null
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.keystore.type = JKS
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.protocol = TLSv1.3
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.provider = null
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.secure.random.implementation = null
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.trustmanager.algorithm = PKIX
[2024-11-12T09:42:30.099+0000] {spark_submit.py:495} INFO - ssl.truststore.certificates = null
[2024-11-12T09:42:30.100+0000] {spark_submit.py:495} INFO - ssl.truststore.location = null
[2024-11-12T09:42:30.100+0000] {spark_submit.py:495} INFO - ssl.truststore.password = null
[2024-11-12T09:42:30.100+0000] {spark_submit.py:495} INFO - ssl.truststore.type = JKS
[2024-11-12T09:42:30.100+0000] {spark_submit.py:495} INFO - 
[2024-11-12T09:42:30.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:30 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-11-12T09:42:30.273+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:30 INFO AppInfoParser: Kafka version: 3.3.2
[2024-11-12T09:42:30.273+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:30 INFO AppInfoParser: Kafka commitId: b66af662e61082cb
[2024-11-12T09:42:30.274+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:30 INFO AppInfoParser: Kafka startTimeMs: 1731404550269
[2024-11-12T09:42:30.929+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/sources/0/0 using temp file hdfs://namenode:9000/spark_checkpoint/sources/0/.0.233b7e5f-017f-4aaa-963d-daf7e207dd34.tmp
[2024-11-12T09:42:31.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/sources/0/.0.233b7e5f-017f-4aaa-963d-daf7e207dd34.tmp to hdfs://namenode:9000/spark_checkpoint/sources/0/0
[2024-11-12T09:42:31.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:31 INFO KafkaMicroBatchStream: Initial offsets: {"raw_data":{"0":566}}
[2024-11-12T09:42:31.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/0 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.0.eac99b47-04ec-4549-b7de-2c62e03ff324.tmp
[2024-11-12T09:42:31.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.0.eac99b47-04ec-4549-b7de-2c62e03ff324.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/0
[2024-11-12T09:42:31.201+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:31 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1731404551041,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:32.479+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:32.673+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:32.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:32.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:33.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:33 INFO FileStreamSinkLog: BatchIds found from listing:
[2024-11-12T09:42:33.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:34.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:34 INFO CodeGenerator: Code generated in 719.122038 ms
[2024-11-12T09:42:34.911+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:34.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:34 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:34.994+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:34 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:34.994+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:34 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:34.999+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:34 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:35.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:35.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.3 KiB, free 434.1 MiB)
[2024-11-12T09:42:35.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 96.4 KiB, free 434.0 MiB)
[2024-11-12T09:42:35.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b51985a4b8f1:33097 (size: 96.4 KiB, free: 434.3 MiB)
[2024-11-12T09:42:35.576+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:35.628+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:35.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-12T09:42:35.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 7388 bytes)
[2024-11-12T09:42:36.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.0.7:42109 (size: 96.4 KiB, free: 434.3 MiB)
[2024-11-12T09:42:42.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7105 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:42.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-12T09:42:42.847+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 7.777 s
[2024-11-12T09:42:42.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:42.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-12T09:42:42.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 7.972578 s
[2024-11-12T09:42:42.890+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO FileFormatWriter: Start to commit write Job 79c41cd0-6513-471a-b9bf-666cf8377a8f.
[2024-11-12T09:42:42.903+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
[2024-11-12T09:42:42.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/0 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.0.fd96867b-d8ad-4762-bfdb-772058726bd9.tmp
[2024-11-12T09:42:43.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.0.fd96867b-d8ad-4762-bfdb-772058726bd9.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/0
[2024-11-12T09:42:43.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO ManifestFileCommitProtocol: Committed batch 0
[2024-11-12T09:42:43.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO FileFormatWriter: Write Job 79c41cd0-6513-471a-b9bf-666cf8377a8f committed. Elapsed time: 571 ms.
[2024-11-12T09:42:43.465+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO FileFormatWriter: Finished processing stats for write job 79c41cd0-6513-471a-b9bf-666cf8377a8f.
[2024-11-12T09:42:43.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/0 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.0.2db82a1b-b632-4641-b04e-1584646e6e22.tmp
[2024-11-12T09:42:43.601+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.0.2db82a1b-b632-4641-b04e-1584646e6e22.tmp to hdfs://namenode:9000/spark_checkpoint/commits/0
[2024-11-12T09:42:43.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:43.729+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:43.729+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:43.729+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:43.729+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:29.127Z",
[2024-11-12T09:42:43.729+0000] {spark_submit.py:495} INFO - "batchId" : 0,
[2024-11-12T09:42:43.730+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:42:43.731+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:42:43.731+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:42:43.731+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:43.731+0000] {spark_submit.py:495} INFO - "addBatch" : 10702,
[2024-11-12T09:42:43.732+0000] {spark_submit.py:495} INFO - "commitOffsets" : 116,
[2024-11-12T09:42:43.732+0000] {spark_submit.py:495} INFO - "getBatch" : 265,
[2024-11-12T09:42:43.733+0000] {spark_submit.py:495} INFO - "latestOffset" : 1839,
[2024-11-12T09:42:43.733+0000] {spark_submit.py:495} INFO - "queryPlanning" : 1284,
[2024-11-12T09:42:43.733+0000] {spark_submit.py:495} INFO - "triggerExecution" : 14485,
[2024-11-12T09:42:43.734+0000] {spark_submit.py:495} INFO - "walCommit" : 153
[2024-11-12T09:42:43.734+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:43.734+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:43.734+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:43.734+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:43.735+0000] {spark_submit.py:495} INFO - "startOffset" : null,
[2024-11-12T09:42:43.735+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:43.736+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:43.736+0000] {spark_submit.py:495} INFO - "0" : 566
[2024-11-12T09:42:43.737+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:43.737+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:43.738+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:43.740+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:43.741+0000] {spark_submit.py:495} INFO - "0" : 566
[2024-11-12T09:42:43.741+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:43.741+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:43.741+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:42:43.750+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:42:43.753+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:42:43.753+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:43.753+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:43.754+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:43.754+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:43.754+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:43.754+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:43.755+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:43.755+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:43.755+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:43.755+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:43.755+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:43.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/1 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.1.930f5910-2eba-4ecd-8e21-8a1369343cf3.tmp
[2024-11-12T09:42:43.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.1.930f5910-2eba-4ecd-8e21-8a1369343cf3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/1
[2024-11-12T09:42:43.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:43 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1731404563761,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:44.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:44.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:44.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:44.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:44.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO FileStreamSinkLog: BatchIds found from listing: 0, 0
[2024-11-12T09:42:44.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:44.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:44.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:44.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:44.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:44.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:44.398+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:44.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b51985a4b8f1:33097 in memory (size: 96.4 KiB, free: 434.4 MiB)
[2024-11-12T09:42:44.429+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.19.0.7:42109 in memory (size: 96.4 KiB, free: 434.4 MiB)
[2024-11-12T09:42:44.567+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:42:44.581+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:42:44.587+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:44.589+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:44.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:44.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-11-12T09:42:44.597+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:44.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:50.537+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5942 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:50.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-11-12T09:42:50.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 6.140 s
[2024-11-12T09:42:50.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:50.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-11-12T09:42:50.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 6.179204 s
[2024-11-12T09:42:50.558+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO FileFormatWriter: Start to commit write Job 370e27a6-dd84-4f86-8ccf-7520805fad91.
[2024-11-12T09:42:50.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/1 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.1.97749fea-8c6e-4e59-90c7-23e6b724a415.tmp
[2024-11-12T09:42:50.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.1.97749fea-8c6e-4e59-90c7-23e6b724a415.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/1
[2024-11-12T09:42:50.655+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO ManifestFileCommitProtocol: Committed batch 1
[2024-11-12T09:42:50.655+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO FileFormatWriter: Write Job 370e27a6-dd84-4f86-8ccf-7520805fad91 committed. Elapsed time: 96 ms.
[2024-11-12T09:42:50.657+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO FileFormatWriter: Finished processing stats for write job 370e27a6-dd84-4f86-8ccf-7520805fad91.
[2024-11-12T09:42:50.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/1 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.1.e03e0611-6fac-4aa0-b0a3-5173515abfbf.tmp
[2024-11-12T09:42:50.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.1.e03e0611-6fac-4aa0-b0a3-5173515abfbf.tmp to hdfs://namenode:9000/spark_checkpoint/commits/1
[2024-11-12T09:42:50.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:50.767+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:50.768+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:50.768+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:50.768+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:43.729Z",
[2024-11-12T09:42:50.769+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-12T09:42:50.769+0000] {spark_submit.py:495} INFO - "numInputRows" : 13,
[2024-11-12T09:42:50.769+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8902890015066429,
[2024-11-12T09:42:50.770+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.8513244090002847,
[2024-11-12T09:42:50.771+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:50.771+0000] {spark_submit.py:495} INFO - "addBatch" : 6596,
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "commitOffsets" : 93,
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "latestOffset" : 32,
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "queryPlanning" : 145,
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7022,
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "walCommit" : 142
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - "0" : 566
[2024-11-12T09:42:50.772+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - "0" : 579
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:50.773+0000] {spark_submit.py:495} INFO - "0" : 579
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "numInputRows" : 13,
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8902890015066429,
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.8513244090002847,
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:50.774+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:50.775+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:50.775+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:50.775+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:50.775+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:50.775+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:50.775+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:50.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/2 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.2.31162c7e-6758-4910-87cb-5aa8986f0328.tmp
[2024-11-12T09:42:50.913+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.2.31162c7e-6758-4910-87cb-5aa8986f0328.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/2
[2024-11-12T09:42:50.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1731404570785,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:50.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:50.996+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:51.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:51.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:51.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO FileStreamSinkLog: BatchIds found from listing: 0, 0, 1, 1
[2024-11-12T09:42:51.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:51.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:51.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:51.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:51.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:51.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:51.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:51.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:51.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:42:51.277+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:51.283+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:42:51.284+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:51.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:51.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:51.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-11-12T09:42:51.298+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:51.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:52.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1491 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:52.795+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-11-12T09:42:52.796+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.583 s
[2024-11-12T09:42:52.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:52.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-11-12T09:42:52.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.604036 s
[2024-11-12T09:42:52.803+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO FileFormatWriter: Start to commit write Job f81721f4-a20f-47b2-8f26-55e340f8ad2b.
[2024-11-12T09:42:52.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/2 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.2.fa403c2c-a8fd-43d2-9b3c-d80737b9a65c.tmp
[2024-11-12T09:42:52.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.2.fa403c2c-a8fd-43d2-9b3c-d80737b9a65c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/2
[2024-11-12T09:42:52.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO ManifestFileCommitProtocol: Committed batch 2
[2024-11-12T09:42:52.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO FileFormatWriter: Write Job f81721f4-a20f-47b2-8f26-55e340f8ad2b committed. Elapsed time: 127 ms.
[2024-11-12T09:42:52.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO FileFormatWriter: Finished processing stats for write job f81721f4-a20f-47b2-8f26-55e340f8ad2b.
[2024-11-12T09:42:52.936+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/2 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.2.1356caf6-bba8-48b7-928c-1f7b71ecb662.tmp
[2024-11-12T09:42:53.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.2.1356caf6-bba8-48b7-928c-1f7b71ecb662.tmp to hdfs://namenode:9000/spark_checkpoint/commits/2
[2024-11-12T09:42:53.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:53.026+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:53.027+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:53.028+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:53.031+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:50.766Z",
[2024-11-12T09:42:53.034+0000] {spark_submit.py:495} INFO - "batchId" : 2,
[2024-11-12T09:42:53.036+0000] {spark_submit.py:495} INFO - "numInputRows" : 7,
[2024-11-12T09:42:53.036+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9947420775898821,
[2024-11-12T09:42:53.036+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 3.1097290093291874,
[2024-11-12T09:42:53.036+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:53.036+0000] {spark_submit.py:495} INFO - "addBatch" : 1916,
[2024-11-12T09:42:53.037+0000] {spark_submit.py:495} INFO - "commitOffsets" : 91,
[2024-11-12T09:42:53.037+0000] {spark_submit.py:495} INFO - "getBatch" : 2,
[2024-11-12T09:42:53.037+0000] {spark_submit.py:495} INFO - "latestOffset" : 19,
[2024-11-12T09:42:53.037+0000] {spark_submit.py:495} INFO - "queryPlanning" : 79,
[2024-11-12T09:42:53.037+0000] {spark_submit.py:495} INFO - "triggerExecution" : 2251,
[2024-11-12T09:42:53.037+0000] {spark_submit.py:495} INFO - "walCommit" : 130
[2024-11-12T09:42:53.038+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:53.038+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:53.038+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:53.039+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:53.039+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - "0" : 579
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:53.040+0000] {spark_submit.py:495} INFO - "0" : 586
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "0" : 586
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "numInputRows" : 7,
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9947420775898821,
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 3.1097290093291874,
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:53.041+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:53.042+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:53.065+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/3 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.3.2ddf7690-3818-4194-b6e7-93fd6705a437.tmp
[2024-11-12T09:42:53.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.3.2ddf7690-3818-4194-b6e7-93fd6705a437.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/3
[2024-11-12T09:42:53.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1731404573045,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:53.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:53.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:53.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:53.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:53.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:53.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:53.314+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 1, 2, 2
[2024-11-12T09:42:53.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:53.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:53.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:53.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:53.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:53.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:53.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:53.469+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:42:53.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:42:53.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:53.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:53.487+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:53.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-11-12T09:42:53.490+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:53.549+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:54.322+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 831 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:54.323+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-11-12T09:42:54.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 0.889 s
[2024-11-12T09:42:54.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:54.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-11-12T09:42:54.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 0.907422 s
[2024-11-12T09:42:54.338+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO FileFormatWriter: Start to commit write Job 5ac85b4b-635f-4689-acc6-6c1d4d8c1e78.
[2024-11-12T09:42:54.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/3 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.3.6d8f1220-104f-4a4f-9e09-a00ea20cb255.tmp
[2024-11-12T09:42:54.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.3.6d8f1220-104f-4a4f-9e09-a00ea20cb255.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/3
[2024-11-12T09:42:54.434+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO ManifestFileCommitProtocol: Committed batch 3
[2024-11-12T09:42:54.436+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO FileFormatWriter: Write Job 5ac85b4b-635f-4689-acc6-6c1d4d8c1e78 committed. Elapsed time: 91 ms.
[2024-11-12T09:42:54.437+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO FileFormatWriter: Finished processing stats for write job 5ac85b4b-635f-4689-acc6-6c1d4d8c1e78.
[2024-11-12T09:42:54.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/3 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.3.a742f928-358d-45ff-a260-b8d83fd881af.tmp
[2024-11-12T09:42:54.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.3.a742f928-358d-45ff-a260-b8d83fd881af.tmp to hdfs://namenode:9000/spark_checkpoint/commits/3
[2024-11-12T09:42:54.550+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:54.551+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:54.552+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:54.552+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:54.552+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:53.025Z",
[2024-11-12T09:42:54.553+0000] {spark_submit.py:495} INFO - "batchId" : 3,
[2024-11-12T09:42:54.557+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:42:54.560+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8853474988933157,
[2024-11-12T09:42:54.563+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3192612137203166,
[2024-11-12T09:42:54.563+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:54.563+0000] {spark_submit.py:495} INFO - "addBatch" : 1188,
[2024-11-12T09:42:54.564+0000] {spark_submit.py:495} INFO - "commitOffsets" : 104,
[2024-11-12T09:42:54.564+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:42:54.564+0000] {spark_submit.py:495} INFO - "latestOffset" : 20,
[2024-11-12T09:42:54.564+0000] {spark_submit.py:495} INFO - "queryPlanning" : 102,
[2024-11-12T09:42:54.564+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1516,
[2024-11-12T09:42:54.564+0000] {spark_submit.py:495} INFO - "walCommit" : 92
[2024-11-12T09:42:54.565+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:54.565+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:54.565+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:54.565+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:54.565+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:54.567+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:54.567+0000] {spark_submit.py:495} INFO - "0" : 586
[2024-11-12T09:42:54.568+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:54.568+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:54.568+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:54.569+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:54.569+0000] {spark_submit.py:495} INFO - "0" : 588
[2024-11-12T09:42:54.569+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:54.569+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:54.570+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:54.570+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:54.570+0000] {spark_submit.py:495} INFO - "0" : 588
[2024-11-12T09:42:54.571+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:54.571+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:54.572+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:42:54.572+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8853474988933157,
[2024-11-12T09:42:54.572+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3192612137203166,
[2024-11-12T09:42:54.573+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:54.573+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:54.573+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:54.574+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:54.574+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:54.575+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:54.575+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:54.575+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:54.575+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:54.575+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:54.575+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:54.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/4 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.4.14d5624b-c6f2-450e-974f-3cc9521c196d.tmp
[2024-11-12T09:42:54.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.4.14d5624b-c6f2-450e-974f-3cc9521c196d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/4
[2024-11-12T09:42:54.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1731404574583,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:54.800+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:54.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:54.872+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:54.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:54.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 2, 3, 3
[2024-11-12T09:42:54.912+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:54.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:54.996+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:54.998+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:54.999+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:54.999+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:54 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:55.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:55.052+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:42:55.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:42:55.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:42:55.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:55.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:55.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-11-12T09:42:55.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:55.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:42:55.457+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 369 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:55.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-11-12T09:42:55.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.456 s
[2024-11-12T09:42:55.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:55.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-11-12T09:42:55.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.467163 s
[2024-11-12T09:42:55.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO FileFormatWriter: Start to commit write Job afccfb5d-6bf4-4bef-8df6-33c5e2a41183.
[2024-11-12T09:42:55.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/4 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.4.9a0b2ea8-14c4-4f0d-860d-cdc5e1256296.tmp
[2024-11-12T09:42:55.593+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.4.9a0b2ea8-14c4-4f0d-860d-cdc5e1256296.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/4
[2024-11-12T09:42:55.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO ManifestFileCommitProtocol: Committed batch 4
[2024-11-12T09:42:55.595+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO FileFormatWriter: Write Job afccfb5d-6bf4-4bef-8df6-33c5e2a41183 committed. Elapsed time: 133 ms.
[2024-11-12T09:42:55.595+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO FileFormatWriter: Finished processing stats for write job afccfb5d-6bf4-4bef-8df6-33c5e2a41183.
[2024-11-12T09:42:55.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/4 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.4.263ff4fc-49e5-4cda-aed4-b28300d96697.tmp
[2024-11-12T09:42:55.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.4.263ff4fc-49e5-4cda-aed4-b28300d96697.tmp to hdfs://namenode:9000/spark_checkpoint/commits/4
[2024-11-12T09:42:55.691+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:55.692+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:55.692+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:55.693+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:55.693+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:54.550Z",
[2024-11-12T09:42:55.694+0000] {spark_submit.py:495} INFO - "batchId" : 4,
[2024-11-12T09:42:55.694+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:55.694+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6557377049180328,
[2024-11-12T09:42:55.694+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8787346221441126,
[2024-11-12T09:42:55.694+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:55.694+0000] {spark_submit.py:495} INFO - "addBatch" : 764,
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "commitOffsets" : 92,
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "latestOffset" : 28,
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "queryPlanning" : 88,
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1138,
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "walCommit" : 141
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:55.695+0000] {spark_submit.py:495} INFO - "0" : 588
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "0" : 589
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "0" : 589
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:55.696+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6557377049180328,
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8787346221441126,
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:55.697+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:55.698+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:55.698+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:55.698+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:55.698+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:55.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/5 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.5.0702ad09-3bca-440b-8a4f-2fb39e10d153.tmp
[2024-11-12T09:42:55.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.5.0702ad09-3bca-440b-8a4f-2fb39e10d153.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/5
[2024-11-12T09:42:55.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1731404575707,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:55.855+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:55.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:55.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:55.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:55.940+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 3, 4, 4
[2024-11-12T09:42:55.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:56.000+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:56.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:56.021+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:56.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:56.029+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:56.037+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:56.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:56.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:56.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:56.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:56.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:42:56.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:42:56.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:56.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:56.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:56.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-11-12T09:42:56.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:56.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:56.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 705 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:56.810+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-11-12T09:42:56.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.782 s
[2024-11-12T09:42:56.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:56.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-11-12T09:42:56.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.791706 s
[2024-11-12T09:42:56.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO FileFormatWriter: Start to commit write Job f40430d9-6ef9-4a48-a021-15aec881caf8.
[2024-11-12T09:42:56.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/5 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.5.dec7771d-28cf-489d-895a-2b955a206c44.tmp
[2024-11-12T09:42:56.903+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.5.dec7771d-28cf-489d-895a-2b955a206c44.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/5
[2024-11-12T09:42:56.904+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO ManifestFileCommitProtocol: Committed batch 5
[2024-11-12T09:42:56.905+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO FileFormatWriter: Write Job f40430d9-6ef9-4a48-a021-15aec881caf8 committed. Elapsed time: 89 ms.
[2024-11-12T09:42:56.906+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO FileFormatWriter: Finished processing stats for write job f40430d9-6ef9-4a48-a021-15aec881caf8.
[2024-11-12T09:42:56.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/5 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.5.489b2ac1-29f8-40a8-b31c-7ec5994ff80c.tmp
[2024-11-12T09:42:57.037+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.5.489b2ac1-29f8-40a8-b31c-7ec5994ff80c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/5
[2024-11-12T09:42:57.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:57.046+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:57.047+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:57.047+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:57.047+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:55.691Z",
[2024-11-12T09:42:57.048+0000] {spark_submit.py:495} INFO - "batchId" : 5,
[2024-11-12T09:42:57.048+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:42:57.049+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7528483786152498,
[2024-11-12T09:42:57.050+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.488095238095238,
[2024-11-12T09:42:57.050+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:57.051+0000] {spark_submit.py:495} INFO - "addBatch" : 1022,
[2024-11-12T09:42:57.051+0000] {spark_submit.py:495} INFO - "commitOffsets" : 129,
[2024-11-12T09:42:57.051+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:42:57.052+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-12T09:42:57.052+0000] {spark_submit.py:495} INFO - "queryPlanning" : 80,
[2024-11-12T09:42:57.052+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1344,
[2024-11-12T09:42:57.052+0000] {spark_submit.py:495} INFO - "walCommit" : 82
[2024-11-12T09:42:57.053+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:57.053+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:57.054+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:57.054+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:57.054+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:57.055+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:57.055+0000] {spark_submit.py:495} INFO - "0" : 589
[2024-11-12T09:42:57.055+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:57.056+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:57.056+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:57.056+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:57.057+0000] {spark_submit.py:495} INFO - "0" : 591
[2024-11-12T09:42:57.057+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:57.057+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:57.057+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:57.067+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:57.069+0000] {spark_submit.py:495} INFO - "0" : 591
[2024-11-12T09:42:57.069+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:57.069+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:57.070+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:42:57.070+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7528483786152498,
[2024-11-12T09:42:57.070+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.488095238095238,
[2024-11-12T09:42:57.072+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:57.073+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:57.073+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:57.074+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:57.074+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:57.074+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:57.074+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:57.075+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:57.075+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:57.075+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:57.075+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:57.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/6 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.6.a1b93bc7-6bca-4f87-bbc8-add39ed2f6c3.tmp
[2024-11-12T09:42:57.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.6.a1b93bc7-6bca-4f87-bbc8-add39ed2f6c3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/6
[2024-11-12T09:42:57.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1731404577076,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:57.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:57.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:57.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:57.379+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:57.407+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 4, 5, 5
[2024-11-12T09:42:57.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:57.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:57.490+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:57.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:57.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:57.492+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:57.493+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[28] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:57.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:42:57.524+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:42:57.528+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:42:57.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:57.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:57.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-11-12T09:42:57.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:57.593+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:42:57.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 349 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:57.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-11-12T09:42:57.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.396 s
[2024-11-12T09:42:57.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:57.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-11-12T09:42:57.908+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.410558 s
[2024-11-12T09:42:57.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO FileFormatWriter: Start to commit write Job 06809a27-0e89-41c2-aad7-ad100af17fd9.
[2024-11-12T09:42:57.946+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/6 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.6.549ecbf4-6702-43d3-bbd7-d95fe35e7904.tmp
[2024-11-12T09:42:58.022+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.6.549ecbf4-6702-43d3-bbd7-d95fe35e7904.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/6
[2024-11-12T09:42:58.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO ManifestFileCommitProtocol: Committed batch 6
[2024-11-12T09:42:58.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO FileFormatWriter: Write Job 06809a27-0e89-41c2-aad7-ad100af17fd9 committed. Elapsed time: 108 ms.
[2024-11-12T09:42:58.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO FileFormatWriter: Finished processing stats for write job 06809a27-0e89-41c2-aad7-ad100af17fd9.
[2024-11-12T09:42:58.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/6 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.6.70f9fd11-0b69-4674-a68e-d295d4c61273.tmp
[2024-11-12T09:42:58.128+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.6.70f9fd11-0b69-4674-a68e-d295d4c61273.tmp to hdfs://namenode:9000/spark_checkpoint/commits/6
[2024-11-12T09:42:58.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:58.132+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:58.132+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:58.133+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:58.133+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:57.045Z",
[2024-11-12T09:42:58.133+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-12T09:42:58.134+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:58.134+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7385524372230428,
[2024-11-12T09:42:58.134+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9233610341643583,
[2024-11-12T09:42:58.135+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:58.135+0000] {spark_submit.py:495} INFO - "addBatch" : 716,
[2024-11-12T09:42:58.135+0000] {spark_submit.py:495} INFO - "commitOffsets" : 104,
[2024-11-12T09:42:58.136+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:42:58.136+0000] {spark_submit.py:495} INFO - "latestOffset" : 30,
[2024-11-12T09:42:58.136+0000] {spark_submit.py:495} INFO - "queryPlanning" : 76,
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1083,
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "walCommit" : 140
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - "0" : 591
[2024-11-12T09:42:58.137+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:58.138+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:58.138+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:58.138+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:58.138+0000] {spark_submit.py:495} INFO - "0" : 592
[2024-11-12T09:42:58.138+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:58.139+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:58.139+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:58.139+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:58.140+0000] {spark_submit.py:495} INFO - "0" : 592
[2024-11-12T09:42:58.140+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:58.140+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:58.140+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:58.141+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7385524372230428,
[2024-11-12T09:42:58.141+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9233610341643583,
[2024-11-12T09:42:58.141+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:58.141+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:58.141+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:58.141+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:58.142+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:58.142+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:58.142+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:58.142+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:58.143+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:58.143+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:58.143+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:58.184+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/7 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.7.2095f25c-bb22-4bb8-90e6-b3721445d3cc.tmp
[2024-11-12T09:42:58.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.7.2095f25c-bb22-4bb8-90e6-b3721445d3cc.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/7
[2024-11-12T09:42:58.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1731404578164,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:58.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:58.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:58.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:58.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:58.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 5, 6, 6
[2024-11-12T09:42:58.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:58.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:58.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:58.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:58.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:58.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:58.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:58.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:42:58.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:42:58.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:42:58.565+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:58.567+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:58.567+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-11-12T09:42:58.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:58.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:42:58.848+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 274 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:58.849+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-11-12T09:42:58.849+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.330 s
[2024-11-12T09:42:58.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:58.851+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-11-12T09:42:58.851+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.336885 s
[2024-11-12T09:42:58.852+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO FileFormatWriter: Start to commit write Job f4a6797f-50a1-46ed-aa89-cbb3c8ca6777.
[2024-11-12T09:42:58.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/7 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.7.63c70b3e-829c-45be-a4e6-2b7b16c9b3c8.tmp
[2024-11-12T09:42:58.943+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.7.63c70b3e-829c-45be-a4e6-2b7b16c9b3c8.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/7
[2024-11-12T09:42:58.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO ManifestFileCommitProtocol: Committed batch 7
[2024-11-12T09:42:58.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO FileFormatWriter: Write Job f4a6797f-50a1-46ed-aa89-cbb3c8ca6777 committed. Elapsed time: 90 ms.
[2024-11-12T09:42:58.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO FileFormatWriter: Finished processing stats for write job f4a6797f-50a1-46ed-aa89-cbb3c8ca6777.
[2024-11-12T09:42:58.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/7 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.7.0c6ac028-69d8-43f2-b339-e1e72edb3b8a.tmp
[2024-11-12T09:42:59.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.7.0c6ac028-69d8-43f2-b339-e1e72edb3b8a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/7
[2024-11-12T09:42:59.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:59.035+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:59.038+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:59.039+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:59.039+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:58.131Z",
[2024-11-12T09:42:59.039+0000] {spark_submit.py:495} INFO - "batchId" : 7,
[2024-11-12T09:42:59.040+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:59.041+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9208103130755064,
[2024-11-12T09:42:59.044+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1111111111111112,
[2024-11-12T09:42:59.044+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:59.044+0000] {spark_submit.py:495} INFO - "addBatch" : 602,
[2024-11-12T09:42:59.045+0000] {spark_submit.py:495} INFO - "commitOffsets" : 88,
[2024-11-12T09:42:59.045+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:42:59.045+0000] {spark_submit.py:495} INFO - "latestOffset" : 32,
[2024-11-12T09:42:59.051+0000] {spark_submit.py:495} INFO - "queryPlanning" : 56,
[2024-11-12T09:42:59.051+0000] {spark_submit.py:495} INFO - "triggerExecution" : 900,
[2024-11-12T09:42:59.051+0000] {spark_submit.py:495} INFO - "walCommit" : 109
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "0" : 592
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.052+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - "0" : 593
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - "0" : 593
[2024-11-12T09:42:59.053+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.054+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.054+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9208103130755064,
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1111111111111112,
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:59.055+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:59.056+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.060+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/8 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.8.9923f575-6642-4e22-9c0c-2e66f481422e.tmp
[2024-11-12T09:42:59.152+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.8.9923f575-6642-4e22-9c0c-2e66f481422e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/8
[2024-11-12T09:42:59.153+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1731404579062,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:42:59.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:59.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:59.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:59.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:42:59.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 6, 7, 7
[2024-11-12T09:42:59.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:42:59.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:42:59.281+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:42:59.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:59.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:59.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:42:59.321+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:42:59.321+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:42:59.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:42:59.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:42:59.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[36] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:42:59.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:59.339+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:42:59.352+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:42:59.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:42:59.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:59.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:42:59.367+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[36] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:42:59.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-11-12T09:42:59.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:42:59.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:42:59.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 419 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:42:59.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-11-12T09:42:59.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.463 s
[2024-11-12T09:42:59.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:42:59.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-11-12T09:42:59.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.471616 s
[2024-11-12T09:42:59.795+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO FileFormatWriter: Start to commit write Job ca3887ba-cac8-4786-9ab3-2e3e584aa48a.
[2024-11-12T09:42:59.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/8 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.8.eea48304-8418-469d-8a23-4bf6060dbd0f.tmp
[2024-11-12T09:42:59.872+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.8.eea48304-8418-469d-8a23-4bf6060dbd0f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/8
[2024-11-12T09:42:59.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO ManifestFileCommitProtocol: Committed batch 8
[2024-11-12T09:42:59.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO FileFormatWriter: Write Job ca3887ba-cac8-4786-9ab3-2e3e584aa48a committed. Elapsed time: 77 ms.
[2024-11-12T09:42:59.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO FileFormatWriter: Finished processing stats for write job ca3887ba-cac8-4786-9ab3-2e3e584aa48a.
[2024-11-12T09:42:59.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/8 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.8.88911184-9a3c-4b08-a36a-88ac77e1c9f7.tmp
[2024-11-12T09:42:59.951+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.8.88911184-9a3c-4b08-a36a-88ac77e1c9f7.tmp to hdfs://namenode:9000/spark_checkpoint/commits/8
[2024-11-12T09:42:59.957+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:42:59.958+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:42:59.958+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:42:59.959+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:42:59.959+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:59.035Z",
[2024-11-12T09:42:59.959+0000] {spark_submit.py:495} INFO - "batchId" : 8,
[2024-11-12T09:42:59.960+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:59.960+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1061946902654867,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0917030567685588,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "addBatch" : 670,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "commitOffsets" : 76,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "latestOffset" : 26,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "queryPlanning" : 44,
[2024-11-12T09:42:59.961+0000] {spark_submit.py:495} INFO - "triggerExecution" : 916,
[2024-11-12T09:42:59.962+0000] {spark_submit.py:495} INFO - "walCommit" : 91
[2024-11-12T09:42:59.962+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.962+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:42:59.962+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:42:59.962+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:42:59.963+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:42:59.963+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:59.963+0000] {spark_submit.py:495} INFO - "0" : 593
[2024-11-12T09:42:59.963+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.973+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.973+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:42:59.974+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:59.975+0000] {spark_submit.py:495} INFO - "0" : 594
[2024-11-12T09:42:59.976+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.976+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.978+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:42:59.978+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:42:59.978+0000] {spark_submit.py:495} INFO - "0" : 594
[2024-11-12T09:42:59.979+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.979+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:42:59.979+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:42:59.979+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1061946902654867,
[2024-11-12T09:42:59.980+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0917030567685588,
[2024-11-12T09:42:59.980+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:42:59.980+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:42:59.980+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:42:59.980+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:42:59.980+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.981+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:42:59.981+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:42:59.981+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:42:59.981+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:42:59.982+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.982+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:42:59.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:42:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/9 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.9.58ec633c-f029-4ca7-af7f-141fe74627dc.tmp
[2024-11-12T09:43:00.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.9.58ec633c-f029-4ca7-af7f-141fe74627dc.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/9
[2024-11-12T09:43:00.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1731404579982,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:43:00.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:00.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:00.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:00.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:00.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 8
[2024-11-12T09:43:00.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:43:00.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:43:00.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:43:00.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:43:00.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:43:00.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:43:00.216+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[40] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:43:00.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:43:00.243+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:43:00.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:43:00.246+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:43:00.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[40] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:43:00.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-11-12T09:43:00.253+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:43:00.298+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:43:00.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 567 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:43:00.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-11-12T09:43:00.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.603 s
[2024-11-12T09:43:00.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:43:00.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-11-12T09:43:00.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.612259 s
[2024-11-12T09:43:00.838+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO FileFormatWriter: Start to commit write Job 1863e7ed-9b65-4338-b657-4b155db851a8.
[2024-11-12T09:43:00.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/9.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.9.compact.254c2f26-46c1-4352-900b-29af422de059.tmp
[2024-11-12T09:43:01.601+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.9.compact.254c2f26-46c1-4352-900b-29af422de059.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/9.compact
[2024-11-12T09:43:01.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:01 INFO ManifestFileCommitProtocol: Committed batch 9
[2024-11-12T09:43:01.605+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:01 INFO FileFormatWriter: Write Job 1863e7ed-9b65-4338-b657-4b155db851a8 committed. Elapsed time: 776 ms.
[2024-11-12T09:43:01.610+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:01 INFO FileFormatWriter: Finished processing stats for write job 1863e7ed-9b65-4338-b657-4b155db851a8.
[2024-11-12T09:43:01.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/9 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.9.30b2ae3e-7281-4071-a57c-742b788d9935.tmp
[2024-11-12T09:43:02.076+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.9.30b2ae3e-7281-4071-a57c-742b788d9935.tmp to hdfs://namenode:9000/spark_checkpoint/commits/9
[2024-11-12T09:43:02.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:02.079+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:02.079+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:02.080+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:02.080+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:42:59.957Z",
[2024-11-12T09:43:02.080+0000] {spark_submit.py:495} INFO - "batchId" : 9,
[2024-11-12T09:43:02.080+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:43:02.081+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0845986984815619,
[2024-11-12T09:43:02.081+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.4716981132075471,
[2024-11-12T09:43:02.081+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:02.081+0000] {spark_submit.py:495} INFO - "addBatch" : 1486,
[2024-11-12T09:43:02.081+0000] {spark_submit.py:495} INFO - "commitOffsets" : 466,
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "latestOffset" : 25,
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "queryPlanning" : 59,
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "triggerExecution" : 2120,
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "walCommit" : 73
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:02.082+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - "0" : 594
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - "0" : 595
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:02.083+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - "0" : 595
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0845986984815619,
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.4716981132075471,
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:02.084+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:02.085+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.086+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/10 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.10.e75c0345-7ed4-40d5-b023-fd0ef37eaab8.tmp
[2024-11-12T09:43:02.152+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.10.e75c0345-7ed4-40d5-b023-fd0ef37eaab8.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/10
[2024-11-12T09:43:02.153+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1731404582098,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:43:02.198+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:02.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:02.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:02.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:02.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 8, 9
[2024-11-12T09:43:02.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:43:02.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:43:02.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:43:02.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:43:02.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:43:02.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:43:02.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:43:02.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:43:02.332+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:43:02.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:43:02.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:43:02.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:43:02.367+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:43:02.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:43:02.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:43:02.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:43:02.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-11-12T09:43:02.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:43:02.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:43:02.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 408 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:43:02.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-11-12T09:43:02.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.485 s
[2024-11-12T09:43:02.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:43:02.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-11-12T09:43:02.796+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.494032 s
[2024-11-12T09:43:02.796+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO FileFormatWriter: Start to commit write Job d0372da2-3937-4a59-9953-51fd84bb8756.
[2024-11-12T09:43:02.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/10 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.10.78416129-b78d-4d6f-a117-d0f7af36f83f.tmp
[2024-11-12T09:43:02.863+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.10.78416129-b78d-4d6f-a117-d0f7af36f83f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/10
[2024-11-12T09:43:02.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO ManifestFileCommitProtocol: Committed batch 10
[2024-11-12T09:43:02.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO FileFormatWriter: Write Job d0372da2-3937-4a59-9953-51fd84bb8756 committed. Elapsed time: 67 ms.
[2024-11-12T09:43:02.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO FileFormatWriter: Finished processing stats for write job d0372da2-3937-4a59-9953-51fd84bb8756.
[2024-11-12T09:43:02.878+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/10 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.10.50387e00-1c8b-443f-bb87-e6e35a82e6f4.tmp
[2024-11-12T09:43:02.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.10.50387e00-1c8b-443f-bb87-e6e35a82e6f4.tmp to hdfs://namenode:9000/spark_checkpoint/commits/10
[2024-11-12T09:43:02.934+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:02.935+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:02.942+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:02.078Z",
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "batchId" : 10,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9429514380009429,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.347417840375587,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "addBatch" : 650,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "commitOffsets" : 66,
[2024-11-12T09:43:02.943+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "latestOffset" : 20,
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "queryPlanning" : 50,
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "triggerExecution" : 852,
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "walCommit" : 56
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - "0" : 595
[2024-11-12T09:43:02.944+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.945+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.945+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:02.946+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:02.946+0000] {spark_submit.py:495} INFO - "0" : 597
[2024-11-12T09:43:02.947+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.947+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.947+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:02.947+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:02.948+0000] {spark_submit.py:495} INFO - "0" : 597
[2024-11-12T09:43:02.948+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.948+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9429514380009429,
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.347417840375587,
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.949+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:02.950+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:02.950+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:02.950+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:02.950+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.951+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:02.962+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/11 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.11.289614d1-0230-44bb-baef-1397b5c2afad.tmp
[2024-11-12T09:43:03.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.11.289614d1-0230-44bb-baef-1397b5c2afad.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/11
[2024-11-12T09:43:03.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1731404582954,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:43:03.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:03.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:03.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:03.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:03.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 10
[2024-11-12T09:43:03.092+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:43:03.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:43:03.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:43:03.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:43:03.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:43:03.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:43:03.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:43:03.163+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:43:03.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:43:03.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:43:03.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:43:03.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:43:03.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-11-12T09:43:03.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:43:03.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:43:03.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 671 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:43:03.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-11-12T09:43:03.849+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.713 s
[2024-11-12T09:43:03.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:43:03.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-11-12T09:43:03.851+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.719957 s
[2024-11-12T09:43:03.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO FileFormatWriter: Start to commit write Job 24977689-b501-42ed-a0df-b102f18857dc.
[2024-11-12T09:43:03.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/11 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.11.6683934c-2b0f-4fec-8f19-617c43b38be8.tmp
[2024-11-12T09:43:03.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.11.6683934c-2b0f-4fec-8f19-617c43b38be8.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/11
[2024-11-12T09:43:03.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO ManifestFileCommitProtocol: Committed batch 11
[2024-11-12T09:43:03.956+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO FileFormatWriter: Write Job 24977689-b501-42ed-a0df-b102f18857dc committed. Elapsed time: 102 ms.
[2024-11-12T09:43:03.956+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO FileFormatWriter: Finished processing stats for write job 24977689-b501-42ed-a0df-b102f18857dc.
[2024-11-12T09:43:03.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/11 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.11.61c57a56-b256-49c1-a2ed-36a7f24edcb2.tmp
[2024-11-12T09:43:04.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.11.61c57a56-b256-49c1-a2ed-36a7f24edcb2.tmp to hdfs://namenode:9000/spark_checkpoint/commits/11
[2024-11-12T09:43:04.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:04.050+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:04.050+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:04.050+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:04.050+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:02.934Z",
[2024-11-12T09:43:04.050+0000] {spark_submit.py:495} INFO - "batchId" : 11,
[2024-11-12T09:43:04.050+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1682242990654206,
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8976660682226211,
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "addBatch" : 904,
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "commitOffsets" : 91,
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:43:04.055+0000] {spark_submit.py:495} INFO - "latestOffset" : 19,
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1114,
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "walCommit" : 63
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - "0" : 597
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:04.056+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - "0" : 598
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - "0" : 598
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:04.057+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1682242990654206,
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8976660682226211,
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:04.058+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:04.059+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:04.059+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:04.059+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:04.059+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:04.059+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:04.059+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:04.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/12 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.12.80ed7728-9b8f-4a9b-9958-16041e90059f.tmp
[2024-11-12T09:43:04.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.12.80ed7728-9b8f-4a9b-9958-16041e90059f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/12
[2024-11-12T09:43:04.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1731404584061,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:43:04.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:04.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:04.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:04.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:43:04.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, 11
[2024-11-12T09:43:04.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:43:04.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:43:04.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:43:04.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:43:04.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:43:04.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:43:04.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[52] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:43:04.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:43:04.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:43:04.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:43:04.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:43:04.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[52] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:43:04.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-11-12T09:43:04.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:43:04.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:43:05.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 811 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:43:05.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-11-12T09:43:05.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.840 s
[2024-11-12T09:43:05.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:43:05.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-11-12T09:43:05.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.843641 s
[2024-11-12T09:43:05.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO FileFormatWriter: Start to commit write Job 3050e84b-9fe2-4590-bc72-cbcb2e053320.
[2024-11-12T09:43:05.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/12 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.12.44f5fb24-7107-4a80-bb78-d96e4acf269c.tmp
[2024-11-12T09:43:05.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.12.44f5fb24-7107-4a80-bb78-d96e4acf269c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/12
[2024-11-12T09:43:05.212+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO ManifestFileCommitProtocol: Committed batch 12
[2024-11-12T09:43:05.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO FileFormatWriter: Write Job 3050e84b-9fe2-4590-bc72-cbcb2e053320 committed. Elapsed time: 100 ms.
[2024-11-12T09:43:05.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO FileFormatWriter: Finished processing stats for write job 3050e84b-9fe2-4590-bc72-cbcb2e053320.
[2024-11-12T09:43:05.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/12 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.12.dfdf5713-4e2f-438c-96c7-27f0f1f4b906.tmp
[2024-11-12T09:43:05.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.12.dfdf5713-4e2f-438c-96c7-27f0f1f4b906.tmp to hdfs://namenode:9000/spark_checkpoint/commits/12
[2024-11-12T09:43:05.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:05.305+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:05.305+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:05.305+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:05.305+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:04.049Z",
[2024-11-12T09:43:05.305+0000] {spark_submit.py:495} INFO - "batchId" : 12,
[2024-11-12T09:43:05.305+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8968609865470852,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8012820512820513,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "addBatch" : 1032,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "commitOffsets" : 84,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "queryPlanning" : 42,
[2024-11-12T09:43:05.306+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1248,
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "walCommit" : 74
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:05.307+0000] {spark_submit.py:495} INFO - "0" : 598
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:05.308+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:05.309+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:05.311+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:05.313+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:05.313+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:05.313+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:05.313+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:43:05.317+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8968609865470852,
[2024-11-12T09:43:05.318+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8012820512820513,
[2024-11-12T09:43:05.318+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:05.318+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:05.318+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:05.318+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:05.318+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:05.319+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:05.319+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:05.319+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:05.319+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:05.319+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:05.319+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:07.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:43:07.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:43:07.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:43:07.276+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:43:07.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:43:07.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:43:15.321+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:15.304Z",
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:43:15.322+0000] {spark_submit.py:495} INFO - "triggerExecution" : 13
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:15.323+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:15.324+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:15.324+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:15.324+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:15.325+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:15.325+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:15.326+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:15.327+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:15.327+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:15.327+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:15.327+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:15.327+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:15.328+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:15.328+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:15.328+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:25.323+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:25.323+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:25.323+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:25.323+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:25.315Z",
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:25.324+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:25.325+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:25.326+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:35.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:35.327+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:35.328+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:35.328+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:35.317Z",
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:35.329+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:35.330+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:35.332+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:35.337+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:35.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:35.338+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:35.339+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:35.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:45.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:45.329+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:45.329+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:45.313Z",
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "latestOffset" : 14,
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "triggerExecution" : 14
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:45.330+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:45.331+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:45.332+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:45.332+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:45.332+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:45.333+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:45.334+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:55.344+0000] {spark_submit.py:495} INFO - 24/11/12 09:43:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:43:55.345+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:43:55.345+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:43:55.346+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:43:55.346+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:43:55.335Z",
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:43:55.347+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:55.348+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:55.349+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:55.349+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:55.349+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:43:55.349+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:43:55.350+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:43:55.350+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:55.350+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:43:55.350+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:43:55.350+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:43:55.351+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:43:55.351+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:43:55.351+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:43:55.352+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:43:55.353+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:43:55.353+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:05.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:44:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:44:05.351+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:44:05.351+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:44:05.351+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:44:05.351+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:44:05.340Z",
[2024-11-12T09:44:05.351+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:44:05.352+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:05.352+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:05.352+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:05.352+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:44:05.353+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:44:05.353+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8
[2024-11-12T09:44:05.353+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:05.353+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:44:05.354+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:44:05.354+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:44:05.354+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:44:05.354+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:05.354+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:05.354+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:05.355+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:44:05.356+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:05.357+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:15.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:44:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:44:15.363+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:44:15.363+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:44:15.364+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:44:15.364+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:44:15.356Z",
[2024-11-12T09:44:15.364+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:44:15.364+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:15.368+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:15.368+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:44:15.369+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:44:15.370+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:44:15.370+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:15.375+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:15.376+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:15.377+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:44:15.378+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:15.378+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:15.378+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:15.378+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:15.378+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:15.378+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:15.379+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:15.379+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:44:15.379+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:44:15.379+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:44:15.380+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:44:15.380+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:15.380+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:44:15.380+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:44:15.381+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:44:15.381+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:44:15.381+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:15.382+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:25.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:44:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:44:25.365+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:44:25.366+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:44:25.366+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:44:25.366+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:44:25.355Z",
[2024-11-12T09:44:25.366+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:44:25.367+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:44:25.368+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:25.368+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:25.368+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:25.368+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:25.368+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:25.369+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:25.370+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:25.372+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:25.372+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:25.373+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:35.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:44:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:44:35.386+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:44:35.386+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:44:35.386+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:44:35.386+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:44:35.372Z",
[2024-11-12T09:44:35.387+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:44:35.387+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:35.388+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:35.388+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:35.388+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:44:35.390+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:44:35.390+0000] {spark_submit.py:495} INFO - "triggerExecution" : 10
[2024-11-12T09:44:35.390+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:35.390+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:44:35.391+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:44:35.391+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:44:35.391+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:44:35.391+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:35.391+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:35.391+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:35.393+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:35.394+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:44:35.397+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:35.405+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:35.406+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:35.407+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:35.409+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:44:35.409+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:35.409+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:35.410+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:35.410+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:35.410+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:35.410+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:35.411+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:35.411+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:44:35.412+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:44:35.412+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:44:35.412+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:44:35.413+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:35.413+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:44:35.413+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:44:35.413+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:44:35.414+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:44:35.414+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:35.414+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:45.392+0000] {spark_submit.py:495} INFO - 24/11/12 09:44:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:44:45.393+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:44:45.394+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:44:45.394+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:44:45.394+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:44:45.388Z",
[2024-11-12T09:44:45.395+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:44:45.395+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:45.395+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:45.395+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:45.395+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:44:45.402+0000] {spark_submit.py:495} INFO - "latestOffset" : 2,
[2024-11-12T09:44:45.402+0000] {spark_submit.py:495} INFO - "triggerExecution" : 2
[2024-11-12T09:44:45.403+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:45.403+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:44:45.403+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:44:45.404+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:44:45.404+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:44:45.405+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:45.405+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:45.405+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:45.406+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:45.406+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:44:45.406+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:45.406+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:45.407+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:45.407+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:45.407+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:44:45.408+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:45.408+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:45.408+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:45.408+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:45.408+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:45.409+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:45.409+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:45.409+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:44:45.409+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:45.410+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:55.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:44:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:44:55.408+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:44:55.399Z",
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:44:55.409+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:55.410+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:55.411+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:55.411+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:55.411+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:44:55.411+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:44:55.411+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:44:55.412+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:55.415+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:44:55.415+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:44:55.415+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:44:55.416+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:44:55.417+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:44:55.417+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:44:55.417+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:44:55.417+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:05.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:05.422+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:05.423+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:05.424+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:05.424+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:05.404Z",
[2024-11-12T09:45:05.424+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:45:05.424+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:45:05.424+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:45:05.425+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:45:05.425+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:05.425+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:45:05.425+0000] {spark_submit.py:495} INFO - "triggerExecution" : 12
[2024-11-12T09:45:05.425+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:05.425+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:05.426+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:45:05.427+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:05.427+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:05.427+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:05.427+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:05.427+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:45:05.427+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:05.428+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:05.429+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:11.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:11 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/13 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.13.b2ee38ed-bf24-451e-aad0-ff1c912bca05.tmp
[2024-11-12T09:45:11.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:11 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.13.b2ee38ed-bf24-451e-aad0-ff1c912bca05.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/13
[2024-11-12T09:45:11.987+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:11 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1731404711901,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:12.021+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:12.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:12.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:12.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:12.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 12
[2024-11-12T09:45:12.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:12.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:12.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:12.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:12.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:12.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:12.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:12.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:45:12.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:45:12.218+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:12.219+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:12.222+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:12.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-11-12T09:45:12.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:12.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:13.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 820 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:13.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-11-12T09:45:13.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 0.872 s
[2024-11-12T09:45:13.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:13.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-11-12T09:45:13.052+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.886100 s
[2024-11-12T09:45:13.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO FileFormatWriter: Start to commit write Job 12e3eb60-15e0-4152-9f86-ed2efd3df145.
[2024-11-12T09:45:13.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/13 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.13.83d8b6d7-b863-4b03-89e5-eeff75276fac.tmp
[2024-11-12T09:45:13.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.13.83d8b6d7-b863-4b03-89e5-eeff75276fac.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/13
[2024-11-12T09:45:13.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO ManifestFileCommitProtocol: Committed batch 13
[2024-11-12T09:45:13.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO FileFormatWriter: Write Job 12e3eb60-15e0-4152-9f86-ed2efd3df145 committed. Elapsed time: 482 ms.
[2024-11-12T09:45:13.537+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO FileFormatWriter: Finished processing stats for write job 12e3eb60-15e0-4152-9f86-ed2efd3df145.
[2024-11-12T09:45:13.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/13 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.13.55b5f7b3-9d16-476d-b767-47e9837970b6.tmp
[2024-11-12T09:45:13.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.13.55b5f7b3-9d16-476d-b767-47e9837970b6.tmp to hdfs://namenode:9000/spark_checkpoint/commits/13
[2024-11-12T09:45:13.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:13.607+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:11.898Z",
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 47.61904761904761,
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.5865102639296187,
[2024-11-12T09:45:13.608+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "addBatch" : 1483,
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "commitOffsets" : 67,
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "queryPlanning" : 45,
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1705,
[2024-11-12T09:45:13.609+0000] {spark_submit.py:495} INFO - "walCommit" : 83
[2024-11-12T09:45:13.610+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:13.610+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:13.610+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:13.610+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - "0" : 599
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:13.611+0000] {spark_submit.py:495} INFO - "0" : 600
[2024-11-12T09:45:13.613+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "0" : 600
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 47.61904761904761,
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.5865102639296187,
[2024-11-12T09:45:13.614+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:13.615+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:13.615+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:13.615+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:13.618+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:13.619+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:13.619+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:13.619+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:13.620+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:13.620+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:13.621+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:13.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/14 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.14.1c0293e4-7ffc-4a55-80b7-2d7239c1b61b.tmp
[2024-11-12T09:45:13.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.14.1c0293e4-7ffc-4a55-80b7-2d7239c1b61b.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/14
[2024-11-12T09:45:13.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1731404713614,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:13.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:13.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:13.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:13.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:13.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 13
[2024-11-12T09:45:13.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:13.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:13.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:13.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:13.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:13.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:13.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:13.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:13.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:13.968+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:13.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:13.972+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:13.973+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-11-12T09:45:13.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:13 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:14.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:14.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 217 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:14.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-11-12T09:45:14.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 0.270 s
[2024-11-12T09:45:14.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:14.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-11-12T09:45:14.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.274269 s
[2024-11-12T09:45:14.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO FileFormatWriter: Start to commit write Job 2f5662d8-f033-488e-b24b-7009d8067a38.
[2024-11-12T09:45:14.219+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/14 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.14.83a0a079-8a17-4441-adf8-94429b647e0f.tmp
[2024-11-12T09:45:14.275+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.14.83a0a079-8a17-4441-adf8-94429b647e0f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/14
[2024-11-12T09:45:14.276+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO ManifestFileCommitProtocol: Committed batch 14
[2024-11-12T09:45:14.276+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO FileFormatWriter: Write Job 2f5662d8-f033-488e-b24b-7009d8067a38 committed. Elapsed time: 79 ms.
[2024-11-12T09:45:14.276+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO FileFormatWriter: Finished processing stats for write job 2f5662d8-f033-488e-b24b-7009d8067a38.
[2024-11-12T09:45:14.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/14 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.14.741b85a2-7443-49bd-8976-698a87b56cbc.tmp
[2024-11-12T09:45:14.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.14.741b85a2-7443-49bd-8976-698a87b56cbc.tmp to hdfs://namenode:9000/spark_checkpoint/commits/14
[2024-11-12T09:45:14.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:14.381+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:14.381+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:14.381+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:14.382+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:13.606Z",
[2024-11-12T09:45:14.383+0000] {spark_submit.py:495} INFO - "batchId" : 14,
[2024-11-12T09:45:14.383+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.585480093676815,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "addBatch" : 470,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "commitOffsets" : 85,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "queryPlanning" : 56,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "triggerExecution" : 756,
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "walCommit" : 117
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:14.384+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "0" : 600
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - "0" : 601
[2024-11-12T09:45:14.385+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - "0" : 601
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:14.386+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.585480093676815,
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:14.387+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:14.388+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:14.388+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:14.388+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:14.389+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:14.389+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:14.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/15 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.15.eebeb110-2536-48c3-a6b0-fd791966ac86.tmp
[2024-11-12T09:45:14.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.15.eebeb110-2536-48c3-a6b0-fd791966ac86.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/15
[2024-11-12T09:45:14.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1731404714384,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:14.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:14.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:14.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:14.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:14.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, 14
[2024-11-12T09:45:14.557+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:14.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:14.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:14.621+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:14.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:14.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:14.624+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:14.624+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:14.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[64] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:14.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:14.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:14.674+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:45:14.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:45:14.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:14.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:14.680+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[64] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:14.680+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-11-12T09:45:14.683+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:14.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:15.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:14 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 317 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:15.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-11-12T09:45:15.009+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 0.379 s
[2024-11-12T09:45:15.010+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:15.010+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-11-12T09:45:15.010+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.389414 s
[2024-11-12T09:45:15.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO FileFormatWriter: Start to commit write Job 35a7895b-ae7d-4759-9b28-53ab8a3063b3.
[2024-11-12T09:45:15.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/15 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.15.fe6446d0-fe02-467a-80d2-ca78573e2196.tmp
[2024-11-12T09:45:15.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.15.fe6446d0-fe02-467a-80d2-ca78573e2196.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/15
[2024-11-12T09:45:15.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO ManifestFileCommitProtocol: Committed batch 15
[2024-11-12T09:45:15.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO FileFormatWriter: Write Job 35a7895b-ae7d-4759-9b28-53ab8a3063b3 committed. Elapsed time: 470 ms.
[2024-11-12T09:45:15.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO FileFormatWriter: Finished processing stats for write job 35a7895b-ae7d-4759-9b28-53ab8a3063b3.
[2024-11-12T09:45:15.510+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/15 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.15.b3adb347-d261-48e7-bec6-25d04498ad5d.tmp
[2024-11-12T09:45:15.558+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.15.b3adb347-d261-48e7-bec6-25d04498ad5d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/15
[2024-11-12T09:45:15.564+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:15.573+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:15.574+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:15.575+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:15.575+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:14.380Z",
[2024-11-12T09:45:15.575+0000] {spark_submit.py:495} INFO - "batchId" : 15,
[2024-11-12T09:45:15.578+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:15.578+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2919896640826873,
[2024-11-12T09:45:15.578+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8488964346349746,
[2024-11-12T09:45:15.579+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:15.579+0000] {spark_submit.py:495} INFO - "addBatch" : 975,
[2024-11-12T09:45:15.579+0000] {spark_submit.py:495} INFO - "commitOffsets" : 73,
[2024-11-12T09:45:15.579+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:15.579+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:45:15.580+0000] {spark_submit.py:495} INFO - "queryPlanning" : 33,
[2024-11-12T09:45:15.580+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1178,
[2024-11-12T09:45:15.580+0000] {spark_submit.py:495} INFO - "walCommit" : 76
[2024-11-12T09:45:15.581+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:15.582+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:15.582+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:15.582+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "0" : 601
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "0" : 602
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:15.583+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:15.584+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:15.584+0000] {spark_submit.py:495} INFO - "0" : 602
[2024-11-12T09:45:15.584+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:15.584+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2919896640826873,
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8488964346349746,
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:15.585+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:15.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:15.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:15.589+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/16 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.16.1781b357-e143-4ab5-b96e-4e2f6f513a5e.tmp
[2024-11-12T09:45:15.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.16.1781b357-e143-4ab5-b96e-4e2f6f513a5e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/16
[2024-11-12T09:45:15.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1731404715580,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:15.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:15.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:15.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:15.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:15.739+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 15, 15
[2024-11-12T09:45:15.743+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:15.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:15.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:15.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:15.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:15.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:15.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:15.818+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:15.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:15.830+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:15.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:15.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:15.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-11-12T09:45:15.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:15.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:16.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 261 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:16.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-11-12T09:45:16.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.307 s
[2024-11-12T09:45:16.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:16.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2024-11-12T09:45:16.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.315891 s
[2024-11-12T09:45:16.109+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO FileFormatWriter: Start to commit write Job e9c802f1-e0ac-42a5-92f3-d5cf6d204e18.
[2024-11-12T09:45:16.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/16 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.16.7949a5c4-047d-4a71-8f17-550231f782c0.tmp
[2024-11-12T09:45:16.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.16.7949a5c4-047d-4a71-8f17-550231f782c0.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/16
[2024-11-12T09:45:16.209+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO ManifestFileCommitProtocol: Committed batch 16
[2024-11-12T09:45:16.209+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO FileFormatWriter: Write Job e9c802f1-e0ac-42a5-92f3-d5cf6d204e18 committed. Elapsed time: 100 ms.
[2024-11-12T09:45:16.210+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO FileFormatWriter: Finished processing stats for write job e9c802f1-e0ac-42a5-92f3-d5cf6d204e18.
[2024-11-12T09:45:16.229+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/16 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.16.b0c35442-0822-4a2e-b1a1-31f9ee38457e.tmp
[2024-11-12T09:45:16.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.16.b0c35442-0822-4a2e-b1a1-31f9ee38457e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/16
[2024-11-12T09:45:16.323+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:16.324+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:16.325+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:16.326+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:16.329+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:15.564Z",
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "batchId" : 16,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8445945945945946,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "addBatch" : 520,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "commitOffsets" : 111,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:16.333+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "queryPlanning" : 37,
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "triggerExecution" : 756,
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:16.334+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:16.335+0000] {spark_submit.py:495} INFO - "0" : 602
[2024-11-12T09:45:16.335+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:16.344+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:16.345+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:16.346+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:16.346+0000] {spark_submit.py:495} INFO - "0" : 603
[2024-11-12T09:45:16.346+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:16.351+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:16.352+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:16.353+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:16.353+0000] {spark_submit.py:495} INFO - "0" : 603
[2024-11-12T09:45:16.357+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:16.359+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:16.359+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:16.360+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8445945945945946,
[2024-11-12T09:45:16.360+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:45:16.360+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:16.360+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:16.361+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:16.361+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:16.361+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:16.362+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:16.362+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:16.363+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:16.363+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:16.363+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:16.363+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:16.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/17 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.17.938f938a-4135-4e80-91db-70b6d1b02ed4.tmp
[2024-11-12T09:45:16.487+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.17.938f938a-4135-4e80-91db-70b6d1b02ed4.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/17
[2024-11-12T09:45:16.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1731404716335,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:16.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:16.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:16.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:16.608+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:16.628+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 16, 16
[2024-11-12T09:45:16.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:16.683+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:16.685+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:16.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:16.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:16.687+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:16.687+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:16.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:16.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:16.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:16.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:16.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:16.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-11-12T09:45:16.738+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:16.776+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:16 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:17.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 332 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:17.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-11-12T09:45:17.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.398 s
[2024-11-12T09:45:17.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:17.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-11-12T09:45:17.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.405973 s
[2024-11-12T09:45:17.092+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO FileFormatWriter: Start to commit write Job 5095ed68-7028-4a65-a36c-ac53058c5fdd.
[2024-11-12T09:45:17.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/17 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.17.f007ae60-d75f-45e9-ab78-c915358dbf46.tmp
[2024-11-12T09:45:17.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.17.f007ae60-d75f-45e9-ab78-c915358dbf46.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/17
[2024-11-12T09:45:17.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO ManifestFileCommitProtocol: Committed batch 17
[2024-11-12T09:45:17.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO FileFormatWriter: Write Job 5095ed68-7028-4a65-a36c-ac53058c5fdd committed. Elapsed time: 84 ms.
[2024-11-12T09:45:17.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO FileFormatWriter: Finished processing stats for write job 5095ed68-7028-4a65-a36c-ac53058c5fdd.
[2024-11-12T09:45:17.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/17 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.17.1e2c8a47-3d35-4337-bd8d-7bbbf0013142.tmp
[2024-11-12T09:45:17.252+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.17.1e2c8a47-3d35-4337-bd8d-7bbbf0013142.tmp to hdfs://namenode:9000/spark_checkpoint/commits/17
[2024-11-12T09:45:17.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:17.254+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:16.323Z",
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "batchId" : 17,
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3175230566534915,
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0764262648008611,
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:17.255+0000] {spark_submit.py:495} INFO - "addBatch" : 620,
[2024-11-12T09:45:17.256+0000] {spark_submit.py:495} INFO - "commitOffsets" : 75,
[2024-11-12T09:45:17.256+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:17.256+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:45:17.256+0000] {spark_submit.py:495} INFO - "queryPlanning" : 51,
[2024-11-12T09:45:17.256+0000] {spark_submit.py:495} INFO - "triggerExecution" : 929,
[2024-11-12T09:45:17.257+0000] {spark_submit.py:495} INFO - "walCommit" : 152
[2024-11-12T09:45:17.257+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:17.257+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:17.257+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:17.258+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:17.259+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:17.259+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:17.260+0000] {spark_submit.py:495} INFO - "0" : 603
[2024-11-12T09:45:17.260+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:17.260+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:17.260+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:17.275+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:17.276+0000] {spark_submit.py:495} INFO - "0" : 604
[2024-11-12T09:45:17.276+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:17.276+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:17.276+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:17.277+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:17.277+0000] {spark_submit.py:495} INFO - "0" : 604
[2024-11-12T09:45:17.277+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:17.277+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:17.277+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:17.278+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3175230566534915,
[2024-11-12T09:45:17.278+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0764262648008611,
[2024-11-12T09:45:17.281+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:17.281+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:17.281+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:17.281+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:17.282+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/18 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.18.7783c17b-be30-4ca1-bf53-fcf516c5f8ae.tmp
[2024-11-12T09:45:17.339+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.18.7783c17b-be30-4ca1-bf53-fcf516c5f8ae.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/18
[2024-11-12T09:45:17.341+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1731404717260,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:17.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:17.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:17.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:17.424+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:17.464+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 17, 17
[2024-11-12T09:45:17.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:17.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:17.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:17.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:17.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:17.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:17.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:17.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:17.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:17.569+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Removed broadcast_17_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:17.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:17.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:17.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:45:17.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:45:17.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:17.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:17.617+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:17.617+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-11-12T09:45:17.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:17.621+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:17.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:17 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:18.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 443 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:18.065+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-11-12T09:45:18.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s
[2024-11-12T09:45:18.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:18.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-11-12T09:45:18.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.533184 s
[2024-11-12T09:45:18.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO FileFormatWriter: Start to commit write Job 08462d8e-5f95-498c-b0b3-51c84cfb2f95.
[2024-11-12T09:45:18.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/18 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.18.6cded518-9abd-44da-9cde-753b957c700c.tmp
[2024-11-12T09:45:18.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.18.6cded518-9abd-44da-9cde-753b957c700c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/18
[2024-11-12T09:45:18.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO ManifestFileCommitProtocol: Committed batch 18
[2024-11-12T09:45:18.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO FileFormatWriter: Write Job 08462d8e-5f95-498c-b0b3-51c84cfb2f95 committed. Elapsed time: 80 ms.
[2024-11-12T09:45:18.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO FileFormatWriter: Finished processing stats for write job 08462d8e-5f95-498c-b0b3-51c84cfb2f95.
[2024-11-12T09:45:18.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/18 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.18.bb38c587-f2ad-4083-be8f-9ea6075a54f5.tmp
[2024-11-12T09:45:18.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.18.bb38c587-f2ad-4083-be8f-9ea6075a54f5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/18
[2024-11-12T09:45:18.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:18.226+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:18.226+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:18.227+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:18.227+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:17.254Z",
[2024-11-12T09:45:18.228+0000] {spark_submit.py:495} INFO - "batchId" : 18,
[2024-11-12T09:45:18.228+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:18.228+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0741138560687433,
[2024-11-12T09:45:18.229+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0351966873706004,
[2024-11-12T09:45:18.229+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:18.229+0000] {spark_submit.py:495} INFO - "addBatch" : 754,
[2024-11-12T09:45:18.230+0000] {spark_submit.py:495} INFO - "commitOffsets" : 72,
[2024-11-12T09:45:18.230+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:18.230+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:45:18.230+0000] {spark_submit.py:495} INFO - "queryPlanning" : 48,
[2024-11-12T09:45:18.230+0000] {spark_submit.py:495} INFO - "triggerExecution" : 966,
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "walCommit" : 79
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "0" : 604
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:18.231+0000] {spark_submit.py:495} INFO - "0" : 605
[2024-11-12T09:45:18.232+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:18.232+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:18.232+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:18.232+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:18.232+0000] {spark_submit.py:495} INFO - "0" : 605
[2024-11-12T09:45:18.232+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:18.233+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:18.233+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:18.233+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0741138560687433,
[2024-11-12T09:45:18.234+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0351966873706004,
[2024-11-12T09:45:18.234+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:18.234+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:18.239+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:18.242+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:18.243+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:18.244+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:18.245+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:18.245+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:18.245+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:18.246+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:18.246+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:18.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/19 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.19.592b1b5a-669a-440a-8948-4b2ca43e8e10.tmp
[2024-11-12T09:45:18.315+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.19.592b1b5a-669a-440a-8948-4b2ca43e8e10.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/19
[2024-11-12T09:45:18.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1731404718229,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:18.332+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:18.336+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:18.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:18.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:18.396+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 18, 18
[2024-11-12T09:45:18.399+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:18.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:18.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:18.437+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:18.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:18.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:18.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:18.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:18.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:18.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:18.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:18.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:18.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-11-12T09:45:18.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:18.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:18 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:19.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 589 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:19.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-11-12T09:45:19.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 0.632 s
[2024-11-12T09:45:19.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:19.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-11-12T09:45:19.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.641341 s
[2024-11-12T09:45:19.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO FileFormatWriter: Start to commit write Job 364ec0a7-6460-4850-a0b4-f7b69576ef81.
[2024-11-12T09:45:19.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/19.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.19.compact.d00a61cf-039c-4905-8671-8eccfd307953.tmp
[2024-11-12T09:45:19.300+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.19.compact.d00a61cf-039c-4905-8671-8eccfd307953.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/19.compact
[2024-11-12T09:45:19.301+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO ManifestFileCommitProtocol: Committed batch 19
[2024-11-12T09:45:19.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO FileFormatWriter: Write Job 364ec0a7-6460-4850-a0b4-f7b69576ef81 committed. Elapsed time: 227 ms.
[2024-11-12T09:45:19.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO FileFormatWriter: Finished processing stats for write job 364ec0a7-6460-4850-a0b4-f7b69576ef81.
[2024-11-12T09:45:19.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/19 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.19.e6ce62fd-667e-4099-a805-8478568c54fc.tmp
[2024-11-12T09:45:19.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.19.e6ce62fd-667e-4099-a805-8478568c54fc.tmp to hdfs://namenode:9000/spark_checkpoint/commits/19
[2024-11-12T09:45:19.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:19.363+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:19.364+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:19.364+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:19.364+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:18.222Z",
[2024-11-12T09:45:19.365+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-12T09:45:19.365+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0330578512396695,
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8802816901408451,
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "addBatch" : 955,
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "commitOffsets" : 57,
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:19.366+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:45:19.367+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:45:19.368+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1136,
[2024-11-12T09:45:19.368+0000] {spark_submit.py:495} INFO - "walCommit" : 86
[2024-11-12T09:45:19.369+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:19.369+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:19.369+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:19.369+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:19.370+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:19.370+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:19.370+0000] {spark_submit.py:495} INFO - "0" : 605
[2024-11-12T09:45:19.370+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:19.371+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:19.371+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:19.371+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:19.373+0000] {spark_submit.py:495} INFO - "0" : 606
[2024-11-12T09:45:19.373+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:19.373+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:19.373+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:19.374+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:19.374+0000] {spark_submit.py:495} INFO - "0" : 606
[2024-11-12T09:45:19.374+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:19.374+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:19.374+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:19.375+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0330578512396695,
[2024-11-12T09:45:19.379+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8802816901408451,
[2024-11-12T09:45:19.379+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:19.380+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:19.381+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:19.383+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:19.384+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:19.384+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:19.385+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:19.385+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:19.386+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:19.386+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:19.387+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:19.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/20 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.20.46035167-8d58-42b2-8a76-16fe4b6893ca.tmp
[2024-11-12T09:45:19.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.20.46035167-8d58-42b2-8a76-16fe4b6893ca.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/20
[2024-11-12T09:45:19.433+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1731404719366,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:19.455+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:19.456+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:19.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:19.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:19.492+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 18, 18, 19
[2024-11-12T09:45:19.498+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:19.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:19.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:19.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:19.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:19.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:19.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[84] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:19.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:19.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:19.565+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:19.565+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:19.566+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[84] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:19.566+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-11-12T09:45:19.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:19.605+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:19 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:20.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 508 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:20.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-11-12T09:45:20.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s
[2024-11-12T09:45:20.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:20.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-11-12T09:45:20.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.543209 s
[2024-11-12T09:45:20.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO FileFormatWriter: Start to commit write Job b7c6b902-62db-47c6-b0aa-68a629c5c0f0.
[2024-11-12T09:45:20.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/20 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.20.40663747-8b6d-49b6-b049-76b194ad923f.tmp
[2024-11-12T09:45:20.153+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.20.40663747-8b6d-49b6-b049-76b194ad923f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/20
[2024-11-12T09:45:20.154+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO ManifestFileCommitProtocol: Committed batch 20
[2024-11-12T09:45:20.154+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO FileFormatWriter: Write Job b7c6b902-62db-47c6-b0aa-68a629c5c0f0 committed. Elapsed time: 75 ms.
[2024-11-12T09:45:20.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO FileFormatWriter: Finished processing stats for write job b7c6b902-62db-47c6-b0aa-68a629c5c0f0.
[2024-11-12T09:45:20.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/20 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.20.a60b0844-508e-4b1a-b9d4-3d7b3b14a63a.tmp
[2024-11-12T09:45:20.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.20.a60b0844-508e-4b1a-b9d4-3d7b3b14a63a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/20
[2024-11-12T09:45:20.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:20.644+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:20.644+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:20.644+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:20.644+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:19.362Z",
[2024-11-12T09:45:20.644+0000] {spark_submit.py:495} INFO - "batchId" : 20,
[2024-11-12T09:45:20.644+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8771929824561404,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7836990595611285,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "addBatch" : 693,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "commitOffsets" : 484,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:45:20.645+0000] {spark_submit.py:495} INFO - "queryPlanning" : 25,
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1276,
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - "0" : 606
[2024-11-12T09:45:20.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "0" : 607
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "0" : 607
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8771929824561404,
[2024-11-12T09:45:20.647+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7836990595611285,
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:20.648+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:20.649+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:20.674+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/21 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.21.13c60f70-b199-45a5-9a9a-73301d770e33.tmp
[2024-11-12T09:45:20.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.21.13c60f70-b199-45a5-9a9a-73301d770e33.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/21
[2024-11-12T09:45:20.738+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1731404720660,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:20.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:20.771+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:20.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:20.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:20.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 18, 19, 20, 20
[2024-11-12T09:45:20.818+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:20.857+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:20.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:20.863+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:20.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:20.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:20.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[88] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:20.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Removed broadcast_20_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:20.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:20.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Removed broadcast_18_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:20.899+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:20.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:20.911+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:45:20.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:20.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:20.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:20.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:20.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[88] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:20.939+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-11-12T09:45:20.940+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:20.974+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:20 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:21.128+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 188 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:21.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-11-12T09:45:21.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 0.263 s
[2024-11-12T09:45:21.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:21.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-11-12T09:45:21.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.276349 s
[2024-11-12T09:45:21.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO FileFormatWriter: Start to commit write Job c921e2bc-c616-4905-bcce-cd21bc9ca9cb.
[2024-11-12T09:45:21.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/21 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.21.3a61a6da-c48e-4843-a50d-9960c4312fe3.tmp
[2024-11-12T09:45:21.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.21.3a61a6da-c48e-4843-a50d-9960c4312fe3.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/21
[2024-11-12T09:45:21.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO ManifestFileCommitProtocol: Committed batch 21
[2024-11-12T09:45:21.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO FileFormatWriter: Write Job c921e2bc-c616-4905-bcce-cd21bc9ca9cb committed. Elapsed time: 474 ms.
[2024-11-12T09:45:21.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO FileFormatWriter: Finished processing stats for write job c921e2bc-c616-4905-bcce-cd21bc9ca9cb.
[2024-11-12T09:45:21.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/21 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.21.7240fd01-f921-4117-a30e-79e16bfec5ee.tmp
[2024-11-12T09:45:21.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.21.7240fd01-f921-4117-a30e-79e16bfec5ee.tmp to hdfs://namenode:9000/spark_checkpoint/commits/21
[2024-11-12T09:45:21.670+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:21.671+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:21.672+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:21.673+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:21.673+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:20.642Z",
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "batchId" : 21,
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.78125,
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9737098344693282,
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "addBatch" : 833,
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "commitOffsets" : 56,
[2024-11-12T09:45:21.674+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:21.675+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-12T09:45:21.675+0000] {spark_submit.py:495} INFO - "queryPlanning" : 35,
[2024-11-12T09:45:21.676+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1027,
[2024-11-12T09:45:21.676+0000] {spark_submit.py:495} INFO - "walCommit" : 77
[2024-11-12T09:45:21.677+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:21.678+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:21.679+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:21.679+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:21.681+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:21.682+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:21.682+0000] {spark_submit.py:495} INFO - "0" : 607
[2024-11-12T09:45:21.687+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:21.688+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:21.688+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:21.689+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:21.690+0000] {spark_submit.py:495} INFO - "0" : 608
[2024-11-12T09:45:21.691+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:21.692+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:21.692+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:21.693+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:21.694+0000] {spark_submit.py:495} INFO - "0" : 608
[2024-11-12T09:45:21.694+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:21.695+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:21.696+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:21.696+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.78125,
[2024-11-12T09:45:21.697+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9737098344693282,
[2024-11-12T09:45:21.698+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:21.702+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:21.703+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:21.703+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:21.704+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:21.704+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:21.704+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:21.705+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:21.705+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:21.705+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:21.706+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:21.708+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/22 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.22.7e5488f4-b7d3-4f88-9f8f-0d6a814454e0.tmp
[2024-11-12T09:45:21.759+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.22.7e5488f4-b7d3-4f88-9f8f-0d6a814454e0.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/22
[2024-11-12T09:45:21.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1731404721678,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:21.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:21.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:21.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:21.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:21.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 20, 21, 21
[2024-11-12T09:45:21.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:21.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:21.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:21.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:21.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:21.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:21.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:21.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:21.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:21.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:21.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:21.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:21.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-11-12T09:45:21.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:21.966+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:21 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:22.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 258 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:22.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-11-12T09:45:22.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 0.298 s
[2024-11-12T09:45:22.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:22.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2024-11-12T09:45:22.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.319005 s
[2024-11-12T09:45:22.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO FileFormatWriter: Start to commit write Job 9ecd8383-a4fa-49cd-bc0a-f140149b1f84.
[2024-11-12T09:45:22.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/22 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.22.a6453ab3-d427-46c1-af6e-033c7d21f084.tmp
[2024-11-12T09:45:22.673+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.22.a6453ab3-d427-46c1-af6e-033c7d21f084.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/22
[2024-11-12T09:45:22.674+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO ManifestFileCommitProtocol: Committed batch 22
[2024-11-12T09:45:22.675+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO FileFormatWriter: Write Job 9ecd8383-a4fa-49cd-bc0a-f140149b1f84 committed. Elapsed time: 482 ms.
[2024-11-12T09:45:22.675+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO FileFormatWriter: Finished processing stats for write job 9ecd8383-a4fa-49cd-bc0a-f140149b1f84.
[2024-11-12T09:45:22.690+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/22 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.22.2515b4fd-b583-4a2a-91a3-5d82a1c96633.tmp
[2024-11-12T09:45:22.761+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.22.2515b4fd-b583-4a2a-91a3-5d82a1c96633.tmp to hdfs://namenode:9000/spark_checkpoint/commits/22
[2024-11-12T09:45:22.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:22.765+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:22.766+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:22.766+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:22.766+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:21.670Z",
[2024-11-12T09:45:22.766+0000] {spark_submit.py:495} INFO - "batchId" : 22,
[2024-11-12T09:45:22.766+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:22.766+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9727626459143969,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9165902841429882,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "addBatch" : 867,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "commitOffsets" : 88,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "getBatch" : 4,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "queryPlanning" : 35,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1091,
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - "walCommit" : 82
[2024-11-12T09:45:22.767+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "0" : 608
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:22.768+0000] {spark_submit.py:495} INFO - "0" : 609
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - "0" : 609
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:22.769+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:22.770+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:22.771+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9727626459143969,
[2024-11-12T09:45:22.771+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9165902841429882,
[2024-11-12T09:45:22.771+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:22.772+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:22.772+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:22.772+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:22.772+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:22.773+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:22.773+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:22.773+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:22.773+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:22.774+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:22.774+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:22.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/23 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.23.49c5940f-645e-4dbd-8e87-2c03c8f5848a.tmp
[2024-11-12T09:45:22.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.23.49c5940f-645e-4dbd-8e87-2c03c8f5848a.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/23
[2024-11-12T09:45:22.854+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1731404722773,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:22.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:22.894+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:22.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:22.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:22.943+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 21, 22, 22
[2024-11-12T09:45:22.945+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:22.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:22.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:22.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:22.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:22.985+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:22.985+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:22 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:23.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:23.020+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:23.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:23.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:23.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:23.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2024-11-12T09:45:23.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:23.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:23.628+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 601 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:23.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-11-12T09:45:23.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.644 s
[2024-11-12T09:45:23.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:23.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2024-11-12T09:45:23.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.648867 s
[2024-11-12T09:45:23.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO FileFormatWriter: Start to commit write Job 4a823e40-3248-47d2-ba8d-c5794f93bda5.
[2024-11-12T09:45:23.641+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/23 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.23.38ff6f8c-331f-4cbb-b57f-689ad4b7e250.tmp
[2024-11-12T09:45:23.692+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.23.38ff6f8c-331f-4cbb-b57f-689ad4b7e250.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/23
[2024-11-12T09:45:23.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO ManifestFileCommitProtocol: Committed batch 23
[2024-11-12T09:45:23.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO FileFormatWriter: Write Job 4a823e40-3248-47d2-ba8d-c5794f93bda5 committed. Elapsed time: 60 ms.
[2024-11-12T09:45:23.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO FileFormatWriter: Finished processing stats for write job 4a823e40-3248-47d2-ba8d-c5794f93bda5.
[2024-11-12T09:45:23.701+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/23 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.23.d6d59f75-01c2-452c-beca-2b6ada1f722d.tmp
[2024-11-12T09:45:23.743+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.23.d6d59f75-01c2-452c-beca-2b6ada1f722d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/23
[2024-11-12T09:45:23.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:23.745+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:23.745+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:23.745+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:23.745+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:22.765Z",
[2024-11-12T09:45:23.745+0000] {spark_submit.py:495} INFO - "batchId" : 23,
[2024-11-12T09:45:23.746+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:23.746+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9132420091324202,
[2024-11-12T09:45:23.746+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0235414534288638,
[2024-11-12T09:45:23.746+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:23.746+0000] {spark_submit.py:495} INFO - "addBatch" : 792,
[2024-11-12T09:45:23.747+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:45:23.747+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:23.747+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:45:23.748+0000] {spark_submit.py:495} INFO - "queryPlanning" : 43,
[2024-11-12T09:45:23.748+0000] {spark_submit.py:495} INFO - "triggerExecution" : 977,
[2024-11-12T09:45:23.748+0000] {spark_submit.py:495} INFO - "walCommit" : 80
[2024-11-12T09:45:23.749+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:23.749+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:23.749+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:23.749+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:23.749+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:23.749+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:23.750+0000] {spark_submit.py:495} INFO - "0" : 609
[2024-11-12T09:45:23.750+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:23.750+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - "0" : 610
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:23.751+0000] {spark_submit.py:495} INFO - "0" : 610
[2024-11-12T09:45:23.752+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:23.752+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:23.752+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:23.752+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9132420091324202,
[2024-11-12T09:45:23.752+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0235414534288638,
[2024-11-12T09:45:23.752+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:23.753+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:23.753+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:23.753+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:23.753+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:23.754+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:23.755+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:23.756+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:23.756+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:23.756+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:23.756+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:23.763+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/24 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.24.2679b1bf-4394-4026-8b31-9dbb5dd9f4c3.tmp
[2024-11-12T09:45:23.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.24.2679b1bf-4394-4026-8b31-9dbb5dd9f4c3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/24
[2024-11-12T09:45:23.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1731404723752,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:23.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:23.838+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:23.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:23.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:23.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 23, 23
[2024-11-12T09:45:23.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:23.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:23.934+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:23.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:23.948+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:23.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:23.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:23.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:23.953+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Removed broadcast_22_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:23.954+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[100] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:23.958+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:23.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:23 INFO BlockManagerInfo: Removed broadcast_21_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:24.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:24.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:45:24.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:45:24.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:24.039+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:24.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[100] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:24.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-11-12T09:45:24.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:24.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:24.332+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 294 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:24.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-11-12T09:45:24.335+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.379 s
[2024-11-12T09:45:24.336+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:24.336+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-11-12T09:45:24.338+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.393905 s
[2024-11-12T09:45:24.339+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO FileFormatWriter: Start to commit write Job e4e9ca34-f91a-4c90-ab83-e03ee31ab6a0.
[2024-11-12T09:45:24.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/24 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.24.69471380-e1ec-4b1f-9ce2-53dbaf3da7fc.tmp
[2024-11-12T09:45:24.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.24.69471380-e1ec-4b1f-9ce2-53dbaf3da7fc.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/24
[2024-11-12T09:45:24.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO ManifestFileCommitProtocol: Committed batch 24
[2024-11-12T09:45:24.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO FileFormatWriter: Write Job e4e9ca34-f91a-4c90-ab83-e03ee31ab6a0 committed. Elapsed time: 65 ms.
[2024-11-12T09:45:24.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO FileFormatWriter: Finished processing stats for write job e4e9ca34-f91a-4c90-ab83-e03ee31ab6a0.
[2024-11-12T09:45:24.424+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/24 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.24.be27d2b0-da65-4afd-9ea7-ac4f524073e3.tmp
[2024-11-12T09:45:24.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.24.be27d2b0-da65-4afd-9ea7-ac4f524073e3.tmp to hdfs://namenode:9000/spark_checkpoint/commits/24
[2024-11-12T09:45:24.470+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:24.471+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:24.471+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:24.472+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:24.472+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:23.744Z",
[2024-11-12T09:45:24.472+0000] {spark_submit.py:495} INFO - "batchId" : 24,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0214504596527068,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3812154696132597,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "addBatch" : 560,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "commitOffsets" : 63,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "queryPlanning" : 34,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "triggerExecution" : 724,
[2024-11-12T09:45:24.473+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:45:24.474+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:24.474+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:24.474+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:24.474+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:24.475+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:24.475+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:24.476+0000] {spark_submit.py:495} INFO - "0" : 610
[2024-11-12T09:45:24.483+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:24.486+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:24.486+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:24.487+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:24.487+0000] {spark_submit.py:495} INFO - "0" : 611
[2024-11-12T09:45:24.487+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:24.488+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:24.488+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:24.488+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:24.488+0000] {spark_submit.py:495} INFO - "0" : 611
[2024-11-12T09:45:24.489+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:24.489+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:24.489+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:24.489+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0214504596527068,
[2024-11-12T09:45:24.489+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3812154696132597,
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:24.490+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:24.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/25 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.25.f7cef687-7d54-4299-9b54-36a794029b36.tmp
[2024-11-12T09:45:24.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.25.f7cef687-7d54-4299-9b54-36a794029b36.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/25
[2024-11-12T09:45:24.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1731404724488,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:24.581+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:24.584+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:24.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:24.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:24.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 24, 24
[2024-11-12T09:45:24.624+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:24.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:24.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:24.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:24.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:24.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:24.665+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:24.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:24.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:24.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:24.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:24.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:24.705+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-11-12T09:45:24.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:24.734+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:24 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:25.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 416 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:25.124+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-11-12T09:45:25.124+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 0.456 s
[2024-11-12T09:45:25.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:25.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-11-12T09:45:25.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.473207 s
[2024-11-12T09:45:25.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO FileFormatWriter: Start to commit write Job 3e15fb54-2837-4a7d-9e81-a5ad51735223.
[2024-11-12T09:45:25.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/25 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.25.323415ec-1bf2-428f-b3a7-834247e29727.tmp
[2024-11-12T09:45:25.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.25.323415ec-1bf2-428f-b3a7-834247e29727.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/25
[2024-11-12T09:45:25.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO ManifestFileCommitProtocol: Committed batch 25
[2024-11-12T09:45:25.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO FileFormatWriter: Write Job 3e15fb54-2837-4a7d-9e81-a5ad51735223 committed. Elapsed time: 56 ms.
[2024-11-12T09:45:25.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO FileFormatWriter: Finished processing stats for write job 3e15fb54-2837-4a7d-9e81-a5ad51735223.
[2024-11-12T09:45:25.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/25 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.25.f3e9c2a6-c31c-4073-bdd8-069d05abd883.tmp
[2024-11-12T09:45:25.257+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.25.f3e9c2a6-c31c-4073-bdd8-069d05abd883.tmp to hdfs://namenode:9000/spark_checkpoint/commits/25
[2024-11-12T09:45:25.265+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:25.267+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:25.267+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:25.268+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:25.268+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:24.470Z",
[2024-11-12T09:45:25.268+0000] {spark_submit.py:495} INFO - "batchId" : 25,
[2024-11-12T09:45:25.272+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:25.272+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3774104683195594,
[2024-11-12T09:45:25.273+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2706480304955527,
[2024-11-12T09:45:25.273+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "addBatch" : 598,
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "commitOffsets" : 69,
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "queryPlanning" : 33,
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "triggerExecution" : 787,
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:25.274+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:25.275+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:25.275+0000] {spark_submit.py:495} INFO - "0" : 611
[2024-11-12T09:45:25.275+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:25.275+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:25.276+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:25.276+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:25.276+0000] {spark_submit.py:495} INFO - "0" : 612
[2024-11-12T09:45:25.276+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:25.276+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:25.277+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:25.277+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:25.277+0000] {spark_submit.py:495} INFO - "0" : 612
[2024-11-12T09:45:25.278+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:25.278+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:25.278+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:25.278+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3774104683195594,
[2024-11-12T09:45:25.279+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2706480304955527,
[2024-11-12T09:45:25.279+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:25.279+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:25.279+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:25.280+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:25.280+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:25.281+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:25.281+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:25.281+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:25.282+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:25.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:25.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:25.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/26 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.26.6bab04ba-e1e4-4c5f-9d63-4bc474897a8d.tmp
[2024-11-12T09:45:25.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.26.6bab04ba-e1e4-4c5f-9d63-4bc474897a8d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/26
[2024-11-12T09:45:25.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1731404725278,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:25.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:25.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:25.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:25.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:25.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 25
[2024-11-12T09:45:25.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:25.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:25.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:25.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:25.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:25.462+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:25.462+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[108] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:25.492+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:25.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:25.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:25.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:25.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[108] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:25.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2024-11-12T09:45:25.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:25.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:25 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:26.154+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 649 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:26.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-11-12T09:45:26.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.693 s
[2024-11-12T09:45:26.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:26.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2024-11-12T09:45:26.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.696480 s
[2024-11-12T09:45:26.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO FileFormatWriter: Start to commit write Job fed65ae0-a56a-4fa1-89f2-01c2888ad58d.
[2024-11-12T09:45:26.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/26 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.26.b798db36-157b-4b06-bd2d-21eeae60b2d5.tmp
[2024-11-12T09:45:26.221+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.26.b798db36-157b-4b06-bd2d-21eeae60b2d5.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/26
[2024-11-12T09:45:26.222+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO ManifestFileCommitProtocol: Committed batch 26
[2024-11-12T09:45:26.222+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO FileFormatWriter: Write Job fed65ae0-a56a-4fa1-89f2-01c2888ad58d committed. Elapsed time: 65 ms.
[2024-11-12T09:45:26.228+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO FileFormatWriter: Finished processing stats for write job fed65ae0-a56a-4fa1-89f2-01c2888ad58d.
[2024-11-12T09:45:26.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/26 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.26.95aaa673-a863-4fbc-9f5f-cb7d7782f7b0.tmp
[2024-11-12T09:45:26.300+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.26.95aaa673-a863-4fbc-9f5f-cb7d7782f7b0.tmp to hdfs://namenode:9000/spark_checkpoint/commits/26
[2024-11-12T09:45:26.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:26.305+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:26.305+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:26.305+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:25.263Z",
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "batchId" : 26,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2610340479192939,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9643201542912248,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "addBatch" : 827,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "commitOffsets" : 78,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:45:26.306+0000] {spark_submit.py:495} INFO - "queryPlanning" : 36,
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1037,
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "walCommit" : 76
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "0" : 612
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:26.307+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - "0" : 613
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - "0" : 613
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:26.308+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2610340479192939,
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9643201542912248,
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:26.309+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:26.310+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:26.310+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:26.310+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:26.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/27 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.27.ec1d714b-8d53-46b2-8fec-eb9d08c32d04.tmp
[2024-11-12T09:45:26.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.27.ec1d714b-8d53-46b2-8fec-eb9d08c32d04.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/27
[2024-11-12T09:45:26.808+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1731404726311,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:26.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:26.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:26.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:26.880+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:26.889+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 25, 26, 26
[2024-11-12T09:45:26.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:26.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO BlockManagerInfo: Removed broadcast_24_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:26.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:26.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:26.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:26.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:26.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:26.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:26.972+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[112] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:26.978+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO BlockManagerInfo: Removed broadcast_25_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:26.989+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:26 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:27.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:27.030+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:27.036+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:27.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:27.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:27.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[112] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:27.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-11-12T09:45:27.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:27.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:27.093+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:27.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 272 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:27.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-11-12T09:45:27.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.347 s
[2024-11-12T09:45:27.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:27.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-11-12T09:45:27.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.359754 s
[2024-11-12T09:45:27.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO FileFormatWriter: Start to commit write Job dcc5f384-3e6b-403e-ba87-8b2129ed8e22.
[2024-11-12T09:45:27.338+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/27 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.27.89b65f5f-0ec5-4247-a3af-272a911a0be4.tmp
[2024-11-12T09:45:27.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.27.89b65f5f-0ec5-4247-a3af-272a911a0be4.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/27
[2024-11-12T09:45:27.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO ManifestFileCommitProtocol: Committed batch 27
[2024-11-12T09:45:27.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO FileFormatWriter: Write Job dcc5f384-3e6b-403e-ba87-8b2129ed8e22 committed. Elapsed time: 83 ms.
[2024-11-12T09:45:27.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO FileFormatWriter: Finished processing stats for write job dcc5f384-3e6b-403e-ba87-8b2129ed8e22.
[2024-11-12T09:45:27.415+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/27 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.27.cbc3ab3b-8e9e-4075-9260-09e1d9c6b8b1.tmp
[2024-11-12T09:45:27.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.27.cbc3ab3b-8e9e-4075-9260-09e1d9c6b8b1.tmp to hdfs://namenode:9000/spark_checkpoint/commits/27
[2024-11-12T09:45:27.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:27.873+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:27.874+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:27.874+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:27.874+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:26.305Z",
[2024-11-12T09:45:27.875+0000] {spark_submit.py:495} INFO - "batchId" : 27,
[2024-11-12T09:45:27.875+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:27.875+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9596928982725528,
[2024-11-12T09:45:27.876+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6397952655150352,
[2024-11-12T09:45:27.877+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:27.877+0000] {spark_submit.py:495} INFO - "addBatch" : 551,
[2024-11-12T09:45:27.877+0000] {spark_submit.py:495} INFO - "commitOffsets" : 464,
[2024-11-12T09:45:27.877+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:27.878+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:45:27.878+0000] {spark_submit.py:495} INFO - "queryPlanning" : 45,
[2024-11-12T09:45:27.878+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1563,
[2024-11-12T09:45:27.878+0000] {spark_submit.py:495} INFO - "walCommit" : 485
[2024-11-12T09:45:27.879+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:27.880+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:27.880+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:27.880+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:27.880+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:27.880+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - "0" : 613
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - "0" : 614
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:27.881+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - "0" : 614
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9596928982725528,
[2024-11-12T09:45:27.882+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6397952655150352,
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:27.883+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:27.884+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:27.884+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:27.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/28 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.28.aebc0199-91e2-48cf-82aa-2d72238e253f.tmp
[2024-11-12T09:45:27.929+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.28.aebc0199-91e2-48cf-82aa-2d72238e253f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/28
[2024-11-12T09:45:27.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1731404727877,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:27.957+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:27.958+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:27.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:27.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:28.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27, 27
[2024-11-12T09:45:28.009+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:28.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:28.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:28.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Final stage: ResultStage 28 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:28.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:28.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:28.057+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[116] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:28.096+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:28.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:28.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:28.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:28.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[116] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:28.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2024-11-12T09:45:28.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:28.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:28.277+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 173 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:28.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-11-12T09:45:28.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: ResultStage 28 (start at NativeMethodAccessorImpl.java:0) finished in 0.219 s
[2024-11-12T09:45:28.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:28.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2024-11-12T09:45:28.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.235819 s
[2024-11-12T09:45:28.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO FileFormatWriter: Start to commit write Job e01832d2-e89d-41a0-8706-5b1334daba51.
[2024-11-12T09:45:28.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/28 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.28.a0034bf9-6615-4478-90ba-98a08bea6d5c.tmp
[2024-11-12T09:45:28.332+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.28.a0034bf9-6615-4478-90ba-98a08bea6d5c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/28
[2024-11-12T09:45:28.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO ManifestFileCommitProtocol: Committed batch 28
[2024-11-12T09:45:28.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO FileFormatWriter: Write Job e01832d2-e89d-41a0-8706-5b1334daba51 committed. Elapsed time: 52 ms.
[2024-11-12T09:45:28.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO FileFormatWriter: Finished processing stats for write job e01832d2-e89d-41a0-8706-5b1334daba51.
[2024-11-12T09:45:28.341+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/28 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.28.fbf62d6f-70d0-4015-9407-9c978d395a08.tmp
[2024-11-12T09:45:28.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.28.fbf62d6f-70d0-4015-9407-9c978d395a08.tmp to hdfs://namenode:9000/spark_checkpoint/commits/28
[2024-11-12T09:45:28.379+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:28.380+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:28.381+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:28.381+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:28.381+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:27.873Z",
[2024-11-12T09:45:28.381+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-12T09:45:28.382+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:28.382+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6377551020408163,
[2024-11-12T09:45:28.382+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.9841269841269842,
[2024-11-12T09:45:28.382+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:28.382+0000] {spark_submit.py:495} INFO - "addBatch" : 369,
[2024-11-12T09:45:28.382+0000] {spark_submit.py:495} INFO - "commitOffsets" : 44,
[2024-11-12T09:45:28.383+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:28.383+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:45:28.383+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:45:28.383+0000] {spark_submit.py:495} INFO - "triggerExecution" : 504,
[2024-11-12T09:45:28.384+0000] {spark_submit.py:495} INFO - "walCommit" : 53
[2024-11-12T09:45:28.385+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:28.387+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:28.387+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "0" : 614
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - "0" : 615
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:28.388+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:28.389+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:28.389+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:28.389+0000] {spark_submit.py:495} INFO - "0" : 615
[2024-11-12T09:45:28.390+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:28.390+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:28.390+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:28.390+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6377551020408163,
[2024-11-12T09:45:28.390+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.9841269841269842,
[2024-11-12T09:45:28.390+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:28.391+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:28.391+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:28.392+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:28.392+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:28.392+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:28.393+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:28.393+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:28.393+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:28.393+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:28.393+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:28.402+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/29 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.29.0a6984e3-cec0-40af-bd79-29ff01a14720.tmp
[2024-11-12T09:45:28.449+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.29.0a6984e3-cec0-40af-bd79-29ff01a14720.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/29
[2024-11-12T09:45:28.450+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1731404728394,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:28.464+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:28.466+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:28.494+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:28.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:28.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 27, 28, 28
[2024-11-12T09:45:28.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:28.559+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:28.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:28.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:28.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:28.561+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:28.561+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:28.587+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:28.589+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:28.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:28.591+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:28.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[120] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:28.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2024-11-12T09:45:28.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:28.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:29.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 595 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:29.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2024-11-12T09:45:29.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.632 s
[2024-11-12T09:45:29.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:29.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2024-11-12T09:45:29.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.635654 s
[2024-11-12T09:45:29.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO FileFormatWriter: Start to commit write Job d1ca68d4-a2d5-4714-87b0-58bf96d9027a.
[2024-11-12T09:45:29.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/29.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.29.compact.3925a59b-c897-4a51-82d9-630ca54ecc37.tmp
[2024-11-12T09:45:29.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.29.compact.3925a59b-c897-4a51-82d9-630ca54ecc37.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/29.compact
[2024-11-12T09:45:29.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO ManifestFileCommitProtocol: Committed batch 29
[2024-11-12T09:45:29.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO FileFormatWriter: Write Job d1ca68d4-a2d5-4714-87b0-58bf96d9027a committed. Elapsed time: 204 ms.
[2024-11-12T09:45:29.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO FileFormatWriter: Finished processing stats for write job d1ca68d4-a2d5-4714-87b0-58bf96d9027a.
[2024-11-12T09:45:29.414+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/29 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.29.764e9fb9-7e4e-4e4b-98a4-aaf93482d1d4.tmp
[2024-11-12T09:45:29.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.29.764e9fb9-7e4e-4e4b-98a4-aaf93482d1d4.tmp to hdfs://namenode:9000/spark_checkpoint/commits/29
[2024-11-12T09:45:29.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:29.864+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:29.865+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:29.865+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:29.865+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:28.379Z",
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "batchId" : 29,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.976284584980237,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6747638326585695,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "addBatch" : 925,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "commitOffsets" : 456,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:45:29.866+0000] {spark_submit.py:495} INFO - "queryPlanning" : 23,
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1482,
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "walCommit" : 56
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - "0" : 615
[2024-11-12T09:45:29.867+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - "0" : 616
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:29.868+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:29.869+0000] {spark_submit.py:495} INFO - "0" : 616
[2024-11-12T09:45:29.869+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:29.869+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:29.869+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.976284584980237,
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6747638326585695,
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:29.870+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:29.872+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:29.873+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:29.873+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:29.874+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:29.874+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:29.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/30 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.30.49b49cf3-def0-42e0-ae17-120f2e094052.tmp
[2024-11-12T09:45:29.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.30.49b49cf3-def0-42e0-ae17-120f2e094052.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/30
[2024-11-12T09:45:29.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1731404729867,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:29.980+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:29.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.004+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.015+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 27, 28, 28, 29
[2024-11-12T09:45:30.020+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:30.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:30.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:30.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Removed broadcast_29_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:30.096+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:30.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:30.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:30.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:30.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:30.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[124] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:30.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:30.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Removed broadcast_28_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:30.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:30.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:45:30.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:45:30.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:30.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:30.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[124] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:30.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2024-11-12T09:45:30.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:30.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:30.344+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 199 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:30.344+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2024-11-12T09:45:30.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.246 s
[2024-11-12T09:45:30.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:30.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2024-11-12T09:45:30.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.249862 s
[2024-11-12T09:45:30.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO FileFormatWriter: Start to commit write Job b7d7e002-8f8f-4baf-9b2a-09973764a761.
[2024-11-12T09:45:30.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/30 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.30.2d00b2e4-8a31-4ac3-83b3-c48e7394c744.tmp
[2024-11-12T09:45:30.409+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.30.2d00b2e4-8a31-4ac3-83b3-c48e7394c744.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/30
[2024-11-12T09:45:30.415+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO ManifestFileCommitProtocol: Committed batch 30
[2024-11-12T09:45:30.417+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO FileFormatWriter: Write Job b7d7e002-8f8f-4baf-9b2a-09973764a761 committed. Elapsed time: 62 ms.
[2024-11-12T09:45:30.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO FileFormatWriter: Finished processing stats for write job b7d7e002-8f8f-4baf-9b2a-09973764a761.
[2024-11-12T09:45:30.425+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/30 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.30.4fc7956c-daf6-41db-9b56-54515e0b9404.tmp
[2024-11-12T09:45:30.466+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.30.4fc7956c-daf6-41db-9b56-54515e0b9404.tmp to hdfs://namenode:9000/spark_checkpoint/commits/30
[2024-11-12T09:45:30.467+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:29.863Z",
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "batchId" : 30,
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6738544474393531,
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.658374792703151,
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:30.468+0000] {spark_submit.py:495} INFO - "addBatch" : 426,
[2024-11-12T09:45:30.469+0000] {spark_submit.py:495} INFO - "commitOffsets" : 49,
[2024-11-12T09:45:30.469+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:30.469+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:45:30.469+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:45:30.469+0000] {spark_submit.py:495} INFO - "triggerExecution" : 603,
[2024-11-12T09:45:30.470+0000] {spark_submit.py:495} INFO - "walCommit" : 92
[2024-11-12T09:45:30.470+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:30.470+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:30.470+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:30.471+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:30.472+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:30.472+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:30.480+0000] {spark_submit.py:495} INFO - "0" : 616
[2024-11-12T09:45:30.484+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:30.485+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:30.485+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - "0" : 617
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - "0" : 617
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:30.486+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6738544474393531,
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.658374792703151,
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:30.487+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:30.488+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:30.488+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:30.488+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:30.488+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:30.488+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:30.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/31 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.31.bb9f0026-1d54-4b09-bf30-580eafef451c.tmp
[2024-11-12T09:45:30.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.31.bb9f0026-1d54-4b09-bf30-580eafef451c.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/31
[2024-11-12T09:45:30.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1731404730472,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:30.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.577+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.581+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:30.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 29, 30, 30
[2024-11-12T09:45:30.598+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:30.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:30.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:30.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Final stage: ResultStage 31 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:30.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:30.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:30.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[128] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:30.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:30.667+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:30.668+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:30.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:30.670+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[128] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:30.671+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2024-11-12T09:45:30.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:30.699+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:30 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:31.182+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 509 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:31.184+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2024-11-12T09:45:31.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: ResultStage 31 (start at NativeMethodAccessorImpl.java:0) finished in 0.546 s
[2024-11-12T09:45:31.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:31.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
[2024-11-12T09:45:31.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.556333 s
[2024-11-12T09:45:31.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO FileFormatWriter: Start to commit write Job a6b5f7d3-f33c-4ae6-9af1-61034a92b96f.
[2024-11-12T09:45:31.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/31 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.31.ba74f35f-0df4-490e-851c-7545ada6e723.tmp
[2024-11-12T09:45:31.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.31.ba74f35f-0df4-490e-851c-7545ada6e723.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/31
[2024-11-12T09:45:31.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO ManifestFileCommitProtocol: Committed batch 31
[2024-11-12T09:45:31.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO FileFormatWriter: Write Job a6b5f7d3-f33c-4ae6-9af1-61034a92b96f committed. Elapsed time: 70 ms.
[2024-11-12T09:45:31.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO FileFormatWriter: Finished processing stats for write job a6b5f7d3-f33c-4ae6-9af1-61034a92b96f.
[2024-11-12T09:45:31.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/31 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.31.d44227bd-2285-412e-b51e-801917b89c5c.tmp
[2024-11-12T09:45:31.324+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.31.d44227bd-2285-412e-b51e-801917b89c5c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/31
[2024-11-12T09:45:31.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:31.329+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:31.330+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:31.330+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:31.331+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:30.467Z",
[2024-11-12T09:45:31.331+0000] {spark_submit.py:495} INFO - "batchId" : 31,
[2024-11-12T09:45:31.331+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:31.331+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.6556291390728477,
[2024-11-12T09:45:31.331+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1668611435239207,
[2024-11-12T09:45:31.333+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:31.334+0000] {spark_submit.py:495} INFO - "addBatch" : 700,
[2024-11-12T09:45:31.334+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-12T09:45:31.334+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:31.334+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:45:31.334+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:45:31.335+0000] {spark_submit.py:495} INFO - "triggerExecution" : 857,
[2024-11-12T09:45:31.335+0000] {spark_submit.py:495} INFO - "walCommit" : 58
[2024-11-12T09:45:31.335+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:31.335+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:31.336+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:31.336+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:31.336+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:31.336+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:31.337+0000] {spark_submit.py:495} INFO - "0" : 617
[2024-11-12T09:45:31.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:31.338+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:31.339+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:31.339+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:31.340+0000] {spark_submit.py:495} INFO - "0" : 618
[2024-11-12T09:45:31.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:31.340+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:31.344+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:31.345+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "0" : 618
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.6556291390728477,
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1668611435239207,
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:31.346+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:31.347+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:31.347+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:31.347+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:31.347+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:31.347+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:31.347+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:31.348+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:31.348+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:31.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/32 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.32.217c6a6e-9636-469b-bd63-65b7da175b71.tmp
[2024-11-12T09:45:31.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.32.217c6a6e-9636-469b-bd63-65b7da175b71.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/32
[2024-11-12T09:45:31.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1731404731337,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:31.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:31.416+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:31.433+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:31.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:31.450+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 31, 31
[2024-11-12T09:45:31.452+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:31.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:31.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:31.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:31.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:31.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:31.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[132] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:31.512+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:31.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:31.516+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:31.516+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:31.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[132] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:31.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2024-11-12T09:45:31.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:31.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:31 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:32.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 687 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:32.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2024-11-12T09:45:32.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.719 s
[2024-11-12T09:45:32.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:32.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2024-11-12T09:45:32.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.722993 s
[2024-11-12T09:45:32.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO FileFormatWriter: Start to commit write Job 66c2df12-2b85-45a0-b970-7413e067c379.
[2024-11-12T09:45:32.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/32 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.32.7971c1e0-886c-475d-9d61-1fba7c7f0368.tmp
[2024-11-12T09:45:32.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.32.7971c1e0-886c-475d-9d61-1fba7c7f0368.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/32
[2024-11-12T09:45:32.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO ManifestFileCommitProtocol: Committed batch 32
[2024-11-12T09:45:32.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO FileFormatWriter: Write Job 66c2df12-2b85-45a0-b970-7413e067c379 committed. Elapsed time: 521 ms.
[2024-11-12T09:45:32.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO FileFormatWriter: Finished processing stats for write job 66c2df12-2b85-45a0-b970-7413e067c379.
[2024-11-12T09:45:32.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/32 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.32.05dad9e7-2b98-4ace-84a5-1a0085db0203.tmp
[2024-11-12T09:45:32.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.32.05dad9e7-2b98-4ace-84a5-1a0085db0203.tmp to hdfs://namenode:9000/spark_checkpoint/commits/32
[2024-11-12T09:45:32.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:32.811+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:32.812+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:32.812+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:32.813+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:31.328Z",
[2024-11-12T09:45:32.813+0000] {spark_submit.py:495} INFO - "batchId" : 32,
[2024-11-12T09:45:32.814+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:32.814+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1614401858304297,
[2024-11-12T09:45:32.814+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.676132521974307,
[2024-11-12T09:45:32.814+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:32.814+0000] {spark_submit.py:495} INFO - "addBatch" : 1307,
[2024-11-12T09:45:32.815+0000] {spark_submit.py:495} INFO - "commitOffsets" : 75,
[2024-11-12T09:45:32.816+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:32.823+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:45:32.823+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:45:32.824+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1479,
[2024-11-12T09:45:32.824+0000] {spark_submit.py:495} INFO - "walCommit" : 51
[2024-11-12T09:45:32.824+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:32.825+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:32.828+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:32.829+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:32.830+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:32.830+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:32.830+0000] {spark_submit.py:495} INFO - "0" : 618
[2024-11-12T09:45:32.831+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:32.831+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:32.831+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:32.831+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - "0" : 619
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - "0" : 619
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:32.832+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1614401858304297,
[2024-11-12T09:45:32.833+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.676132521974307,
[2024-11-12T09:45:32.833+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:32.833+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:32.833+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:32.834+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:32.834+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:32.834+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:32.834+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:32.835+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:32.835+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:32.835+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:32.836+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:32.838+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/33 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.33.9199c8e1-1c8a-4289-a150-9f163699c230.tmp
[2024-11-12T09:45:32.907+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.33.9199c8e1-1c8a-4289-a150-9f163699c230.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/33
[2024-11-12T09:45:32.908+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1731404732824,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:32.936+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:32.936+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:32.966+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:32.970+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:32.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 31, 32, 32
[2024-11-12T09:45:32.980+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:32 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:33.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:33.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:33.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:33.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:33.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:33.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[136] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:33.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:45:33.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:45:33.118+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:33.118+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:33.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[136] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:33.120+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2024-11-12T09:45:33.124+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:33.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:33.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 259 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:33.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2024-11-12T09:45:33.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.327 s
[2024-11-12T09:45:33.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:33.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2024-11-12T09:45:33.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.345423 s
[2024-11-12T09:45:33.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO FileFormatWriter: Start to commit write Job 4b39b098-12ae-40a8-9ed3-7771fd6ab361.
[2024-11-12T09:45:33.400+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/33 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.33.1d6a9fb6-c547-42ae-a62b-d976b6f5fb05.tmp
[2024-11-12T09:45:33.469+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.33.1d6a9fb6-c547-42ae-a62b-d976b6f5fb05.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/33
[2024-11-12T09:45:33.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO ManifestFileCommitProtocol: Committed batch 33
[2024-11-12T09:45:33.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO FileFormatWriter: Write Job 4b39b098-12ae-40a8-9ed3-7771fd6ab361 committed. Elapsed time: 79 ms.
[2024-11-12T09:45:33.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO FileFormatWriter: Finished processing stats for write job 4b39b098-12ae-40a8-9ed3-7771fd6ab361.
[2024-11-12T09:45:33.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/33 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.33.e0aa1a5c-8f96-48c4-bd3e-68a35bde595e.tmp
[2024-11-12T09:45:33.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.33.e0aa1a5c-8f96-48c4-bd3e-68a35bde595e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/33
[2024-11-12T09:45:33.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:33.538+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:33.538+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:33.540+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:33.540+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:32.809Z",
[2024-11-12T09:45:33.540+0000] {spark_submit.py:495} INFO - "batchId" : 33,
[2024-11-12T09:45:33.540+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:33.540+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6752194463200539,
[2024-11-12T09:45:33.541+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3774104683195594,
[2024-11-12T09:45:33.541+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:33.543+0000] {spark_submit.py:495} INFO - "addBatch" : 525,
[2024-11-12T09:45:33.543+0000] {spark_submit.py:495} INFO - "commitOffsets" : 66,
[2024-11-12T09:45:33.544+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:33.547+0000] {spark_submit.py:495} INFO - "latestOffset" : 14,
[2024-11-12T09:45:33.547+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:45:33.548+0000] {spark_submit.py:495} INFO - "triggerExecution" : 726,
[2024-11-12T09:45:33.548+0000] {spark_submit.py:495} INFO - "walCommit" : 84
[2024-11-12T09:45:33.548+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:33.548+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:33.548+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:33.549+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:33.549+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:33.549+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:33.549+0000] {spark_submit.py:495} INFO - "0" : 619
[2024-11-12T09:45:33.549+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:33.550+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:33.550+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:33.550+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:33.551+0000] {spark_submit.py:495} INFO - "0" : 620
[2024-11-12T09:45:33.551+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:33.551+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:33.551+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:33.551+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:33.552+0000] {spark_submit.py:495} INFO - "0" : 620
[2024-11-12T09:45:33.552+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:33.552+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:33.552+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:33.553+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6752194463200539,
[2024-11-12T09:45:33.553+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3774104683195594,
[2024-11-12T09:45:33.553+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:33.555+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:33.555+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:33.555+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:33.556+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:33.556+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:33.556+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:33.556+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:33.557+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:33.557+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:33.558+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:33.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/34 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.34.b1a45bb4-e18a-4cd5-8afb-2bad46313eff.tmp
[2024-11-12T09:45:33.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.34.b1a45bb4-e18a-4cd5-8afb-2bad46313eff.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/34
[2024-11-12T09:45:33.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1731404733562,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:33.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:33.665+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:33.706+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:33.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:33.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 32, 33, 33
[2024-11-12T09:45:33.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:33.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:33.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:33.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Final stage: ResultStage 34 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:33.796+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:33.796+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:33.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[140] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:33.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:45:33.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:45:33.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:33.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:33.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[140] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:33.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2024-11-12T09:45:33.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:33.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:33 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:34.222+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 375 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:34.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-11-12T09:45:34.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: ResultStage 34 (start at NativeMethodAccessorImpl.java:0) finished in 0.429 s
[2024-11-12T09:45:34.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:34.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2024-11-12T09:45:34.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.437990 s
[2024-11-12T09:45:34.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO FileFormatWriter: Start to commit write Job 6f016b83-dc72-4372-b007-b7987f7e68a6.
[2024-11-12T09:45:34.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/34 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.34.1f3a5c3a-7255-45c0-b833-441d6f44da3d.tmp
[2024-11-12T09:45:34.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.34.1f3a5c3a-7255-45c0-b833-441d6f44da3d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/34
[2024-11-12T09:45:34.310+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO ManifestFileCommitProtocol: Committed batch 34
[2024-11-12T09:45:34.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO FileFormatWriter: Write Job 6f016b83-dc72-4372-b007-b7987f7e68a6 committed. Elapsed time: 83 ms.
[2024-11-12T09:45:34.314+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO FileFormatWriter: Finished processing stats for write job 6f016b83-dc72-4372-b007-b7987f7e68a6.
[2024-11-12T09:45:34.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/34 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.34.fc510abf-d03e-44f6-86db-41c6e24a17f8.tmp
[2024-11-12T09:45:34.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.34.fc510abf-d03e-44f6-86db-41c6e24a17f8.tmp to hdfs://namenode:9000/spark_checkpoint/commits/34
[2024-11-12T09:45:34.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:34.414+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:34.415+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:34.415+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:34.415+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:33.537Z",
[2024-11-12T09:45:34.416+0000] {spark_submit.py:495} INFO - "batchId" : 34,
[2024-11-12T09:45:34.416+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:34.416+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3736263736263736,
[2024-11-12T09:45:34.417+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1441647597254005,
[2024-11-12T09:45:34.417+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:34.417+0000] {spark_submit.py:495} INFO - "addBatch" : 635,
[2024-11-12T09:45:34.417+0000] {spark_submit.py:495} INFO - "commitOffsets" : 98,
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "latestOffset" : 25,
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "queryPlanning" : 42,
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "triggerExecution" : 874,
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "walCommit" : 67
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:34.418+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:34.427+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:34.429+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:34.432+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:34.433+0000] {spark_submit.py:495} INFO - "0" : 620
[2024-11-12T09:45:34.433+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:34.433+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:34.433+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:34.434+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:34.434+0000] {spark_submit.py:495} INFO - "0" : 621
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - "0" : 621
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:34.436+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:34.437+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:34.437+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3736263736263736,
[2024-11-12T09:45:34.438+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1441647597254005,
[2024-11-12T09:45:34.438+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:34.438+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:34.439+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:34.439+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:34.440+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:34.441+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:34.442+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:34.442+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:34.443+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:34.443+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:34.443+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:34.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/35 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.35.d6b74ece-6cbc-42a4-b060-cccd9d0285ff.tmp
[2024-11-12T09:45:34.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.35.d6b74ece-6cbc-42a4-b060-cccd9d0285ff.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/35
[2024-11-12T09:45:34.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1731404734438,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:34.577+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:34.580+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:34.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:34.620+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:34.650+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 34
[2024-11-12T09:45:34.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:34.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:34.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:34.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:34.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:34.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:34.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[144] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:34.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:45:34.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:45:34.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:34.811+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:34.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[144] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:34.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2024-11-12T09:45:34.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:34.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:35.242+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 424 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:35.243+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-11-12T09:45:35.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.492 s
[2024-11-12T09:45:35.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:35.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2024-11-12T09:45:35.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.507263 s
[2024-11-12T09:45:35.246+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO FileFormatWriter: Start to commit write Job cc6251e1-de5a-4e83-a24e-58d6864e9b0e.
[2024-11-12T09:45:35.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/35 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.35.00d6c453-c8a0-4af3-a407-550e73ce8b2b.tmp
[2024-11-12T09:45:35.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.35.00d6c453-c8a0-4af3-a407-550e73ce8b2b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/35
[2024-11-12T09:45:35.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO ManifestFileCommitProtocol: Committed batch 35
[2024-11-12T09:45:35.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO FileFormatWriter: Write Job cc6251e1-de5a-4e83-a24e-58d6864e9b0e committed. Elapsed time: 111 ms.
[2024-11-12T09:45:35.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO FileFormatWriter: Finished processing stats for write job cc6251e1-de5a-4e83-a24e-58d6864e9b0e.
[2024-11-12T09:45:35.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/35 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.35.38a5481d-8b48-4850-afcd-2ff574ba7617.tmp
[2024-11-12T09:45:35.447+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.35.38a5481d-8b48-4850-afcd-2ff574ba7617.tmp to hdfs://namenode:9000/spark_checkpoint/commits/35
[2024-11-12T09:45:35.450+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:35.451+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:35.452+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:35.452+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:35.452+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:34.413Z",
[2024-11-12T09:45:35.452+0000] {spark_submit.py:495} INFO - "batchId" : 35,
[2024-11-12T09:45:35.453+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:35.453+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1415525114155252,
[2024-11-12T09:45:35.453+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9671179883945841,
[2024-11-12T09:45:35.454+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:35.454+0000] {spark_submit.py:495} INFO - "addBatch" : 773,
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "commitOffsets" : 88,
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "latestOffset" : 25,
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "queryPlanning" : 47,
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1034,
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "walCommit" : 97
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:35.455+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:35.461+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:35.463+0000] {spark_submit.py:495} INFO - "0" : 621
[2024-11-12T09:45:35.463+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:35.463+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:35.464+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:35.464+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:35.466+0000] {spark_submit.py:495} INFO - "0" : 622
[2024-11-12T09:45:35.469+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:35.471+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:35.478+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:35.479+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:35.483+0000] {spark_submit.py:495} INFO - "0" : 622
[2024-11-12T09:45:35.486+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:35.487+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:35.489+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:35.490+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1415525114155252,
[2024-11-12T09:45:35.490+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9671179883945841,
[2024-11-12T09:45:35.491+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:35.492+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:35.492+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:35.492+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:35.492+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:35.493+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:35.494+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:35.494+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:35.494+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:35.495+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:35.495+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:35.495+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/36 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.36.26e4f717-0c64-4e0c-93f1-f9ad367c9d6f.tmp
[2024-11-12T09:45:35.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.36.26e4f717-0c64-4e0c-93f1-f9ad367c9d6f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/36
[2024-11-12T09:45:35.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1731404735460,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:35.602+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:35.607+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:35.640+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:35.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:35.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 35, 35
[2024-11-12T09:45:35.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:35.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:35.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:35.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:35.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:35.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:35.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:35.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:45:35.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.6 MiB)
[2024-11-12T09:45:35.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_33_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:35.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:35.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:35.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[148] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:35.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2024-11-12T09:45:35.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:35.810+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:35.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_32_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:35.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:35.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:35.871+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_35_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:35.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:35.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_34_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:35.908+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:35.935+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_30_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:35.940+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:35.953+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_31_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:35.966+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:35 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:36.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 438 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:36.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2024-11-12T09:45:36.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.507 s
[2024-11-12T09:45:36.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:36.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2024-11-12T09:45:36.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.513744 s
[2024-11-12T09:45:36.252+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO FileFormatWriter: Start to commit write Job 5d5bac0f-f5a7-47a1-95bc-551eee13ab8f.
[2024-11-12T09:45:36.262+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/36 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.36.0db5fdd6-56b6-4748-8af4-47ac214c8ece.tmp
[2024-11-12T09:45:36.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.36.0db5fdd6-56b6-4748-8af4-47ac214c8ece.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/36
[2024-11-12T09:45:36.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO ManifestFileCommitProtocol: Committed batch 36
[2024-11-12T09:45:36.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO FileFormatWriter: Write Job 5d5bac0f-f5a7-47a1-95bc-551eee13ab8f committed. Elapsed time: 78 ms.
[2024-11-12T09:45:36.327+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO FileFormatWriter: Finished processing stats for write job 5d5bac0f-f5a7-47a1-95bc-551eee13ab8f.
[2024-11-12T09:45:36.340+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/36 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.36.6788ae03-5d62-42be-9a6c-7f46bd09e991.tmp
[2024-11-12T09:45:36.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.36.6788ae03-5d62-42be-9a6c-7f46bd09e991.tmp to hdfs://namenode:9000/spark_checkpoint/commits/36
[2024-11-12T09:45:36.407+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:36.407+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:36.408+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:36.408+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:36.408+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:35.449Z",
[2024-11-12T09:45:36.409+0000] {spark_submit.py:495} INFO - "batchId" : 36,
[2024-11-12T09:45:36.409+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:36.409+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9652509652509652,
[2024-11-12T09:45:36.410+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0460251046025104,
[2024-11-12T09:45:36.410+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:36.410+0000] {spark_submit.py:495} INFO - "addBatch" : 712,
[2024-11-12T09:45:36.411+0000] {spark_submit.py:495} INFO - "commitOffsets" : 79,
[2024-11-12T09:45:36.411+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:36.411+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:45:36.411+0000] {spark_submit.py:495} INFO - "queryPlanning" : 55,
[2024-11-12T09:45:36.411+0000] {spark_submit.py:495} INFO - "triggerExecution" : 956,
[2024-11-12T09:45:36.411+0000] {spark_submit.py:495} INFO - "walCommit" : 94
[2024-11-12T09:45:36.412+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:36.412+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:36.412+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:36.413+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:36.413+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:36.413+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - "0" : 622
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - "0" : 623
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:36.414+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:36.415+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:36.415+0000] {spark_submit.py:495} INFO - "0" : 623
[2024-11-12T09:45:36.415+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:36.417+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:36.422+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:36.423+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9652509652509652,
[2024-11-12T09:45:36.423+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0460251046025104,
[2024-11-12T09:45:36.423+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:36.423+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:36.423+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:36.424+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:36.425+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:36.425+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:36.425+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:36.426+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:36.426+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:36.426+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:36.427+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:36.433+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/37 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.37.4be56a44-a63e-43be-ac8a-16bb553faf41.tmp
[2024-11-12T09:45:36.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.37.4be56a44-a63e-43be-ac8a-16bb553faf41.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/37
[2024-11-12T09:45:36.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1731404736420,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:36.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:36.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:36.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:36.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:36.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 36, 36
[2024-11-12T09:45:36.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:36.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:36.615+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:36.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Final stage: ResultStage 37 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:36.628+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:36.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:36.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[152] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:36.670+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:36.674+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:36.676+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:36.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:36.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[152] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:36.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2024-11-12T09:45:36.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:36.714+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:36 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:37.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 558 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:37.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2024-11-12T09:45:37.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO DAGScheduler: ResultStage 37 (start at NativeMethodAccessorImpl.java:0) finished in 0.625 s
[2024-11-12T09:45:37.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:37.242+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
[2024-11-12T09:45:37.249+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.629366 s
[2024-11-12T09:45:37.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO FileFormatWriter: Start to commit write Job 61307e90-0a77-4338-8113-c922c2fdc144.
[2024-11-12T09:45:37.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/37 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.37.81919135-6ecd-41db-8cea-5202861e4201.tmp
[2024-11-12T09:45:37.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.37.81919135-6ecd-41db-8cea-5202861e4201.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/37
[2024-11-12T09:45:37.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO ManifestFileCommitProtocol: Committed batch 37
[2024-11-12T09:45:37.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO FileFormatWriter: Write Job 61307e90-0a77-4338-8113-c922c2fdc144 committed. Elapsed time: 78 ms.
[2024-11-12T09:45:37.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO FileFormatWriter: Finished processing stats for write job 61307e90-0a77-4338-8113-c922c2fdc144.
[2024-11-12T09:45:37.356+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/37 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.37.8b125d39-c9fd-435a-8f2e-9023f59289ea.tmp
[2024-11-12T09:45:37.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.37.8b125d39-c9fd-435a-8f2e-9023f59289ea.tmp to hdfs://namenode:9000/spark_checkpoint/commits/37
[2024-11-12T09:45:37.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:37.422+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:37.425+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:37.426+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:37.426+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:36.407Z",
[2024-11-12T09:45:37.426+0000] {spark_submit.py:495} INFO - "batchId" : 37,
[2024-11-12T09:45:37.427+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:37.429+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0438413361169103,
[2024-11-12T09:45:37.429+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9881422924901185,
[2024-11-12T09:45:37.429+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:37.429+0000] {spark_submit.py:495} INFO - "addBatch" : 821,
[2024-11-12T09:45:37.430+0000] {spark_submit.py:495} INFO - "commitOffsets" : 90,
[2024-11-12T09:45:37.430+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:37.430+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:45:37.430+0000] {spark_submit.py:495} INFO - "queryPlanning" : 30,
[2024-11-12T09:45:37.431+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1012,
[2024-11-12T09:45:37.431+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:45:37.431+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:37.432+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:37.432+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:37.432+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:37.433+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:37.433+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:37.433+0000] {spark_submit.py:495} INFO - "0" : 623
[2024-11-12T09:45:37.435+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:37.435+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:37.436+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:37.436+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:37.438+0000] {spark_submit.py:495} INFO - "0" : 624
[2024-11-12T09:45:37.438+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:37.439+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:37.440+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:37.440+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:37.440+0000] {spark_submit.py:495} INFO - "0" : 624
[2024-11-12T09:45:37.440+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:37.440+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:37.441+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:37.442+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0438413361169103,
[2024-11-12T09:45:37.442+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9881422924901185,
[2024-11-12T09:45:37.443+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:37.443+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:37.443+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:37.444+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:37.444+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:37.444+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:37.444+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:37.444+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:37.447+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:37.447+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:37.454+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:37.455+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/38 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.38.4d4f25f2-4716-4045-a1a9-5a95b2ae5fd5.tmp
[2024-11-12T09:45:37.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.38.4d4f25f2-4716-4045-a1a9-5a95b2ae5fd5.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/38
[2024-11-12T09:45:37.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1731404737435,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:37.950+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:37.951+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:37.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:37.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:37.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:37 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 36, 37, 37
[2024-11-12T09:45:38.000+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:38.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:38.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:38.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:38.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:38.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:38.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[156] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:38.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:38.151+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO BlockManagerInfo: Removed broadcast_36_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:38.157+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:38.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:38.162+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:38.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:38.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[156] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:38.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
[2024-11-12T09:45:38.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO BlockManagerInfo: Removed broadcast_37_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:38.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:38.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:45:38.257+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:38.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 320 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:38.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
[2024-11-12T09:45:38.498+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.433 s
[2024-11-12T09:45:38.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:38.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
[2024-11-12T09:45:38.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.447447 s
[2024-11-12T09:45:38.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO FileFormatWriter: Start to commit write Job 7e553a37-2307-4f97-b4e1-a526994b15e2.
[2024-11-12T09:45:38.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/38 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.38.4bd49f6f-55ee-4bec-98c0-1de45a8adae5.tmp
[2024-11-12T09:45:38.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.38.4bd49f6f-55ee-4bec-98c0-1de45a8adae5.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/38
[2024-11-12T09:45:38.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO ManifestFileCommitProtocol: Committed batch 38
[2024-11-12T09:45:38.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO FileFormatWriter: Write Job 7e553a37-2307-4f97-b4e1-a526994b15e2 committed. Elapsed time: 86 ms.
[2024-11-12T09:45:38.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO FileFormatWriter: Finished processing stats for write job 7e553a37-2307-4f97-b4e1-a526994b15e2.
[2024-11-12T09:45:38.621+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/38 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.38.c2f70608-8a03-478f-a51d-ac52f408a024.tmp
[2024-11-12T09:45:38.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.38.c2f70608-8a03-478f-a51d-ac52f408a024.tmp to hdfs://namenode:9000/spark_checkpoint/commits/38
[2024-11-12T09:45:38.697+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:37.421Z",
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "batchId" : 38,
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:38.700+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9861932938856016,
[2024-11-12T09:45:38.701+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7861635220125786,
[2024-11-12T09:45:38.701+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:38.701+0000] {spark_submit.py:495} INFO - "addBatch" : 632,
[2024-11-12T09:45:38.701+0000] {spark_submit.py:495} INFO - "commitOffsets" : 99,
[2024-11-12T09:45:38.701+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:38.701+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:45:38.702+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:45:38.705+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1272,
[2024-11-12T09:45:38.707+0000] {spark_submit.py:495} INFO - "walCommit" : 493
[2024-11-12T09:45:38.707+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:38.708+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:38.708+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:38.709+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:38.709+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:38.710+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:38.726+0000] {spark_submit.py:495} INFO - "0" : 624
[2024-11-12T09:45:38.729+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:38.730+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:38.730+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:38.731+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:38.731+0000] {spark_submit.py:495} INFO - "0" : 625
[2024-11-12T09:45:38.735+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:38.736+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:38.737+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:38.737+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:38.737+0000] {spark_submit.py:495} INFO - "0" : 625
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9861932938856016,
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7861635220125786,
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:38.738+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:38.739+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:38.739+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:38.739+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:38.740+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:38.741+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:38.741+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:38.742+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:38.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/39 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.39.47dc1235-e545-4825-9512-372c89ac9253.tmp
[2024-11-12T09:45:38.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.39.47dc1235-e545-4825-9512-372c89ac9253.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/39
[2024-11-12T09:45:38.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1731404738706,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:38.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:38.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:38.922+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:38.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:38.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 38, 38
[2024-11-12T09:45:38.942+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:38 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:39.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:39.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:39.021+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:39.022+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:39.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:39.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[160] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:39.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:39.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:39.051+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:39.052+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:39.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[160] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:39.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2024-11-12T09:45:39.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:39.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:39.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 262 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:39.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2024-11-12T09:45:39.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 0.308 s
[2024-11-12T09:45:39.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:39.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
[2024-11-12T09:45:39.323+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.312415 s
[2024-11-12T09:45:39.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO FileFormatWriter: Start to commit write Job ab271a65-c06d-4739-a561-45d436a529bf.
[2024-11-12T09:45:39.339+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/39.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.39.compact.263aafbc-61b5-43ea-8641-c1f5a912a268.tmp
[2024-11-12T09:45:39.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.39.compact.263aafbc-61b5-43ea-8641-c1f5a912a268.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/39.compact
[2024-11-12T09:45:39.593+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO ManifestFileCommitProtocol: Committed batch 39
[2024-11-12T09:45:39.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO FileFormatWriter: Write Job ab271a65-c06d-4739-a561-45d436a529bf committed. Elapsed time: 268 ms.
[2024-11-12T09:45:39.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO FileFormatWriter: Finished processing stats for write job ab271a65-c06d-4739-a561-45d436a529bf.
[2024-11-12T09:45:39.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/39 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.39.88267659-a9bc-40c3-9d2e-c63b25e36df6.tmp
[2024-11-12T09:45:39.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.39.88267659-a9bc-40c3-9d2e-c63b25e36df6.tmp to hdfs://namenode:9000/spark_checkpoint/commits/39
[2024-11-12T09:45:39.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:39.659+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:39.661+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:39.662+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:39.665+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:38.697Z",
[2024-11-12T09:45:39.666+0000] {spark_submit.py:495} INFO - "batchId" : 39,
[2024-11-12T09:45:39.668+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:39.669+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7836990595611285,
[2024-11-12T09:45:39.669+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0427528675703859,
[2024-11-12T09:45:39.669+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:39.670+0000] {spark_submit.py:495} INFO - "addBatch" : 697,
[2024-11-12T09:45:39.673+0000] {spark_submit.py:495} INFO - "commitOffsets" : 63,
[2024-11-12T09:45:39.675+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - "queryPlanning" : 60,
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - "triggerExecution" : 959,
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - "walCommit" : 126
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:39.676+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:39.677+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:39.677+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:39.677+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:39.677+0000] {spark_submit.py:495} INFO - "0" : 625
[2024-11-12T09:45:39.677+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:39.678+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:39.678+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:39.681+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:39.682+0000] {spark_submit.py:495} INFO - "0" : 626
[2024-11-12T09:45:39.682+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:39.682+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:39.682+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:39.682+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:39.682+0000] {spark_submit.py:495} INFO - "0" : 626
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7836990595611285,
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0427528675703859,
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:39.683+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:39.684+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:39.684+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:39.684+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:39.684+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:39.684+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:39.684+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:39.685+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:39.689+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/40 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.40.7bfa2042-6eb9-405f-a7d2-aa812a1f803b.tmp
[2024-11-12T09:45:39.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.40.7bfa2042-6eb9-405f-a7d2-aa812a1f803b.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/40
[2024-11-12T09:45:39.757+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1731404739668,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:39.773+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:39.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:39.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:39.803+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:39.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 38, 38, 39
[2024-11-12T09:45:39.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:39.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:39.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:39.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Final stage: ResultStage 40 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:39.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:39.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:39.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:39.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:39.913+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:39.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:39.918+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:39.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[164] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:39.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
[2024-11-12T09:45:39.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:39.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:39 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:40.275+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 352 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:40.275+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2024-11-12T09:45:40.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO DAGScheduler: ResultStage 40 (start at NativeMethodAccessorImpl.java:0) finished in 0.409 s
[2024-11-12T09:45:40.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:40.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2024-11-12T09:45:40.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.415858 s
[2024-11-12T09:45:40.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO FileFormatWriter: Start to commit write Job 4960f729-2b50-47e3-8137-9219ad809396.
[2024-11-12T09:45:40.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/40 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.40.34e1297d-9d48-4457-ae32-1f8eb31b25e8.tmp
[2024-11-12T09:45:40.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.40.34e1297d-9d48-4457-ae32-1f8eb31b25e8.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/40
[2024-11-12T09:45:40.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO ManifestFileCommitProtocol: Committed batch 40
[2024-11-12T09:45:40.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO FileFormatWriter: Write Job 4960f729-2b50-47e3-8137-9219ad809396 committed. Elapsed time: 97 ms.
[2024-11-12T09:45:40.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO FileFormatWriter: Finished processing stats for write job 4960f729-2b50-47e3-8137-9219ad809396.
[2024-11-12T09:45:40.397+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/40 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.40.651348f3-3432-4119-8ba8-6789dc5ce3b0.tmp
[2024-11-12T09:45:40.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.40.651348f3-3432-4119-8ba8-6789dc5ce3b0.tmp to hdfs://namenode:9000/spark_checkpoint/commits/40
[2024-11-12T09:45:40.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:40.443+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:40.443+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:40.443+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:40.444+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:39.658Z",
[2024-11-12T09:45:40.444+0000] {spark_submit.py:495} INFO - "batchId" : 40,
[2024-11-12T09:45:40.444+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:40.444+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.040582726326743,
[2024-11-12T09:45:40.445+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.277139208173691,
[2024-11-12T09:45:40.445+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:40.445+0000] {spark_submit.py:495} INFO - "addBatch" : 587,
[2024-11-12T09:45:40.445+0000] {spark_submit.py:495} INFO - "commitOffsets" : 64,
[2024-11-12T09:45:40.446+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:40.446+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:45:40.446+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:45:40.446+0000] {spark_submit.py:495} INFO - "triggerExecution" : 783,
[2024-11-12T09:45:40.447+0000] {spark_submit.py:495} INFO - "walCommit" : 85
[2024-11-12T09:45:40.447+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:40.447+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:40.447+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:40.448+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:40.448+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - "0" : 626
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - "0" : 627
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:40.449+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:40.456+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:40.458+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:40.459+0000] {spark_submit.py:495} INFO - "0" : 627
[2024-11-12T09:45:40.460+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:40.460+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:40.461+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:40.461+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.040582726326743,
[2024-11-12T09:45:40.461+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.277139208173691,
[2024-11-12T09:45:40.462+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:40.462+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:40.463+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:40.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/41 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.41.d352db99-a3ff-49e9-815c-91ca3271d534.tmp
[2024-11-12T09:45:40.934+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.41.d352db99-a3ff-49e9-815c-91ca3271d534.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/41
[2024-11-12T09:45:40.935+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1731404740462,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:40.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:40.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 38, 39, 40, 40
[2024-11-12T09:45:41.039+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:41.065+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:41.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:41.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:41.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:41.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:41.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[168] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:41.096+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:45:41.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.0 MiB)
[2024-11-12T09:45:41.106+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Removed broadcast_38_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:41.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:41.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:41.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[168] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:41.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2024-11-12T09:45:41.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:41.109+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:41.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Removed broadcast_40_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:41.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:41.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Removed broadcast_39_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:41.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:41.153+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:41.275+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 163 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:41.277+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2024-11-12T09:45:41.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.206 s
[2024-11-12T09:45:41.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:41.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
[2024-11-12T09:45:41.281+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.212681 s
[2024-11-12T09:45:41.282+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO FileFormatWriter: Start to commit write Job 008316c8-10c9-488b-a88e-a3e1c0ac261c.
[2024-11-12T09:45:41.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/41 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.41.336d52e2-a87d-46ed-9ed8-f2b197784f40.tmp
[2024-11-12T09:45:41.733+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.41.336d52e2-a87d-46ed-9ed8-f2b197784f40.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/41
[2024-11-12T09:45:41.733+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO ManifestFileCommitProtocol: Committed batch 41
[2024-11-12T09:45:41.734+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO FileFormatWriter: Write Job 008316c8-10c9-488b-a88e-a3e1c0ac261c committed. Elapsed time: 454 ms.
[2024-11-12T09:45:41.735+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO FileFormatWriter: Finished processing stats for write job 008316c8-10c9-488b-a88e-a3e1c0ac261c.
[2024-11-12T09:45:41.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/41 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.41.2abc9982-fb84-43df-aeda-09f9cbdd291c.tmp
[2024-11-12T09:45:41.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.41.2abc9982-fb84-43df-aeda-09f9cbdd291c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/41
[2024-11-12T09:45:41.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:41.791+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:41.791+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:41.791+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:41.791+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:40.443Z",
[2024-11-12T09:45:41.791+0000] {spark_submit.py:495} INFO - "batchId" : 41,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2738853503184713,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7429420505200593,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "addBatch" : 733,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "commitOffsets" : 55,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "latestOffset" : 19,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "queryPlanning" : 57,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1346,
[2024-11-12T09:45:41.792+0000] {spark_submit.py:495} INFO - "walCommit" : 472
[2024-11-12T09:45:41.793+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:41.793+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:41.794+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:41.794+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:41.794+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:41.794+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:41.794+0000] {spark_submit.py:495} INFO - "0" : 627
[2024-11-12T09:45:41.794+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:41.795+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - "0" : 628
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:41.796+0000] {spark_submit.py:495} INFO - "0" : 628
[2024-11-12T09:45:41.803+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:41.806+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:41.806+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2738853503184713,
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7429420505200593,
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:41.807+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:41.808+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:41.808+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:41.808+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:41.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/42 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.42.8886ee11-d024-4df3-8d74-5b1d454fb9c2.tmp
[2024-11-12T09:45:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.42.8886ee11-d024-4df3-8d74-5b1d454fb9c2.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/42
[2024-11-12T09:45:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1731404741794,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:41.880+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.881+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.913+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:41.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 41, 41
[2024-11-12T09:45:41.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:41.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:41.967+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:41.968+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:41.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:41.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:41.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[172] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:42.003+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:41 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:42.012+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:42.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:42.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:42.022+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[172] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:42.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
[2024-11-12T09:45:42.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:42.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:42.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 325 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:42.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool
[2024-11-12T09:45:42.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.378 s
[2024-11-12T09:45:42.352+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:42.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
[2024-11-12T09:45:42.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 0.388163 s
[2024-11-12T09:45:42.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO FileFormatWriter: Start to commit write Job 8d47b777-ead7-46c6-9b53-d75a4096a953.
[2024-11-12T09:45:42.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/42 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.42.9e46ba2c-b4a7-4585-8dab-edd42ea4879d.tmp
[2024-11-12T09:45:42.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.42.9e46ba2c-b4a7-4585-8dab-edd42ea4879d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/42
[2024-11-12T09:45:42.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO ManifestFileCommitProtocol: Committed batch 42
[2024-11-12T09:45:42.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO FileFormatWriter: Write Job 8d47b777-ead7-46c6-9b53-d75a4096a953 committed. Elapsed time: 483 ms.
[2024-11-12T09:45:42.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO FileFormatWriter: Finished processing stats for write job 8d47b777-ead7-46c6-9b53-d75a4096a953.
[2024-11-12T09:45:42.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/42 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.42.ab8e6d2c-b8c3-4648-b0c4-a635b25bed86.tmp
[2024-11-12T09:45:42.909+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.42.ab8e6d2c-b8c3-4648-b0c4-a635b25bed86.tmp to hdfs://namenode:9000/spark_checkpoint/commits/42
[2024-11-12T09:45:42.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:41.791Z",
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "batchId" : 42,
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:42.926+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7418397626112759,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8944543828264757,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "addBatch" : 945,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "commitOffsets" : 75,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1118,
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "walCommit" : 61
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:42.927+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "0" : 628
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:42.928+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:42.929+0000] {spark_submit.py:495} INFO - "0" : 629
[2024-11-12T09:45:42.929+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:42.929+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:42.930+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:42.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:42.930+0000] {spark_submit.py:495} INFO - "0" : 629
[2024-11-12T09:45:42.930+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:42.931+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:42.931+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:42.931+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7418397626112759,
[2024-11-12T09:45:42.932+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8944543828264757,
[2024-11-12T09:45:42.932+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:42.932+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:42.933+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:42.941+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/43 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.43.5e830e30-bd66-4a1a-9107-27dc7724d39f.tmp
[2024-11-12T09:45:42.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.43.5e830e30-bd66-4a1a-9107-27dc7724d39f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/43
[2024-11-12T09:45:42.994+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:42 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1731404742930,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:43.021+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:43.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:43.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:43.051+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:43.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 41, 42, 42
[2024-11-12T09:45:43.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:43.109+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:43.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:43.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Final stage: ResultStage 43 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:43.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:43.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:43.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[176] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:43.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:43.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:43.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:43.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:43.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[176] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:43.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2024-11-12T09:45:43.153+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:43.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:43.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 196 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:43.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2024-11-12T09:45:43.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: ResultStage 43 (start at NativeMethodAccessorImpl.java:0) finished in 0.236 s
[2024-11-12T09:45:43.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:43.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2024-11-12T09:45:43.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.239947 s
[2024-11-12T09:45:43.351+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO FileFormatWriter: Start to commit write Job a0f307b8-11c6-4de0-aaa3-a2bf02e5c865.
[2024-11-12T09:45:43.360+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/43 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.43.811b01d9-07f4-41d5-90ae-fb6d2b4ab654.tmp
[2024-11-12T09:45:43.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.43.811b01d9-07f4-41d5-90ae-fb6d2b4ab654.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/43
[2024-11-12T09:45:43.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO ManifestFileCommitProtocol: Committed batch 43
[2024-11-12T09:45:43.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO FileFormatWriter: Write Job a0f307b8-11c6-4de0-aaa3-a2bf02e5c865 committed. Elapsed time: 62 ms.
[2024-11-12T09:45:43.414+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO FileFormatWriter: Finished processing stats for write job a0f307b8-11c6-4de0-aaa3-a2bf02e5c865.
[2024-11-12T09:45:43.433+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/43 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.43.8971ff4b-e476-4de6-9341-7db9976ee45d.tmp
[2024-11-12T09:45:43.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.43.8971ff4b-e476-4de6-9341-7db9976ee45d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/43
[2024-11-12T09:45:43.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:43.901+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:43.902+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:43.902+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:43.903+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:42.923Z",
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "batchId" : 43,
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.88339222614841,
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.026694045174538,
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "addBatch" : 377,
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "commitOffsets" : 484,
[2024-11-12T09:45:43.904+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - "queryPlanning" : 38,
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - "triggerExecution" : 974,
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - "walCommit" : 63
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:43.905+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:43.906+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:43.906+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:43.906+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:43.906+0000] {spark_submit.py:495} INFO - "0" : 629
[2024-11-12T09:45:43.907+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:43.907+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:43.907+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:43.908+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:43.908+0000] {spark_submit.py:495} INFO - "0" : 630
[2024-11-12T09:45:43.917+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:43.918+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:43.918+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:43.918+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:43.919+0000] {spark_submit.py:495} INFO - "0" : 630
[2024-11-12T09:45:43.919+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:43.919+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:43.919+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:43.919+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.88339222614841,
[2024-11-12T09:45:43.920+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.026694045174538,
[2024-11-12T09:45:43.920+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:43.920+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:43.921+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:43.921+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:43.921+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:43.922+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:43.922+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:43.923+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:43.923+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:43.923+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:43.923+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:43.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/44 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.44.4bd9c155-e301-4145-b2e4-4f580bd10f96.tmp
[2024-11-12T09:45:43.974+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.44.4bd9c155-e301-4145-b2e4-4f580bd10f96.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/44
[2024-11-12T09:45:43.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:43 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1731404743907,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:44.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.030+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 42, 43, 43
[2024-11-12T09:45:44.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:44.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:44.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:44.093+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:44.093+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:44.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:44.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[180] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:44.117+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:45:44.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:45:44.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:44.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:44.125+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[180] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:44.126+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
[2024-11-12T09:45:44.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:44.158+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:44.301+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 170 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:44.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool
[2024-11-12T09:45:44.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.209 s
[2024-11-12T09:45:44.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:44.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
[2024-11-12T09:45:44.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 0.216323 s
[2024-11-12T09:45:44.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO FileFormatWriter: Start to commit write Job fd3cf0fd-3c8d-418c-a32d-c2645f4265c8.
[2024-11-12T09:45:44.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/44 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.44.06dc4565-82d3-4844-aa6b-eba41e4ddf84.tmp
[2024-11-12T09:45:44.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.44.06dc4565-82d3-4844-aa6b-eba41e4ddf84.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/44
[2024-11-12T09:45:44.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO ManifestFileCommitProtocol: Committed batch 44
[2024-11-12T09:45:44.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO FileFormatWriter: Write Job fd3cf0fd-3c8d-418c-a32d-c2645f4265c8 committed. Elapsed time: 60 ms.
[2024-11-12T09:45:44.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO FileFormatWriter: Finished processing stats for write job fd3cf0fd-3c8d-418c-a32d-c2645f4265c8.
[2024-11-12T09:45:44.378+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/44 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.44.cda9a47f-c9c1-4cfd-8bb4-56a419645776.tmp
[2024-11-12T09:45:44.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.44.cda9a47f-c9c1-4cfd-8bb4-56a419645776.tmp to hdfs://namenode:9000/spark_checkpoint/commits/44
[2024-11-12T09:45:44.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:44.437+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:44.437+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:43.900Z",
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "batchId" : 44,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0235414534288638,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.8939393939393938,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "addBatch" : 357,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "commitOffsets" : 61,
[2024-11-12T09:45:44.438+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "queryPlanning" : 32,
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "triggerExecution" : 528,
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "walCommit" : 68
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "0" : 630
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:44.439+0000] {spark_submit.py:495} INFO - "0" : 631
[2024-11-12T09:45:44.440+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:44.452+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:44.454+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:44.454+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:44.454+0000] {spark_submit.py:495} INFO - "0" : 631
[2024-11-12T09:45:44.455+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:44.455+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:44.455+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:44.455+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0235414534288638,
[2024-11-12T09:45:44.456+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.8939393939393938,
[2024-11-12T09:45:44.456+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:44.457+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:44.459+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:44.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/45 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.45.7b0273f2-b15b-4b7e-9378-409786f971c9.tmp
[2024-11-12T09:45:44.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.45.7b0273f2-b15b-4b7e-9378-409786f971c9.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/45
[2024-11-12T09:45:44.507+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1731404744440,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:44.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.529+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.549+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:44.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 43, 44, 44
[2024-11-12T09:45:44.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:44.610+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:44.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:44.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:44.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:44.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:44.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[184] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:44.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:45:44.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:45:44.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:44.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:44.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[184] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:44.645+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
[2024-11-12T09:45:44.648+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:44.684+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:44 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:45.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 656 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:45.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2024-11-12T09:45:45.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 0.693 s
[2024-11-12T09:45:45.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:45.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
[2024-11-12T09:45:45.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.697089 s
[2024-11-12T09:45:45.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO FileFormatWriter: Start to commit write Job 87a2b502-78cd-4d87-bb0c-d32d681af8b2.
[2024-11-12T09:45:45.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/45 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.45.4f00ef97-78ce-4e0c-bbee-945d83c61bbe.tmp
[2024-11-12T09:45:45.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.45.4f00ef97-78ce-4e0c-bbee-945d83c61bbe.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/45
[2024-11-12T09:45:45.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO ManifestFileCommitProtocol: Committed batch 45
[2024-11-12T09:45:45.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO FileFormatWriter: Write Job 87a2b502-78cd-4d87-bb0c-d32d681af8b2 committed. Elapsed time: 55 ms.
[2024-11-12T09:45:45.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO FileFormatWriter: Finished processing stats for write job 87a2b502-78cd-4d87-bb0c-d32d681af8b2.
[2024-11-12T09:45:45.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/45 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.45.1d18c4e1-f5cd-40d9-9841-859364fb8609.tmp
[2024-11-12T09:45:45.437+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.45.1d18c4e1-f5cd-40d9-9841-859364fb8609.tmp to hdfs://namenode:9000/spark_checkpoint/commits/45
[2024-11-12T09:45:45.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:45.439+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:45.439+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:45.439+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:45.439+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:44.430Z",
[2024-11-12T09:45:45.439+0000] {spark_submit.py:495} INFO - "batchId" : 45,
[2024-11-12T09:45:45.439+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.8867924528301885,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.99304865938431,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "addBatch" : 824,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "commitOffsets" : 75,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:45:45.440+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:45:45.441+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1007,
[2024-11-12T09:45:45.441+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:45:45.441+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:45.441+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:45.442+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:45.442+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:45.442+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:45.443+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:45.443+0000] {spark_submit.py:495} INFO - "0" : 631
[2024-11-12T09:45:45.443+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:45.443+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:45.443+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:45.444+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:45.444+0000] {spark_submit.py:495} INFO - "0" : 632
[2024-11-12T09:45:45.444+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:45.444+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:45.445+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:45.445+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:45.445+0000] {spark_submit.py:495} INFO - "0" : 632
[2024-11-12T09:45:45.445+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:45.449+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:45.450+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:45.450+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.8867924528301885,
[2024-11-12T09:45:45.451+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.99304865938431,
[2024-11-12T09:45:45.452+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:45.452+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:45.453+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:45.453+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:45.453+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:45.454+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:45.454+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:45.455+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:45.455+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:45.455+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:45.455+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:45.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/46 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.46.1c642507-b925-4957-98b2-4a875f11ccaf.tmp
[2024-11-12T09:45:45.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.46.1c642507-b925-4957-98b2-4a875f11ccaf.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/46
[2024-11-12T09:45:45.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1731404745443,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:45.520+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:45.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:45.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:45.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:45.558+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 44, 45, 45
[2024-11-12T09:45:45.559+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:45.586+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:45.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:45.591+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Final stage: ResultStage 46 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:45.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:45.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:45.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[188] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:45.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:45:45.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:45:45.620+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:45.620+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:45.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[188] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:45.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2024-11-12T09:45:45.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:45.652+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:45 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:46.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 664 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:46.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2024-11-12T09:45:46.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: ResultStage 46 (start at NativeMethodAccessorImpl.java:0) finished in 0.695 s
[2024-11-12T09:45:46.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:46.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2024-11-12T09:45:46.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.701324 s
[2024-11-12T09:45:46.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO FileFormatWriter: Start to commit write Job 51ba61ee-da23-4041-a29f-ae76559b3494.
[2024-11-12T09:45:46.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/46 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.46.c966da5c-2ec8-488a-b8cc-95862cd9d839.tmp
[2024-11-12T09:45:46.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.46.c966da5c-2ec8-488a-b8cc-95862cd9d839.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/46
[2024-11-12T09:45:46.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO ManifestFileCommitProtocol: Committed batch 46
[2024-11-12T09:45:46.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO FileFormatWriter: Write Job 51ba61ee-da23-4041-a29f-ae76559b3494 committed. Elapsed time: 82 ms.
[2024-11-12T09:45:46.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO FileFormatWriter: Finished processing stats for write job 51ba61ee-da23-4041-a29f-ae76559b3494.
[2024-11-12T09:45:46.378+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/46 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.46.77fddc88-a86a-4677-8dbe-dff2a642c365.tmp
[2024-11-12T09:45:46.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.46.77fddc88-a86a-4677-8dbe-dff2a642c365.tmp to hdfs://namenode:9000/spark_checkpoint/commits/46
[2024-11-12T09:45:46.445+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:46.446+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:46.446+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:46.446+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:46.446+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:45.438Z",
[2024-11-12T09:45:46.447+0000] {spark_submit.py:495} INFO - "batchId" : 46,
[2024-11-12T09:45:46.447+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:46.447+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9920634920634921,
[2024-11-12T09:45:46.447+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9940357852882704,
[2024-11-12T09:45:46.448+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:46.448+0000] {spark_submit.py:495} INFO - "addBatch" : 840,
[2024-11-12T09:45:46.448+0000] {spark_submit.py:495} INFO - "commitOffsets" : 72,
[2024-11-12T09:45:46.448+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:46.448+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:45:46.454+0000] {spark_submit.py:495} INFO - "queryPlanning" : 26,
[2024-11-12T09:45:46.455+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1006,
[2024-11-12T09:45:46.456+0000] {spark_submit.py:495} INFO - "walCommit" : 60
[2024-11-12T09:45:46.456+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:46.456+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:46.456+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:46.457+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:46.457+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:46.457+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:46.457+0000] {spark_submit.py:495} INFO - "0" : 632
[2024-11-12T09:45:46.458+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:46.458+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:46.458+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:46.458+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:46.459+0000] {spark_submit.py:495} INFO - "0" : 633
[2024-11-12T09:45:46.460+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:46.461+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:46.461+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:46.461+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:46.461+0000] {spark_submit.py:495} INFO - "0" : 633
[2024-11-12T09:45:46.461+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:46.462+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:46.462+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:46.463+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9920634920634921,
[2024-11-12T09:45:46.464+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9940357852882704,
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:46.465+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:46.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/47 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.47.89d0410d-11a1-4e27-8b9d-9bdefe25a1c8.tmp
[2024-11-12T09:45:46.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.47.89d0410d-11a1-4e27-8b9d-9bdefe25a1c8.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/47
[2024-11-12T09:45:46.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1731404746453,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:46.529+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:46.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:46.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:46.556+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:46.564+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 45, 46, 46
[2024-11-12T09:45:46.569+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:46.599+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:46.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Got job 47 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:46.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:46.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:46.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:46.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:46.620+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:45:46.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:45:46.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:45:46.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:46.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:46.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0
[2024-11-12T09:45:46.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:46.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:46 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:45:47.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 674 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:47.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool
[2024-11-12T09:45:47.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.705 s
[2024-11-12T09:45:47.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:47.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
[2024-11-12T09:45:47.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO DAGScheduler: Job 47 finished: start at NativeMethodAccessorImpl.java:0, took 0.707739 s
[2024-11-12T09:45:47.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO FileFormatWriter: Start to commit write Job fe062990-cc8d-42cf-89a1-6b71c944e5e4.
[2024-11-12T09:45:47.332+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/47 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.47.21c7a39a-6473-4f43-8175-ad6f8eb7b47c.tmp
[2024-11-12T09:45:47.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.47.21c7a39a-6473-4f43-8175-ad6f8eb7b47c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/47
[2024-11-12T09:45:47.810+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO ManifestFileCommitProtocol: Committed batch 47
[2024-11-12T09:45:47.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO FileFormatWriter: Write Job fe062990-cc8d-42cf-89a1-6b71c944e5e4 committed. Elapsed time: 502 ms.
[2024-11-12T09:45:47.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO FileFormatWriter: Finished processing stats for write job fe062990-cc8d-42cf-89a1-6b71c944e5e4.
[2024-11-12T09:45:47.817+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/47 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.47.1163c7da-eb0a-42d5-8316-4fde6d0b8b2a.tmp
[2024-11-12T09:45:47.878+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.47.1163c7da-eb0a-42d5-8316-4fde6d0b8b2a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/47
[2024-11-12T09:45:47.880+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:47.881+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:47.881+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:47.881+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:47.881+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:46.448Z",
[2024-11-12T09:45:47.882+0000] {spark_submit.py:495} INFO - "batchId" : 47,
[2024-11-12T09:45:47.882+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:47.882+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9900990099009901,
[2024-11-12T09:45:47.883+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6993006993006994,
[2024-11-12T09:45:47.883+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:47.883+0000] {spark_submit.py:495} INFO - "addBatch" : 1271,
[2024-11-12T09:45:47.883+0000] {spark_submit.py:495} INFO - "commitOffsets" : 68,
[2024-11-12T09:45:47.883+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:47.883+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1430,
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "walCommit" : 61
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:47.884+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:47.885+0000] {spark_submit.py:495} INFO - "0" : 633
[2024-11-12T09:45:47.887+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:47.888+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:47.888+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:47.888+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:47.889+0000] {spark_submit.py:495} INFO - "0" : 634
[2024-11-12T09:45:47.889+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:47.889+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:47.889+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:47.896+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:47.896+0000] {spark_submit.py:495} INFO - "0" : 634
[2024-11-12T09:45:47.897+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:47.897+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:47.897+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:47.897+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9900990099009901,
[2024-11-12T09:45:47.897+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6993006993006994,
[2024-11-12T09:45:47.898+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:47.898+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:47.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:47.915+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/48 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.48.d6269fcf-cc31-46ac-be5f-6257f6f2e348.tmp
[2024-11-12T09:45:47.978+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.48.d6269fcf-cc31-46ac-be5f-6257f6f2e348.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/48
[2024-11-12T09:45:47.979+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1731404747897,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:47.998+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:48.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:48.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:48.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:48.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 46, 47, 47
[2024-11-12T09:45:48.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:48.081+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:48.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Got job 48 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:48.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:48.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:48.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:48.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[196] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:48.117+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 320.7 KiB, free 431.2 MiB)
[2024-11-12T09:45:48.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.1 MiB)
[2024-11-12T09:45:48.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.6 MiB)
[2024-11-12T09:45:48.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:48.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[196] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:48.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
[2024-11-12T09:45:48.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:48.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_43_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:45:48.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:48.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:45:48.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_44_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:48.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:48.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_46_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:48.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:48.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_47_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:48.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:48.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_42_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:48.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:48.282+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_45_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:48.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:48.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_41_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:48.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:48.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 225 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:48.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2024-11-12T09:45:48.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 0.288 s
[2024-11-12T09:45:48.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:48.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2024-11-12T09:45:48.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO DAGScheduler: Job 48 finished: start at NativeMethodAccessorImpl.java:0, took 0.291579 s
[2024-11-12T09:45:48.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO FileFormatWriter: Start to commit write Job 7f12ccdc-5384-4f4d-aa52-fbb3a2408266.
[2024-11-12T09:45:48.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/48 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.48.e236e7bc-0436-4530-84c2-721f9735a3dd.tmp
[2024-11-12T09:45:48.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.48.e236e7bc-0436-4530-84c2-721f9735a3dd.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/48
[2024-11-12T09:45:48.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO ManifestFileCommitProtocol: Committed batch 48
[2024-11-12T09:45:48.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO FileFormatWriter: Write Job 7f12ccdc-5384-4f4d-aa52-fbb3a2408266 committed. Elapsed time: 67 ms.
[2024-11-12T09:45:48.446+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO FileFormatWriter: Finished processing stats for write job 7f12ccdc-5384-4f4d-aa52-fbb3a2408266.
[2024-11-12T09:45:48.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/48 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.48.15ba77a6-94ea-45cc-829d-4e48cfd864aa.tmp
[2024-11-12T09:45:48.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.48.15ba77a6-94ea-45cc-829d-4e48cfd864aa.tmp to hdfs://namenode:9000/spark_checkpoint/commits/48
[2024-11-12T09:45:48.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:48.927+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:47.880Z",
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "batchId" : 48,
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6983240223463687,
[2024-11-12T09:45:48.928+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9578544061302682,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "addBatch" : 433,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "commitOffsets" : 480,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "queryPlanning" : 26,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1044,
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "walCommit" : 81
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:48.929+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "0" : 634
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "0" : 635
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "0" : 635
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:48.930+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:48.931+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6983240223463687,
[2024-11-12T09:45:48.932+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9578544061302682,
[2024-11-12T09:45:48.932+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:48.933+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:48.940+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/49 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.49.bbf8f671-bd1a-4665-ac9c-b0b3753a7aeb.tmp
[2024-11-12T09:45:49.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.49.bbf8f671-bd1a-4665-ac9c-b0b3753a7aeb.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/49
[2024-11-12T09:45:49.392+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO MicroBatchExecution: Committed offsets for batch 49. Metadata OffsetSeqMetadata(0,1731404748934,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:49.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:49.425+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:49.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:49.440+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:49.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 47, 48, 48
[2024-11-12T09:45:49.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:49.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Got job 49 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Final stage: ResultStage 49 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:49.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[200] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:49.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:49.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:49.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:49.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:49.527+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[200] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:49.527+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0
[2024-11-12T09:45:49.529+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:49.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:49.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 198 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:49.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2024-11-12T09:45:49.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: ResultStage 49 (start at NativeMethodAccessorImpl.java:0) finished in 0.238 s
[2024-11-12T09:45:49.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:49.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
[2024-11-12T09:45:49.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO DAGScheduler: Job 49 finished: start at NativeMethodAccessorImpl.java:0, took 0.240878 s
[2024-11-12T09:45:49.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO FileFormatWriter: Start to commit write Job 0b43adfc-7a18-48d2-9e5f-ec150119a6e2.
[2024-11-12T09:45:49.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/49.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.49.compact.efe9cb14-f349-4ebb-a08e-f6cac9b31f4c.tmp
[2024-11-12T09:45:49.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.49.compact.efe9cb14-f349-4ebb-a08e-f6cac9b31f4c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/49.compact
[2024-11-12T09:45:49.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO ManifestFileCommitProtocol: Committed batch 49
[2024-11-12T09:45:49.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO FileFormatWriter: Write Job 0b43adfc-7a18-48d2-9e5f-ec150119a6e2 committed. Elapsed time: 263 ms.
[2024-11-12T09:45:49.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO FileFormatWriter: Finished processing stats for write job 0b43adfc-7a18-48d2-9e5f-ec150119a6e2.
[2024-11-12T09:45:49.999+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/49 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.49.87fcaa34-974c-48c9-b376-de8baa216041.tmp
[2024-11-12T09:45:50.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.49.87fcaa34-974c-48c9-b376-de8baa216041.tmp to hdfs://namenode:9000/spark_checkpoint/commits/49
[2024-11-12T09:45:50.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:50.050+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:50.051+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:50.051+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:50.051+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:48.927Z",
[2024-11-12T09:45:50.051+0000] {spark_submit.py:495} INFO - "batchId" : 49,
[2024-11-12T09:45:50.052+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:50.052+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9551098376313276,
[2024-11-12T09:45:50.052+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8920606601248885,
[2024-11-12T09:45:50.052+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:50.052+0000] {spark_submit.py:495} INFO - "addBatch" : 560,
[2024-11-12T09:45:50.053+0000] {spark_submit.py:495} INFO - "commitOffsets" : 56,
[2024-11-12T09:45:50.053+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:50.053+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:45:50.053+0000] {spark_submit.py:495} INFO - "queryPlanning" : 37,
[2024-11-12T09:45:50.053+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1121,
[2024-11-12T09:45:50.053+0000] {spark_submit.py:495} INFO - "walCommit" : 457
[2024-11-12T09:45:50.054+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.054+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:50.054+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:50.054+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:50.054+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:50.054+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:50.055+0000] {spark_submit.py:495} INFO - "0" : 635
[2024-11-12T09:45:50.055+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.055+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - "0" : 636
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:50.056+0000] {spark_submit.py:495} INFO - "0" : 636
[2024-11-12T09:45:50.057+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.058+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.058+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:50.059+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9551098376313276,
[2024-11-12T09:45:50.059+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8920606601248885,
[2024-11-12T09:45:50.059+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:50.060+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:50.060+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:50.060+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:50.060+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.061+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:50.061+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:50.061+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:50.061+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:50.061+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.062+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.076+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/50 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.50.cea0ca08-839d-4afc-9e57-4f8713439924.tmp
[2024-11-12T09:45:50.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.50.cea0ca08-839d-4afc-9e57-4f8713439924.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/50
[2024-11-12T09:45:50.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO MicroBatchExecution: Committed offsets for batch 50. Metadata OffsetSeqMetadata(0,1731404750058,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:50.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:50.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:50.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:50.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:50.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 47, 48, 48, 49
[2024-11-12T09:45:50.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:50.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:50.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Got job 50 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:50.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:50.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:50.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:50.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:50.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:50.281+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:50.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:50.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:50.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:50.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
[2024-11-12T09:45:50.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:50.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:50.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 182 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:50.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool
[2024-11-12T09:45:50.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.245 s
[2024-11-12T09:45:50.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:50.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
[2024-11-12T09:45:50.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO DAGScheduler: Job 50 finished: start at NativeMethodAccessorImpl.java:0, took 0.252736 s
[2024-11-12T09:45:50.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO FileFormatWriter: Start to commit write Job 08929cd0-5916-418a-b40b-3f38a64fa91b.
[2024-11-12T09:45:50.495+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/50 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.50.7d055b92-00c3-4cde-87de-275ff34307f6.tmp
[2024-11-12T09:45:50.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.50.7d055b92-00c3-4cde-87de-275ff34307f6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/50
[2024-11-12T09:45:50.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO ManifestFileCommitProtocol: Committed batch 50
[2024-11-12T09:45:50.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO FileFormatWriter: Write Job 08929cd0-5916-418a-b40b-3f38a64fa91b committed. Elapsed time: 63 ms.
[2024-11-12T09:45:50.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO FileFormatWriter: Finished processing stats for write job 08929cd0-5916-418a-b40b-3f38a64fa91b.
[2024-11-12T09:45:50.558+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/50 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.50.bd9f72c0-680c-4c37-8f47-21339037d2f6.tmp
[2024-11-12T09:45:50.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.50.bd9f72c0-680c-4c37-8f47-21339037d2f6.tmp to hdfs://namenode:9000/spark_checkpoint/commits/50
[2024-11-12T09:45:50.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:50.050Z",
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "batchId" : 50,
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:50.607+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8904719501335708,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.8083182640144664,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "addBatch" : 393,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "commitOffsets" : 62,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "triggerExecution" : 553,
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "walCommit" : 55
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:50.608+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "0" : 636
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - "0" : 637
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.609+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "0" : 637
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8904719501335708,
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.8083182640144664,
[2024-11-12T09:45:50.610+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:50.611+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:50.613+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:50.614+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.614+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:50.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/51 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.51.37b44fe7-64bd-43a9-a3f2-80519c7e29e5.tmp
[2024-11-12T09:45:51.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.51.37b44fe7-64bd-43a9-a3f2-80519c7e29e5.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/51
[2024-11-12T09:45:51.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MicroBatchExecution: Committed offsets for batch 51. Metadata OffsetSeqMetadata(0,1731404750616,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:51.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.120+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.162+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 48, 49, 50, 50
[2024-11-12T09:45:51.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:51.231+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:51.233+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Got job 51 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:51.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:51.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:51.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:51.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:51.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:45:51.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:45:51.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:51.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:51.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:51.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
[2024-11-12T09:45:51.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:51.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:51.466+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 176 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:51.467+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool
[2024-11-12T09:45:51.467+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.229 s
[2024-11-12T09:45:51.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:51.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
[2024-11-12T09:45:51.470+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Job 51 finished: start at NativeMethodAccessorImpl.java:0, took 0.237137 s
[2024-11-12T09:45:51.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO FileFormatWriter: Start to commit write Job 30a5225c-7338-4a05-a9a7-ea0fc3d5bf80.
[2024-11-12T09:45:51.489+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/51 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.51.07c56f2c-5eb2-4b29-ab73-4359db037030.tmp
[2024-11-12T09:45:51.530+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.51.07c56f2c-5eb2-4b29-ab73-4359db037030.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/51
[2024-11-12T09:45:51.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO ManifestFileCommitProtocol: Committed batch 51
[2024-11-12T09:45:51.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO FileFormatWriter: Write Job 30a5225c-7338-4a05-a9a7-ea0fc3d5bf80 committed. Elapsed time: 59 ms.
[2024-11-12T09:45:51.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO FileFormatWriter: Finished processing stats for write job 30a5225c-7338-4a05-a9a7-ea0fc3d5bf80.
[2024-11-12T09:45:51.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/51 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.51.4110fd52-4dfd-4151-ab55-affa05d13ecf.tmp
[2024-11-12T09:45:51.586+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.51.4110fd52-4dfd-4151-ab55-affa05d13ecf.tmp to hdfs://namenode:9000/spark_checkpoint/commits/51
[2024-11-12T09:45:51.587+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:51.588+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:51.588+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:51.588+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:51.588+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:50.606Z",
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "batchId" : 51,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7985611510791366,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0224948875255624,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "addBatch" : 397,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "commitOffsets" : 52,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:51.589+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:45:51.591+0000] {spark_submit.py:495} INFO - "queryPlanning" : 37,
[2024-11-12T09:45:51.592+0000] {spark_submit.py:495} INFO - "triggerExecution" : 978,
[2024-11-12T09:45:51.592+0000] {spark_submit.py:495} INFO - "walCommit" : 473
[2024-11-12T09:45:51.592+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "0" : 637
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:51.593+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - "0" : 638
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - "0" : 638
[2024-11-12T09:45:51.594+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:51.595+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:51.595+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:51.595+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7985611510791366,
[2024-11-12T09:45:51.595+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0224948875255624,
[2024-11-12T09:45:51.595+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:51.596+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:51.596+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:51.596+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:51.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:51.596+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:51.596+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:51.598+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:51.598+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:51.599+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:51.599+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:51.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/52 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.52.b9479a5e-d67d-4afd-8119-f636c8f935ab.tmp
[2024-11-12T09:45:51.651+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.52.b9479a5e-d67d-4afd-8119-f636c8f935ab.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/52
[2024-11-12T09:45:51.652+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MicroBatchExecution: Committed offsets for batch 52. Metadata OffsetSeqMetadata(0,1731404751597,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:51.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.680+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.706+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:51.714+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 50, 51, 51
[2024-11-12T09:45:51.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:51.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:51.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Got job 52 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:51.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Final stage: ResultStage 52 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:51.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:51.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:51.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[212] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:51.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:45:51.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:45:51.800+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:51.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:51.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[212] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:51.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
[2024-11-12T09:45:51.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:51.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:51 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:52.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 575 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:52.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool
[2024-11-12T09:45:52.384+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: ResultStage 52 (start at NativeMethodAccessorImpl.java:0) finished in 0.622 s
[2024-11-12T09:45:52.384+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:52.392+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
[2024-11-12T09:45:52.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Job 52 finished: start at NativeMethodAccessorImpl.java:0, took 0.631387 s
[2024-11-12T09:45:52.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO FileFormatWriter: Start to commit write Job f8da3d8f-8bf5-4726-895f-3a2b0983f6a1.
[2024-11-12T09:45:52.399+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/52 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.52.4775fbec-b35e-445a-92cb-acf6e9463fb1.tmp
[2024-11-12T09:45:52.447+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.52.4775fbec-b35e-445a-92cb-acf6e9463fb1.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/52
[2024-11-12T09:45:52.448+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO ManifestFileCommitProtocol: Committed batch 52
[2024-11-12T09:45:52.448+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO FileFormatWriter: Write Job f8da3d8f-8bf5-4726-895f-3a2b0983f6a1 committed. Elapsed time: 65 ms.
[2024-11-12T09:45:52.449+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO FileFormatWriter: Finished processing stats for write job f8da3d8f-8bf5-4726-895f-3a2b0983f6a1.
[2024-11-12T09:45:52.470+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/52 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.52.436cba0f-8200-421b-b2e4-178a762faa0f.tmp
[2024-11-12T09:45:52.512+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.52.436cba0f-8200-421b-b2e4-178a762faa0f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/52
[2024-11-12T09:45:52.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:52.514+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:52.514+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:52.515+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:52.515+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:51.586Z",
[2024-11-12T09:45:52.516+0000] {spark_submit.py:495} INFO - "batchId" : 52,
[2024-11-12T09:45:52.516+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:52.516+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0204081632653061,
[2024-11-12T09:45:52.517+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.079913606911447,
[2024-11-12T09:45:52.517+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:52.518+0000] {spark_submit.py:495} INFO - "addBatch" : 762,
[2024-11-12T09:45:52.518+0000] {spark_submit.py:495} INFO - "commitOffsets" : 64,
[2024-11-12T09:45:52.518+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:52.519+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:45:52.519+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:45:52.519+0000] {spark_submit.py:495} INFO - "triggerExecution" : 926,
[2024-11-12T09:45:52.519+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:45:52.519+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:52.519+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - "0" : 638
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:52.520+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:52.521+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:52.521+0000] {spark_submit.py:495} INFO - "0" : 639
[2024-11-12T09:45:52.521+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:52.521+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:52.522+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:52.522+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:52.523+0000] {spark_submit.py:495} INFO - "0" : 639
[2024-11-12T09:45:52.523+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:52.523+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:52.523+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:52.523+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0204081632653061,
[2024-11-12T09:45:52.523+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.079913606911447,
[2024-11-12T09:45:52.524+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:52.524+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:52.524+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:52.524+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:52.524+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:52.525+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:52.525+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:52.525+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:52.525+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:52.525+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:52.525+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:52.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/53 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.53.6696a874-ce12-4bbf-bc27-f09ff929a37b.tmp
[2024-11-12T09:45:52.578+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.53.6696a874-ce12-4bbf-bc27-f09ff929a37b.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/53
[2024-11-12T09:45:52.579+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO MicroBatchExecution: Committed offsets for batch 53. Metadata OffsetSeqMetadata(0,1731404752530,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:52.605+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:52.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:52.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:52.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:52.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 51, 52, 52
[2024-11-12T09:45:52.650+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:52.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:52.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Got job 53 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:52.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:52.700+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:52.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:52.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[216] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:52.740+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:45:52.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:45:52.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:52.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:52.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[216] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:52.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
[2024-11-12T09:45:52.759+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 53) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:52.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:52 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:53.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 53) in 632 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:53.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool
[2024-11-12T09:45:53.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.684 s
[2024-11-12T09:45:53.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:53.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
[2024-11-12T09:45:53.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Job 53 finished: start at NativeMethodAccessorImpl.java:0, took 0.690282 s
[2024-11-12T09:45:53.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO FileFormatWriter: Start to commit write Job 97de885b-9e9a-4ccd-aa17-8a99877102d5.
[2024-11-12T09:45:53.401+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/53 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.53.c12b944b-cabb-411c-a277-1cf2f6c0e512.tmp
[2024-11-12T09:45:53.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.53.c12b944b-cabb-411c-a277-1cf2f6c0e512.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/53
[2024-11-12T09:45:53.446+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO ManifestFileCommitProtocol: Committed batch 53
[2024-11-12T09:45:53.446+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO FileFormatWriter: Write Job 97de885b-9e9a-4ccd-aa17-8a99877102d5 committed. Elapsed time: 58 ms.
[2024-11-12T09:45:53.447+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO FileFormatWriter: Finished processing stats for write job 97de885b-9e9a-4ccd-aa17-8a99877102d5.
[2024-11-12T09:45:53.455+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/53 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.53.52ceb955-9664-495f-a692-d39cc11d59f2.tmp
[2024-11-12T09:45:53.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.53.52ceb955-9664-495f-a692-d39cc11d59f2.tmp to hdfs://namenode:9000/spark_checkpoint/commits/53
[2024-11-12T09:45:53.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:53.520+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:53.520+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:52.513Z",
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "batchId" : 53,
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0787486515641855,
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9950248756218907,
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "addBatch" : 832,
[2024-11-12T09:45:53.521+0000] {spark_submit.py:495} INFO - "commitOffsets" : 72,
[2024-11-12T09:45:53.522+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:53.523+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-12T09:45:53.523+0000] {spark_submit.py:495} INFO - "queryPlanning" : 30,
[2024-11-12T09:45:53.523+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1005,
[2024-11-12T09:45:53.523+0000] {spark_submit.py:495} INFO - "walCommit" : 48
[2024-11-12T09:45:53.524+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:53.524+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:53.524+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:53.524+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:53.525+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:53.525+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:53.525+0000] {spark_submit.py:495} INFO - "0" : 639
[2024-11-12T09:45:53.532+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:53.533+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:53.534+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:53.534+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:53.534+0000] {spark_submit.py:495} INFO - "0" : 640
[2024-11-12T09:45:53.536+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:53.536+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:53.536+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:53.536+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:53.536+0000] {spark_submit.py:495} INFO - "0" : 640
[2024-11-12T09:45:53.537+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:53.537+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:53.537+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:53.537+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0787486515641855,
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9950248756218907,
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:53.538+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:53.539+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:53.539+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:53.539+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:53.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/54 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.54.6450acf0-ed29-46dc-a1ae-8237bf31f6ef.tmp
[2024-11-12T09:45:53.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.54.6450acf0-ed29-46dc-a1ae-8237bf31f6ef.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/54
[2024-11-12T09:45:53.635+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO MicroBatchExecution: Committed offsets for batch 54. Metadata OffsetSeqMetadata(0,1731404753541,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:53.691+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:53.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:53.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:53.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:53.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 52, 53, 53
[2024-11-12T09:45:53.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:53.782+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:53.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Got job 54 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:53.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:53.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:53.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:53.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[220] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:53.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:45:53.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:45:53.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:45:53.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_49_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:53.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:53.848+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[220] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:53.849+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
[2024-11-12T09:45:53.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 54) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:53.851+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:53.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_51_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:53.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:53.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:53.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_52_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:53.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:53.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_50_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:53.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:53.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_53_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:53.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:53.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_48_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:53.934+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:53 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:54.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 54) in 556 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:54.407+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool
[2024-11-12T09:45:54.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.612 s
[2024-11-12T09:45:54.409+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:54.409+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
[2024-11-12T09:45:54.409+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Job 54 finished: start at NativeMethodAccessorImpl.java:0, took 0.626739 s
[2024-11-12T09:45:54.410+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO FileFormatWriter: Start to commit write Job 717e2a1b-7e8a-4803-b940-7a34c5f85dc2.
[2024-11-12T09:45:54.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/54 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.54.c8b93597-28c6-4f73-9993-57dfa878f6b6.tmp
[2024-11-12T09:45:54.480+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.54.c8b93597-28c6-4f73-9993-57dfa878f6b6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/54
[2024-11-12T09:45:54.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO ManifestFileCommitProtocol: Committed batch 54
[2024-11-12T09:45:54.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO FileFormatWriter: Write Job 717e2a1b-7e8a-4803-b940-7a34c5f85dc2 committed. Elapsed time: 71 ms.
[2024-11-12T09:45:54.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO FileFormatWriter: Finished processing stats for write job 717e2a1b-7e8a-4803-b940-7a34c5f85dc2.
[2024-11-12T09:45:54.492+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/54 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.54.daf5db92-a08b-4087-aed8-dd9c8fe25081.tmp
[2024-11-12T09:45:54.550+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.54.daf5db92-a08b-4087-aed8-dd9c8fe25081.tmp to hdfs://namenode:9000/spark_checkpoint/commits/54
[2024-11-12T09:45:54.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:54.555+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:53.519Z",
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "batchId" : 54,
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9940357852882704,
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9699321047526674,
[2024-11-12T09:45:54.556+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "addBatch" : 777,
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "commitOffsets" : 68,
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "latestOffset" : 22,
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "queryPlanning" : 63,
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1031,
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "walCommit" : 94
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:54.557+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:54.558+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:54.558+0000] {spark_submit.py:495} INFO - "0" : 640
[2024-11-12T09:45:54.558+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:54.558+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:54.558+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:54.558+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:54.559+0000] {spark_submit.py:495} INFO - "0" : 641
[2024-11-12T09:45:54.559+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:54.559+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:54.560+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:54.560+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:54.561+0000] {spark_submit.py:495} INFO - "0" : 641
[2024-11-12T09:45:54.561+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:54.561+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9940357852882704,
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9699321047526674,
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:54.562+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:54.563+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:54.563+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:54.563+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:54.563+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:54.563+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:54.563+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:54.564+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:54.588+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/55 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.55.3352657c-6865-4f58-ad6b-d4fbfe8a5f22.tmp
[2024-11-12T09:45:54.645+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.55.3352657c-6865-4f58-ad6b-d4fbfe8a5f22.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/55
[2024-11-12T09:45:54.650+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO MicroBatchExecution: Committed offsets for batch 55. Metadata OffsetSeqMetadata(0,1731404754564,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:54.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:54.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:54.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:54.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:54.689+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 53, 54, 54
[2024-11-12T09:45:54.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:54.724+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:54.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Got job 55 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:54.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Final stage: ResultStage 55 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:54.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:54.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:54.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[224] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:54.756+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:45:54.760+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:45:54.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:54.763+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:54.764+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[224] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:54.764+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
[2024-11-12T09:45:54.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 55) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:54.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:54 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:55.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 55) in 662 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:55.429+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool
[2024-11-12T09:45:55.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO DAGScheduler: ResultStage 55 (start at NativeMethodAccessorImpl.java:0) finished in 0.701 s
[2024-11-12T09:45:55.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:55.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
[2024-11-12T09:45:55.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO DAGScheduler: Job 55 finished: start at NativeMethodAccessorImpl.java:0, took 0.705401 s
[2024-11-12T09:45:55.433+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO FileFormatWriter: Start to commit write Job 1129ab49-9eff-4038-bc18-3c213b954b43.
[2024-11-12T09:45:55.449+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/55 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.55.961dfa69-d3ad-42a2-a8a8-386da3dcaebb.tmp
[2024-11-12T09:45:55.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.55.961dfa69-d3ad-42a2-a8a8-386da3dcaebb.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/55
[2024-11-12T09:45:55.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO ManifestFileCommitProtocol: Committed batch 55
[2024-11-12T09:45:55.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO FileFormatWriter: Write Job 1129ab49-9eff-4038-bc18-3c213b954b43 committed. Elapsed time: 70 ms.
[2024-11-12T09:45:55.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO FileFormatWriter: Finished processing stats for write job 1129ab49-9eff-4038-bc18-3c213b954b43.
[2024-11-12T09:45:55.523+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/55 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.55.98db1d83-c876-407f-8af5-7775172cbbf5.tmp
[2024-11-12T09:45:55.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.55.98db1d83-c876-407f-8af5-7775172cbbf5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/55
[2024-11-12T09:45:55.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:55.972+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:55.972+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:55.973+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:55.973+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:54.552Z",
[2024-11-12T09:45:55.974+0000] {spark_submit.py:495} INFO - "batchId" : 55,
[2024-11-12T09:45:55.975+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9680542110358181,
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7052186177715092,
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "addBatch" : 836,
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "commitOffsets" : 468,
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:55.976+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:45:55.977+0000] {spark_submit.py:495} INFO - "queryPlanning" : 17,
[2024-11-12T09:45:55.977+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1418,
[2024-11-12T09:45:55.978+0000] {spark_submit.py:495} INFO - "walCommit" : 83
[2024-11-12T09:45:55.978+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:55.978+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:55.978+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:55.978+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:55.978+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:55.979+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:55.979+0000] {spark_submit.py:495} INFO - "0" : 641
[2024-11-12T09:45:55.981+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:55.982+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:55.983+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:55.983+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:55.983+0000] {spark_submit.py:495} INFO - "0" : 642
[2024-11-12T09:45:55.984+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:55.985+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:55.986+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:55.987+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:55.987+0000] {spark_submit.py:495} INFO - "0" : 642
[2024-11-12T09:45:55.987+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:55.987+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:55.988+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:55.988+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9680542110358181,
[2024-11-12T09:45:55.988+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7052186177715092,
[2024-11-12T09:45:55.989+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:55.989+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:55.989+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:55.990+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:55.990+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:55.990+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:55.996+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:55.997+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:55.998+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:55.998+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:55.998+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:55.998+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/56 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.56.74d88a75-dd04-4790-a1d8-b779478f8b96.tmp
[2024-11-12T09:45:56.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.56.74d88a75-dd04-4790-a1d8-b779478f8b96.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/56
[2024-11-12T09:45:56.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MicroBatchExecution: Committed offsets for batch 56. Metadata OffsetSeqMetadata(0,1731404755977,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:56.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.093+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 54, 55, 55
[2024-11-12T09:45:56.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:56.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:56.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Got job 56 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:56.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:56.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:56.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:56.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[228] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:56.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:45:56.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:45:56.209+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:56.210+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:56.210+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[228] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:56.210+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
[2024-11-12T09:45:56.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 56) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:56.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:56.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 56) in 199 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:56.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool
[2024-11-12T09:45:56.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.239 s
[2024-11-12T09:45:56.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:56.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
[2024-11-12T09:45:56.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Job 56 finished: start at NativeMethodAccessorImpl.java:0, took 0.255026 s
[2024-11-12T09:45:56.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO FileFormatWriter: Start to commit write Job fc264825-96cf-4474-ba57-540f33d3a31c.
[2024-11-12T09:45:56.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/56 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.56.04adb3cd-d7a4-4b14-b4cf-e682981ae69a.tmp
[2024-11-12T09:45:56.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.56.04adb3cd-d7a4-4b14-b4cf-e682981ae69a.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/56
[2024-11-12T09:45:56.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO ManifestFileCommitProtocol: Committed batch 56
[2024-11-12T09:45:56.479+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO FileFormatWriter: Write Job fc264825-96cf-4474-ba57-540f33d3a31c committed. Elapsed time: 59 ms.
[2024-11-12T09:45:56.479+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO FileFormatWriter: Finished processing stats for write job fc264825-96cf-4474-ba57-540f33d3a31c.
[2024-11-12T09:45:56.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/56 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.56.c801f870-56fb-4aaf-a7a8-a7cb1a83599f.tmp
[2024-11-12T09:45:56.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.56.c801f870-56fb-4aaf-a7a8-a7cb1a83599f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/56
[2024-11-12T09:45:56.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:56.542+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:56.543+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:56.544+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:56.544+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:55.971Z",
[2024-11-12T09:45:56.544+0000] {spark_submit.py:495} INFO - "batchId" : 56,
[2024-11-12T09:45:56.545+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.704721634954193,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.7605633802816902,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "addBatch" : 400,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "commitOffsets" : 61,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:45:56.546+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:45:56.547+0000] {spark_submit.py:495} INFO - "triggerExecution" : 568,
[2024-11-12T09:45:56.547+0000] {spark_submit.py:495} INFO - "walCommit" : 67
[2024-11-12T09:45:56.547+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:56.548+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:56.548+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "0" : 642
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:56.549+0000] {spark_submit.py:495} INFO - "0" : 643
[2024-11-12T09:45:56.550+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:56.558+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:56.561+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:56.561+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:56.561+0000] {spark_submit.py:495} INFO - "0" : 643
[2024-11-12T09:45:56.561+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:56.561+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.704721634954193,
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.7605633802816902,
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:56.562+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:56.563+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:56.563+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:56.563+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:56.563+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:56.563+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:56.563+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:56.564+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:56.569+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/57 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.57.c5d94634-c3d6-4bf6-a748-d334f59bd2cc.tmp
[2024-11-12T09:45:56.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.57.c5d94634-c3d6-4bf6-a748-d334f59bd2cc.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/57
[2024-11-12T09:45:56.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MicroBatchExecution: Committed offsets for batch 57. Metadata OffsetSeqMetadata(0,1731404756557,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:56.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.635+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:56.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 55, 56, 56
[2024-11-12T09:45:56.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:56.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:56.683+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Got job 57 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:56.685+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:56.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:56.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:56.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[232] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:56.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:45:56.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:45:56.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:56.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:56.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[232] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:56.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
[2024-11-12T09:45:56.724+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 57) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:56.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:56 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:57.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 57) in 640 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:57.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool
[2024-11-12T09:45:57.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.678 s
[2024-11-12T09:45:57.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:57.367+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
[2024-11-12T09:45:57.367+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO DAGScheduler: Job 57 finished: start at NativeMethodAccessorImpl.java:0, took 0.685133 s
[2024-11-12T09:45:57.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO FileFormatWriter: Start to commit write Job d48b6508-aa24-41a4-80ca-35d186a8fd71.
[2024-11-12T09:45:57.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/57 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.57.19ef60d5-f70f-49d7-8488-541826ce2085.tmp
[2024-11-12T09:45:57.416+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.57.19ef60d5-f70f-49d7-8488-541826ce2085.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/57
[2024-11-12T09:45:57.417+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO ManifestFileCommitProtocol: Committed batch 57
[2024-11-12T09:45:57.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO FileFormatWriter: Write Job d48b6508-aa24-41a4-80ca-35d186a8fd71 committed. Elapsed time: 47 ms.
[2024-11-12T09:45:57.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO FileFormatWriter: Finished processing stats for write job d48b6508-aa24-41a4-80ca-35d186a8fd71.
[2024-11-12T09:45:57.426+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/57 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.57.3ed2c4d2-a9af-434d-a915-39293e50cb7a.tmp
[2024-11-12T09:45:57.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.57.3ed2c4d2-a9af-434d-a915-39293e50cb7a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/57
[2024-11-12T09:45:57.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:57.869+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:57.870+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:57.871+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:57.871+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:56.540Z",
[2024-11-12T09:45:57.871+0000] {spark_submit.py:495} INFO - "batchId" : 57,
[2024-11-12T09:45:57.871+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:57.871+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7574692442882252,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.753012048192771,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "addBatch" : 788,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "commitOffsets" : 451,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:45:57.872+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1328,
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "0" : 643
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:57.873+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - "0" : 644
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - "0" : 644
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:57.874+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7574692442882252,
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.753012048192771,
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:57.875+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:57.876+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:57.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/58 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.58.915d9bc6-28f0-4da3-aa55-e6f8c27cec59.tmp
[2024-11-12T09:45:57.933+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.58.915d9bc6-28f0-4da3-aa55-e6f8c27cec59.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/58
[2024-11-12T09:45:57.934+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO MicroBatchExecution: Committed offsets for batch 58. Metadata OffsetSeqMetadata(0,1731404757876,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:57.964+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:57.967+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:57.979+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:57.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:57.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:57 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 56, 57, 57
[2024-11-12T09:45:58.004+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:58.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:58.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Got job 58 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:58.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Final stage: ResultStage 58 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:58.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:58.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:58.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:58.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:45:58.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:45:58.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:58.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:58.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[236] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:58.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0
[2024-11-12T09:45:58.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 58) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:58.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:58.417+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 58) in 329 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:58.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool
[2024-11-12T09:45:58.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: ResultStage 58 (start at NativeMethodAccessorImpl.java:0) finished in 0.363 s
[2024-11-12T09:45:58.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:58.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
[2024-11-12T09:45:58.422+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Job 58 finished: start at NativeMethodAccessorImpl.java:0, took 0.374510 s
[2024-11-12T09:45:58.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO FileFormatWriter: Start to commit write Job 54060687-f5b1-4f63-b5d3-2b4a5bfb98c1.
[2024-11-12T09:45:58.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/58 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.58.62205662-c7b1-49e4-92b2-02e5754497da.tmp
[2024-11-12T09:45:58.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.58.62205662-c7b1-49e4-92b2-02e5754497da.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/58
[2024-11-12T09:45:58.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO ManifestFileCommitProtocol: Committed batch 58
[2024-11-12T09:45:58.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO FileFormatWriter: Write Job 54060687-f5b1-4f63-b5d3-2b4a5bfb98c1 committed. Elapsed time: 57 ms.
[2024-11-12T09:45:58.479+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO FileFormatWriter: Finished processing stats for write job 54060687-f5b1-4f63-b5d3-2b4a5bfb98c1.
[2024-11-12T09:45:58.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/58 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.58.7bbd27fc-2d4c-4197-bcca-1635e7cda82b.tmp
[2024-11-12T09:45:58.537+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.58.7bbd27fc-2d4c-4197-bcca-1635e7cda82b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/58
[2024-11-12T09:45:58.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:58.539+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:58.539+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:58.539+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:58.540+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:57.870Z",
[2024-11-12T09:45:58.540+0000] {spark_submit.py:495} INFO - "batchId" : 58,
[2024-11-12T09:45:58.540+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:58.540+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7518796992481203,
[2024-11-12T09:45:58.540+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4992503748125936,
[2024-11-12T09:45:58.541+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:58.541+0000] {spark_submit.py:495} INFO - "addBatch" : 508,
[2024-11-12T09:45:58.541+0000] {spark_submit.py:495} INFO - "commitOffsets" : 59,
[2024-11-12T09:45:58.541+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:58.541+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:45:58.542+0000] {spark_submit.py:495} INFO - "queryPlanning" : 34,
[2024-11-12T09:45:58.542+0000] {spark_submit.py:495} INFO - "triggerExecution" : 667,
[2024-11-12T09:45:58.542+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:45:58.542+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:58.543+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:58.543+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:58.543+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:58.544+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:58.544+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:58.544+0000] {spark_submit.py:495} INFO - "0" : 644
[2024-11-12T09:45:58.545+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:58.545+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:58.545+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:58.545+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:58.546+0000] {spark_submit.py:495} INFO - "0" : 645
[2024-11-12T09:45:58.546+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "0" : 645
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7518796992481203,
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4992503748125936,
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:58.547+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:58.548+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:58.559+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/59 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.59.390658da-006c-4f41-8c4f-4b1000ee72fb.tmp
[2024-11-12T09:45:58.605+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.59.390658da-006c-4f41-8c4f-4b1000ee72fb.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/59
[2024-11-12T09:45:58.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO MicroBatchExecution: Committed offsets for batch 59. Metadata OffsetSeqMetadata(0,1731404758549,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:58.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:58.634+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:58.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:58.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:58.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 57, 58, 58
[2024-11-12T09:45:58.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:58.691+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:58.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Got job 59 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:58.693+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:58.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:58.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:58.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[240] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:58.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:45:58.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:45:58.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:58.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:58.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[240] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:58.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0
[2024-11-12T09:45:58.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 59) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:58.760+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:58 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:59.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 59) in 654 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:45:59.384+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool
[2024-11-12T09:45:59.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.689 s
[2024-11-12T09:45:59.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:45:59.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
[2024-11-12T09:45:59.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Job 59 finished: start at NativeMethodAccessorImpl.java:0, took 0.693753 s
[2024-11-12T09:45:59.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO FileFormatWriter: Start to commit write Job 6865331c-a888-4a63-9843-db4d037b642c.
[2024-11-12T09:45:59.395+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/59.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.59.compact.1d6420af-f0a8-47a9-b1f5-cf7e40d74daa.tmp
[2024-11-12T09:45:59.530+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.59.compact.1d6420af-f0a8-47a9-b1f5-cf7e40d74daa.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/59.compact
[2024-11-12T09:45:59.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO ManifestFileCommitProtocol: Committed batch 59
[2024-11-12T09:45:59.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO FileFormatWriter: Write Job 6865331c-a888-4a63-9843-db4d037b642c committed. Elapsed time: 144 ms.
[2024-11-12T09:45:59.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO FileFormatWriter: Finished processing stats for write job 6865331c-a888-4a63-9843-db4d037b642c.
[2024-11-12T09:45:59.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/59 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.59.8c7d466e-7985-4dd2-84a9-68ae2774a8b5.tmp
[2024-11-12T09:45:59.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.59.8c7d466e-7985-4dd2-84a9-68ae2774a8b5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/59
[2024-11-12T09:45:59.577+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:45:59.577+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:45:59.577+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:45:59.578+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:45:59.578+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:58.538Z",
[2024-11-12T09:45:59.578+0000] {spark_submit.py:495} INFO - "batchId" : 59,
[2024-11-12T09:45:59.578+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:59.578+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.497005988023952,
[2024-11-12T09:45:59.578+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9643201542912248,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "addBatch" : 897,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "commitOffsets" : 42,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1037,
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "walCommit" : 56
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:45:59.579+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - "0" : 645
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:45:59.580+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - "0" : 646
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - "0" : 646
[2024-11-12T09:45:59.581+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:59.588+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:45:59.589+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:45:59.590+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.497005988023952,
[2024-11-12T09:45:59.591+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9643201542912248,
[2024-11-12T09:45:59.591+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:45:59.592+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:45:59.592+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:45:59.592+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:45:59.592+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:59.593+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:45:59.593+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:45:59.593+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:45:59.593+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:45:59.593+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:59.593+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:45:59.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/60 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.60.ef5ec6d1-6c78-4b7a-96e2-460566960116.tmp
[2024-11-12T09:45:59.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.60.ef5ec6d1-6c78-4b7a-96e2-460566960116.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/60
[2024-11-12T09:45:59.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO MicroBatchExecution: Committed offsets for batch 60. Metadata OffsetSeqMetadata(0,1731404759593,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:45:59.655+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:59.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:59.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:59.673+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:45:59.689+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 57, 58, 58, 59
[2024-11-12T09:45:59.691+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:45:59.724+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:45:59.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Got job 60 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:45:59.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:45:59.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:45:59.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:45:59.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[244] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:45:59.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_56_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:59.763+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 320.7 KiB, free 431.7 MiB)
[2024-11-12T09:45:59.766+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:45:59.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:45:59.768+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:45:59.768+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:59.768+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[244] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:45:59.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
[2024-11-12T09:45:59.771+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 60) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:45:59.774+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_59_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:45:59.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:59.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_57_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:59.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:59.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:45:59.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_55_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:59.815+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:45:59.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_58_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:59.831+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:45:59.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_54_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:45:59.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:45:59 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:46:00.437+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 60) in 666 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:46:00.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool
[2024-11-12T09:46:00.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.709 s
[2024-11-12T09:46:00.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:46:00.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
[2024-11-12T09:46:00.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO DAGScheduler: Job 60 finished: start at NativeMethodAccessorImpl.java:0, took 0.714329 s
[2024-11-12T09:46:00.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO FileFormatWriter: Start to commit write Job 79302aed-0b7a-43b8-ae65-781379dd44ab.
[2024-11-12T09:46:00.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/60 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.60.4a74ea5d-2c2d-443f-bee2-487236d456e0.tmp
[2024-11-12T09:46:00.894+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.60.4a74ea5d-2c2d-443f-bee2-487236d456e0.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/60
[2024-11-12T09:46:00.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO ManifestFileCommitProtocol: Committed batch 60
[2024-11-12T09:46:00.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO FileFormatWriter: Write Job 79302aed-0b7a-43b8-ae65-781379dd44ab committed. Elapsed time: 450 ms.
[2024-11-12T09:46:00.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO FileFormatWriter: Finished processing stats for write job 79302aed-0b7a-43b8-ae65-781379dd44ab.
[2024-11-12T09:46:00.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/60 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.60.e557ea4d-7dfa-4510-aedb-eaef529b0417.tmp
[2024-11-12T09:46:00.948+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.60.e557ea4d-7dfa-4510-aedb-eaef529b0417.tmp to hdfs://namenode:9000/spark_checkpoint/commits/60
[2024-11-12T09:46:00.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:00.949+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:00.949+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:00.949+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:00.950+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:45:59.577Z",
[2024-11-12T09:46:00.950+0000] {spark_submit.py:495} INFO - "batchId" : 60,
[2024-11-12T09:46:00.950+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:00.950+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9624639076034649,
[2024-11-12T09:46:00.951+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7304601899196493,
[2024-11-12T09:46:00.951+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:00.951+0000] {spark_submit.py:495} INFO - "addBatch" : 1235,
[2024-11-12T09:46:00.951+0000] {spark_submit.py:495} INFO - "commitOffsets" : 51,
[2024-11-12T09:46:00.951+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:46:00.951+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:46:00.952+0000] {spark_submit.py:495} INFO - "queryPlanning" : 33,
[2024-11-12T09:46:00.952+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1369,
[2024-11-12T09:46:00.952+0000] {spark_submit.py:495} INFO - "walCommit" : 33
[2024-11-12T09:46:00.952+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:00.952+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:00.952+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:00.953+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:00.953+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:00.953+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:00.953+0000] {spark_submit.py:495} INFO - "0" : 646
[2024-11-12T09:46:00.954+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:00.954+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:00.954+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:00.955+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:00.955+0000] {spark_submit.py:495} INFO - "0" : 647
[2024-11-12T09:46:00.955+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:00.956+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:00.956+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:00.956+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:00.956+0000] {spark_submit.py:495} INFO - "0" : 647
[2024-11-12T09:46:00.956+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9624639076034649,
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7304601899196493,
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:00.957+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:00.958+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:00.958+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:00.958+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:00.959+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:00.959+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:00.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/61 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.61.5f3be71c-6d51-4e6b-823e-0ca7f3ec99fd.tmp
[2024-11-12T09:46:01.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.61.5f3be71c-6d51-4e6b-823e-0ca7f3ec99fd.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/61
[2024-11-12T09:46:01.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MicroBatchExecution: Committed offsets for batch 61. Metadata OffsetSeqMetadata(0,1731404760956,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:46:01.020+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.022+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 58, 59, 60, 60
[2024-11-12T09:46:01.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:46:01.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:46:01.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Got job 61 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:46:01.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Final stage: ResultStage 61 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:46:01.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:46:01.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:46:01.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[248] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:46:01.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:46:01.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:46:01.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:46:01.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:46:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[248] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:46:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0
[2024-11-12T09:46:01.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 61) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:46:01.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:46:01.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 61) in 321 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:46:01.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool
[2024-11-12T09:46:01.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: ResultStage 61 (start at NativeMethodAccessorImpl.java:0) finished in 0.351 s
[2024-11-12T09:46:01.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:46:01.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
[2024-11-12T09:46:01.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Job 61 finished: start at NativeMethodAccessorImpl.java:0, took 0.353169 s
[2024-11-12T09:46:01.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO FileFormatWriter: Start to commit write Job 0f0b0a5d-1681-4935-9082-dea17167a77e.
[2024-11-12T09:46:01.439+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/61 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.61.c26895d5-83b4-4de1-a195-c8f183b1e9d6.tmp
[2024-11-12T09:46:01.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.61.c26895d5-83b4-4de1-a195-c8f183b1e9d6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/61
[2024-11-12T09:46:01.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO ManifestFileCommitProtocol: Committed batch 61
[2024-11-12T09:46:01.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO FileFormatWriter: Write Job 0f0b0a5d-1681-4935-9082-dea17167a77e committed. Elapsed time: 45 ms.
[2024-11-12T09:46:01.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO FileFormatWriter: Finished processing stats for write job 0f0b0a5d-1681-4935-9082-dea17167a77e.
[2024-11-12T09:46:01.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/61 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.61.bfc3a8c4-c3b5-487f-98c4-adc86cd6da0f.tmp
[2024-11-12T09:46:01.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.61.bfc3a8c4-c3b5-487f-98c4-adc86cd6da0f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/61
[2024-11-12T09:46:01.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:01.535+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:01.535+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:01.535+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:01.536+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:00.948Z",
[2024-11-12T09:46:01.536+0000] {spark_submit.py:495} INFO - "batchId" : 61,
[2024-11-12T09:46:01.536+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:01.537+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7293946024799417,
[2024-11-12T09:46:01.538+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.7123287671232879,
[2024-11-12T09:46:01.539+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "addBatch" : 441,
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "commitOffsets" : 54,
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "triggerExecution" : 584,
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "walCommit" : 45
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:01.540+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "0" : 647
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:01.541+0000] {spark_submit.py:495} INFO - "0" : 648
[2024-11-12T09:46:01.542+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:01.542+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:01.542+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:01.542+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:01.545+0000] {spark_submit.py:495} INFO - "0" : 648
[2024-11-12T09:46:01.552+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:01.554+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:01.554+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:01.555+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7293946024799417,
[2024-11-12T09:46:01.555+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.7123287671232879,
[2024-11-12T09:46:01.555+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:01.555+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:01.555+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:01.556+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:01.557+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:01.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/62 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.62.c04ea5be-c3c9-4474-8a1d-1d3c93ebd434.tmp
[2024-11-12T09:46:01.599+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.62.c04ea5be-c3c9-4474-8a1d-1d3c93ebd434.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/62
[2024-11-12T09:46:01.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MicroBatchExecution: Committed offsets for batch 62. Metadata OffsetSeqMetadata(0,1731404761541,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:46:01.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:01.671+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 60, 61, 61
[2024-11-12T09:46:01.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:46:01.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:46:01.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Got job 62 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:46:01.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:46:01.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:46:01.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:46:01.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:46:01.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:46:01.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:46:01.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:46:01.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:46:01.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[252] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:46:01.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2024-11-12T09:46:01.756+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 62) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:46:01.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:01 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:46:02.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 62) in 696 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:46:02.453+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2024-11-12T09:46:02.456+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.732 s
[2024-11-12T09:46:02.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:46:02.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
[2024-11-12T09:46:02.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Job 62 finished: start at NativeMethodAccessorImpl.java:0, took 0.739491 s
[2024-11-12T09:46:02.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO FileFormatWriter: Start to commit write Job 6d352381-d0fa-4eb7-8b90-922e4097b15f.
[2024-11-12T09:46:02.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/62 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.62.6532b090-ec47-4850-a1e3-47055c377267.tmp
[2024-11-12T09:46:02.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.62.6532b090-ec47-4850-a1e3-47055c377267.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/62
[2024-11-12T09:46:02.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO ManifestFileCommitProtocol: Committed batch 62
[2024-11-12T09:46:02.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO FileFormatWriter: Write Job 6d352381-d0fa-4eb7-8b90-922e4097b15f committed. Elapsed time: 106 ms.
[2024-11-12T09:46:02.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO FileFormatWriter: Finished processing stats for write job 6d352381-d0fa-4eb7-8b90-922e4097b15f.
[2024-11-12T09:46:02.572+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/62 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.62.3800c9dd-913f-4599-8166-5bb678961502.tmp
[2024-11-12T09:46:02.639+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.62.3800c9dd-913f-4599-8166-5bb678961502.tmp to hdfs://namenode:9000/spark_checkpoint/commits/62
[2024-11-12T09:46:02.641+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:02.642+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:02.642+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:02.642+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:02.642+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:01.533Z",
[2024-11-12T09:46:02.642+0000] {spark_submit.py:495} INFO - "batchId" : 62,
[2024-11-12T09:46:02.642+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7094017094017095,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9041591320072332,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "addBatch" : 924,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "commitOffsets" : 77,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "queryPlanning" : 32,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1106,
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - "walCommit" : 59
[2024-11-12T09:46:02.643+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "0" : 648
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - "0" : 649
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:02.644+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "0" : 649
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7094017094017095,
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9041591320072332,
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:02.645+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:02.646+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:02.646+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:02.646+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:02.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:02.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:02.663+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/63 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.63.a23e1ab7-83e6-41c0-aeeb-b461e591695d.tmp
[2024-11-12T09:46:02.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.63.a23e1ab7-83e6-41c0-aeeb-b461e591695d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/63
[2024-11-12T09:46:02.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO MicroBatchExecution: Committed offsets for batch 63. Metadata OffsetSeqMetadata(0,1731404762646,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:46:02.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:02.728+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:02.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:02.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:02.766+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 61, 62, 62
[2024-11-12T09:46:02.766+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:46:02.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:46:02.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Got job 63 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:46:02.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:46:02.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:46:02.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:46:02.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[256] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:46:02.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:46:02.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:46:02.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:46:02.815+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:46:02.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[256] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:46:02.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
[2024-11-12T09:46:02.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 63) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:46:02.838+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:02 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:46:03.512+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 63) in 695 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:46:03.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool
[2024-11-12T09:46:03.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.720 s
[2024-11-12T09:46:03.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:46:03.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
[2024-11-12T09:46:03.516+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Job 63 finished: start at NativeMethodAccessorImpl.java:0, took 0.723380 s
[2024-11-12T09:46:03.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO FileFormatWriter: Start to commit write Job 41d378c0-5e1c-4740-b06b-67ae218e3dba.
[2024-11-12T09:46:03.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/63 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.63.20981293-8ed0-4bca-abac-e3315f070b85.tmp
[2024-11-12T09:46:03.577+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.63.20981293-8ed0-4bca-abac-e3315f070b85.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/63
[2024-11-12T09:46:03.578+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO ManifestFileCommitProtocol: Committed batch 63
[2024-11-12T09:46:03.578+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO FileFormatWriter: Write Job 41d378c0-5e1c-4740-b06b-67ae218e3dba committed. Elapsed time: 60 ms.
[2024-11-12T09:46:03.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO FileFormatWriter: Finished processing stats for write job 41d378c0-5e1c-4740-b06b-67ae218e3dba.
[2024-11-12T09:46:03.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/63 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.63.faac8f3f-b5c7-44e6-9d71-14c050b736be.tmp
[2024-11-12T09:46:03.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.63.faac8f3f-b5c7-44e6-9d71-14c050b736be.tmp to hdfs://namenode:9000/spark_checkpoint/commits/63
[2024-11-12T09:46:03.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:03.662+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:03.662+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:03.662+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:03.662+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:02.641Z",
[2024-11-12T09:46:03.663+0000] {spark_submit.py:495} INFO - "batchId" : 63,
[2024-11-12T09:46:03.663+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:03.663+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9025270758122743,
[2024-11-12T09:46:03.663+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9852216748768474,
[2024-11-12T09:46:03.663+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:03.663+0000] {spark_submit.py:495} INFO - "addBatch" : 851,
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "commitOffsets" : 73,
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "queryPlanning" : 23,
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1015,
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "walCommit" : 60
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:03.664+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "0" : 649
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "0" : 650
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - "0" : 650
[2024-11-12T09:46:03.665+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9025270758122743,
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9852216748768474,
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:03.666+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:03.667+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:03.667+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:03.687+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/64 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.64.502155ef-2901-4e93-8636-ca9854ccbc92.tmp
[2024-11-12T09:46:03.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.64.502155ef-2901-4e93-8636-ca9854ccbc92.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/64
[2024-11-12T09:46:03.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO MicroBatchExecution: Committed offsets for batch 64. Metadata OffsetSeqMetadata(0,1731404763669,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:46:03.772+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:03.775+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:03.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:03.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:03.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 62, 63, 63
[2024-11-12T09:46:03.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:46:03.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:46:03.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Got job 64 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:46:03.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Final stage: ResultStage 64 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:46:03.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:46:03.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:46:03.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[260] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:46:03.855+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:46:03.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:46:03.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:46:03.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:46:03.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[260] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:46:03.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0
[2024-11-12T09:46:03.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 64) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:46:03.889+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:03 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:46:04.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 64) in 608 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:46:04.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2024-11-12T09:46:04.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO DAGScheduler: ResultStage 64 (start at NativeMethodAccessorImpl.java:0) finished in 0.639 s
[2024-11-12T09:46:04.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:46:04.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
[2024-11-12T09:46:04.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO DAGScheduler: Job 64 finished: start at NativeMethodAccessorImpl.java:0, took 0.642610 s
[2024-11-12T09:46:04.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO FileFormatWriter: Start to commit write Job 93255326-c0d5-4b70-ad1a-11d21598183d.
[2024-11-12T09:46:04.480+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/64 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.64.12498f7e-a538-4508-9082-e46fbaf24370.tmp
[2024-11-12T09:46:04.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.64.12498f7e-a538-4508-9082-e46fbaf24370.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/64
[2024-11-12T09:46:04.520+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO ManifestFileCommitProtocol: Committed batch 64
[2024-11-12T09:46:04.520+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO FileFormatWriter: Write Job 93255326-c0d5-4b70-ad1a-11d21598183d committed. Elapsed time: 42 ms.
[2024-11-12T09:46:04.521+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO FileFormatWriter: Finished processing stats for write job 93255326-c0d5-4b70-ad1a-11d21598183d.
[2024-11-12T09:46:04.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/64 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.64.23b277f5-8900-422a-bab9-b9de29b64e02.tmp
[2024-11-12T09:46:04.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.64.23b277f5-8900-422a-bab9-b9de29b64e02.tmp to hdfs://namenode:9000/spark_checkpoint/commits/64
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:03.663Z",
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "batchId" : 64,
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9784735812133072,
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1049723756906078,
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:04.570+0000] {spark_submit.py:495} INFO - "addBatch" : 740,
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "commitOffsets" : 49,
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "queryPlanning" : 34,
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "triggerExecution" : 905,
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "walCommit" : 73
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:04.571+0000] {spark_submit.py:495} INFO - "0" : 650
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "0" : 651
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "0" : 651
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9784735812133072,
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1049723756906078,
[2024-11-12T09:46:04.572+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:04.573+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:04.579+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/65 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.65.8ee02e56-bc11-4f0a-abaf-05e0eb3de559.tmp
[2024-11-12T09:46:05.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.65.8ee02e56-bc11-4f0a-abaf-05e0eb3de559.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/65
[2024-11-12T09:46:05.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO MicroBatchExecution: Committed offsets for batch 65. Metadata OffsetSeqMetadata(0,1731404764573,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:46:05.061+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:05.063+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:05.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:05.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:46:05.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 63, 64, 64
[2024-11-12T09:46:05.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:46:05.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:46:05.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Got job 65 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:46:05.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:46:05.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:46:05.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:46:05.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[264] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:46:05.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:46:05.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.0 MiB)
[2024-11-12T09:46:05.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_60_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:46:05.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:46:05.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:46:05.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[264] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:46:05.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
[2024-11-12T09:46:05.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:46:05.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 65) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:46:05.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_62_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:46:05.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:46:05.218+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_64_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:46:05.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:46:05.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:46:05.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_61_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:46:05.243+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:46:05.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_63_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:46:05.261+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:46:05.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 65) in 698 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:46:05.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool
[2024-11-12T09:46:05.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.741 s
[2024-11-12T09:46:05.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:46:05.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
[2024-11-12T09:46:05.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO DAGScheduler: Job 65 finished: start at NativeMethodAccessorImpl.java:0, took 0.753475 s
[2024-11-12T09:46:05.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO FileFormatWriter: Start to commit write Job 94bf5fc1-30cf-4def-9d52-5f9c0c303a8f.
[2024-11-12T09:46:05.908+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/65 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.65.566fe717-e221-48a2-9a43-43c6297464f9.tmp
[2024-11-12T09:46:05.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.65.566fe717-e221-48a2-9a43-43c6297464f9.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/65
[2024-11-12T09:46:05.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO ManifestFileCommitProtocol: Committed batch 65
[2024-11-12T09:46:05.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO FileFormatWriter: Write Job 94bf5fc1-30cf-4def-9d52-5f9c0c303a8f committed. Elapsed time: 83 ms.
[2024-11-12T09:46:05.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO FileFormatWriter: Finished processing stats for write job 94bf5fc1-30cf-4def-9d52-5f9c0c303a8f.
[2024-11-12T09:46:05.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/65 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.65.228d9caf-0737-4ba9-9fb1-8006d925351c.tmp
[2024-11-12T09:46:06.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:06 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.65.228d9caf-0737-4ba9-9fb1-8006d925351c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/65
[2024-11-12T09:46:06.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:06 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:06.488+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:06.488+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:06.488+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:06.489+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:04.570Z",
[2024-11-12T09:46:06.489+0000] {spark_submit.py:495} INFO - "batchId" : 65,
[2024-11-12T09:46:06.490+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:06.490+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1025358324145533,
[2024-11-12T09:46:06.492+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.522466039707419,
[2024-11-12T09:46:06.492+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:06.493+0000] {spark_submit.py:495} INFO - "addBatch" : 908,
[2024-11-12T09:46:06.493+0000] {spark_submit.py:495} INFO - "commitOffsets" : 504,
[2024-11-12T09:46:06.494+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "queryPlanning" : 35,
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1914,
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "walCommit" : 460
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:06.495+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - "0" : 651
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:06.496+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:06.497+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:46:06.498+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1025358324145533,
[2024-11-12T09:46:06.498+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.522466039707419,
[2024-11-12T09:46:06.501+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:06.501+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:06.501+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:06.501+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:06.502+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:06.502+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:06.502+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:06.502+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:06.502+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:06.503+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:06.503+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:16.494Z",
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:46:16.501+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:16.502+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:16.502+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:16.502+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:16.503+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:46:16.503+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:46:16.503+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:16.504+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:16.505+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:16.506+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:16.507+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:16.507+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:16.507+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:16.507+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:26.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:26.515+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:26.516+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:26.516+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:26.516+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:26.502Z",
[2024-11-12T09:46:26.516+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "triggerExecution" : 11
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:26.517+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:26.518+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:26.518+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:26.518+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:26.518+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:26.518+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:26.518+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:26.519+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:26.520+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:26.521+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:26.521+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:36.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:36.503Z",
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:36.520+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:36.521+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:36.521+0000] {spark_submit.py:495} INFO - "latestOffset" : 14,
[2024-11-12T09:46:36.521+0000] {spark_submit.py:495} INFO - "triggerExecution" : 15
[2024-11-12T09:46:36.521+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:36.521+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:36.521+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:36.522+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:36.523+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:36.523+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:36.524+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:36.525+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:36.525+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:36.525+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:36.527+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:36.528+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:36.528+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:36.528+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:36.530+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:36.531+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:36.531+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:36.531+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:36.531+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:36.531+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:46.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:46.504Z",
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:46.519+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "triggerExecution" : 14
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:46.520+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:46.521+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:46.521+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:46.521+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:46.521+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:46.521+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:46.521+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:46.522+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:46.522+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:46.522+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:46.522+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:46.522+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:46.522+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:46.523+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:46.523+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:46.523+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:46.524+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:46.525+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:46.525+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:49.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:49 INFO BlockManagerInfo: Removed broadcast_65_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:46:49.871+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:49 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:46:56.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:46:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:46:56.532+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:46:56.532+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:46:56.520Z",
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:46:56.533+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "triggerExecution" : 9
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:56.534+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:46:56.535+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:46:56.536+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:06 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:47:06.534Z",
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:47:06.545+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:06.546+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:06.547+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:47:06.548+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:47:06.550+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:47:06.550+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:06.550+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:16.551+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:47:16.551+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:47:16.546Z",
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:47:16.552+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:16.553+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:47:16.554+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:16.555+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:26.556+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:47:26.557+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:47:26.547Z",
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:26.558+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:26.559+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:47:26.561+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:47:26.562+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5
[2024-11-12T09:47:26.562+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:26.562+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:47:26.563+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:47:26.563+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:47:26.563+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:47:26.564+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:26.564+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:26.565+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:26.565+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:26.566+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:26.567+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:26.567+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:26.568+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:26.568+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:26.568+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:26.568+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:47:26.569+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:47:26.569+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:47:26.569+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:26.570+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:30.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:30 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2024-11-12T09:47:36.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:47:36.561+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:47:36.562+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:47:36.562+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:47:36.563+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:47:36.545Z",
[2024-11-12T09:47:36.563+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:47:36.563+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:36.564+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:36.565+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:36.566+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:47:36.566+0000] {spark_submit.py:495} INFO - "latestOffset" : 14,
[2024-11-12T09:47:36.566+0000] {spark_submit.py:495} INFO - "triggerExecution" : 14
[2024-11-12T09:47:36.566+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:47:36.567+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:36.568+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:36.568+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:36.569+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:36.569+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:47:36.569+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:36.570+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:36.570+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:36.573+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:36.577+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:36.579+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:36.584+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:36.585+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:47:36.585+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:47:36.585+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:47:36.585+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:36.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:46.573+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:47:46.574+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:47:46.574+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:47:46.574+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:47:46.574+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:47:46.566Z",
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:47:46.575+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:46.576+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:46.577+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:46.578+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:47:46.579+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:47:46.579+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:47:46.579+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:47:46.579+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:46.579+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:56.581+0000] {spark_submit.py:495} INFO - 24/11/12 09:47:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:47:56.581+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:47:56.582+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:47:56.582+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:47:56.583+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:47:56.576Z",
[2024-11-12T09:47:56.583+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:47:56.583+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:56.584+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:56.584+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:56.584+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:47:56.585+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:47:56.586+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:56.586+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:56.587+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:47:56.588+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:47:56.589+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:47:56.589+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:47:56.589+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:47:56.589+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:47:56.594+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:56.596+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:47:56.596+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:47:56.596+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:47:56.596+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:47:56.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:47:56.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:06.591+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:06 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:06.592+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:06.592+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:06.592+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:06.592+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:06.583Z",
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:06.593+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:06.594+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:48:06.595+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:06.596+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:06.596+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:06.596+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:06.597+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:06.597+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:06.597+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:06.597+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:06.597+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:06.598+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:06.598+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:16.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:16.578Z",
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:16.593+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "triggerExecution" : 12
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:16.594+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:16.595+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:16.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:16.596+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:16.596+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:16.596+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:16.596+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:16.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:16.597+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:26.597+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:26.597+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:26.598+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:26.598+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:26.599+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:26.586Z",
[2024-11-12T09:48:26.599+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:48:26.599+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:48:26.600+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:48:26.600+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:48:26.600+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:26.600+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:48:26.600+0000] {spark_submit.py:495} INFO - "triggerExecution" : 10
[2024-11-12T09:48:26.601+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:26.601+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:26.602+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:26.602+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:26.602+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:26.603+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:26.604+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:26.605+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:28.708+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/66 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.66.b6c184c1-5e97-4b15-b0eb-66ae6d5d46e8.tmp
[2024-11-12T09:48:29.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.66.b6c184c1-5e97-4b15-b0eb-66ae6d5d46e8.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/66
[2024-11-12T09:48:29.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO MicroBatchExecution: Committed offsets for batch 66. Metadata OffsetSeqMetadata(0,1731404908688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:29.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:29.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:29.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:29.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:29.229+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 65, 65
[2024-11-12T09:48:29.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:29.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:29.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Got job 66 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:29.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:29.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:29.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:29.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[268] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:29.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:48:29.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:48:29.296+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:29.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:29.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[268] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:29.298+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0
[2024-11-12T09:48:29.299+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 66) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:29.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:29.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 66) in 488 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:29.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool
[2024-11-12T09:48:29.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s
[2024-11-12T09:48:29.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:29.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
[2024-11-12T09:48:29.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO DAGScheduler: Job 66 finished: start at NativeMethodAccessorImpl.java:0, took 0.526306 s
[2024-11-12T09:48:29.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO FileFormatWriter: Start to commit write Job 8a64e33f-3796-491d-b10f-76b3624ced69.
[2024-11-12T09:48:29.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/66 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.66.2e3a4950-169b-41da-b3c4-87eaa5c9f79f.tmp
[2024-11-12T09:48:29.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.66.2e3a4950-169b-41da-b3c4-87eaa5c9f79f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/66
[2024-11-12T09:48:29.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO ManifestFileCommitProtocol: Committed batch 66
[2024-11-12T09:48:29.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO FileFormatWriter: Write Job 8a64e33f-3796-491d-b10f-76b3624ced69 committed. Elapsed time: 70 ms.
[2024-11-12T09:48:29.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO FileFormatWriter: Finished processing stats for write job 8a64e33f-3796-491d-b10f-76b3624ced69.
[2024-11-12T09:48:29.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/66 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.66.44e24c72-909c-4a44-a1a7-779d17686bca.tmp
[2024-11-12T09:48:29.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.66.44e24c72-909c-4a44-a1a7-779d17686bca.tmp to hdfs://namenode:9000/spark_checkpoint/commits/66
[2024-11-12T09:48:29.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:29.929+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:29.929+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:29.930+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:29.932+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:28.684Z",
[2024-11-12T09:48:29.933+0000] {spark_submit.py:495} INFO - "batchId" : 66,
[2024-11-12T09:48:29.933+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:29.933+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 52.631578947368425,
[2024-11-12T09:48:29.934+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8090614886731392,
[2024-11-12T09:48:29.934+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:29.934+0000] {spark_submit.py:495} INFO - "addBatch" : 658,
[2024-11-12T09:48:29.934+0000] {spark_submit.py:495} INFO - "commitOffsets" : 58,
[2024-11-12T09:48:29.935+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:29.935+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:48:29.935+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:48:29.935+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1236,
[2024-11-12T09:48:29.936+0000] {spark_submit.py:495} INFO - "walCommit" : 482
[2024-11-12T09:48:29.936+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - "0" : 652
[2024-11-12T09:48:29.937+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:29.938+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:29.938+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:29.938+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - "0" : 653
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - "0" : 653
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:29.939+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 52.631578947368425,
[2024-11-12T09:48:29.940+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8090614886731392,
[2024-11-12T09:48:29.940+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:29.941+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:29.941+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:29.942+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:29.946+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:29.947+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:29.947+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:29.948+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:29.948+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:29.955+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:29.962+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:29.963+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/67 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.67.8136b875-c72c-4660-b466-70f062c30848.tmp
[2024-11-12T09:48:30.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.67.8136b875-c72c-4660-b466-70f062c30848.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/67
[2024-11-12T09:48:30.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO MicroBatchExecution: Committed offsets for batch 67. Metadata OffsetSeqMetadata(0,1731404909936,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:30.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:30.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:30.058+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:30.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:30.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 65, 66, 66
[2024-11-12T09:48:30.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:30.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:30.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Got job 67 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:30.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Final stage: ResultStage 67 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:30.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:30.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:30.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[272] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:30.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:30.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:30.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:30.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:30.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[272] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:30.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
[2024-11-12T09:48:30.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 67) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:30.151+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:30.778+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 67) in 643 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:30.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2024-11-12T09:48:30.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: ResultStage 67 (start at NativeMethodAccessorImpl.java:0) finished in 0.678 s
[2024-11-12T09:48:30.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:30.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
[2024-11-12T09:48:30.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO DAGScheduler: Job 67 finished: start at NativeMethodAccessorImpl.java:0, took 0.681253 s
[2024-11-12T09:48:30.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO FileFormatWriter: Start to commit write Job aec6d6e8-7e69-4844-a725-fc7b6ba0471c.
[2024-11-12T09:48:30.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/67 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.67.b0863124-72c5-42fe-aa46-469c8fdafcac.tmp
[2024-11-12T09:48:30.851+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.67.b0863124-72c5-42fe-aa46-469c8fdafcac.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/67
[2024-11-12T09:48:30.852+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO ManifestFileCommitProtocol: Committed batch 67
[2024-11-12T09:48:30.852+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO FileFormatWriter: Write Job aec6d6e8-7e69-4844-a725-fc7b6ba0471c committed. Elapsed time: 69 ms.
[2024-11-12T09:48:30.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO FileFormatWriter: Finished processing stats for write job aec6d6e8-7e69-4844-a725-fc7b6ba0471c.
[2024-11-12T09:48:30.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/67 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.67.a63b2782-f637-4aab-babb-8ac126a23264.tmp
[2024-11-12T09:48:30.912+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.67.a63b2782-f637-4aab-babb-8ac126a23264.tmp to hdfs://namenode:9000/spark_checkpoint/commits/67
[2024-11-12T09:48:30.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:30.915+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:30.915+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:30.915+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:29.928Z",
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "batchId" : 67,
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8038585209003215,
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.016260162601626,
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "addBatch" : 812,
[2024-11-12T09:48:30.916+0000] {spark_submit.py:495} INFO - "commitOffsets" : 62,
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "queryPlanning" : 19,
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "triggerExecution" : 984,
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "walCommit" : 75
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:30.917+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:30.918+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:30.918+0000] {spark_submit.py:495} INFO - "0" : 653
[2024-11-12T09:48:30.918+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:30.918+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:30.918+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:30.920+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:30.921+0000] {spark_submit.py:495} INFO - "0" : 654
[2024-11-12T09:48:30.921+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:30.921+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:30.921+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:30.921+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "0" : 654
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8038585209003215,
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.016260162601626,
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:30.922+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:30.924+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:30.924+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:30.924+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:30.924+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:30.925+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:30.925+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:30.925+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:30.925+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:30.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/68 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.68.05e0fd29-f881-460b-b34d-1881cbc6042b.tmp
[2024-11-12T09:48:30.976+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.68.05e0fd29-f881-460b-b34d-1881cbc6042b.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/68
[2024-11-12T09:48:30.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO MicroBatchExecution: Committed offsets for batch 68. Metadata OffsetSeqMetadata(0,1731404910922,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:30.995+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:30.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 66, 67, 67
[2024-11-12T09:48:31.013+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:31.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:31.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Got job 68 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:31.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:31.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:31.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:31.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[276] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:31.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:31.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:48:31.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO BlockManagerInfo: Removed broadcast_67_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:31.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:31.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:31.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:31.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[276] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:31.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0
[2024-11-12T09:48:31.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 68) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:31.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:31.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:48:31.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:31.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 68) in 644 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:31.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool
[2024-11-12T09:48:31.733+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.682 s
[2024-11-12T09:48:31.733+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:31.733+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
[2024-11-12T09:48:31.735+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO DAGScheduler: Job 68 finished: start at NativeMethodAccessorImpl.java:0, took 0.693984 s
[2024-11-12T09:48:31.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO FileFormatWriter: Start to commit write Job 7dfce9c6-fc4d-4a3d-be49-c7c885ba7c21.
[2024-11-12T09:48:31.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/68 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.68.ee6985d3-ab55-41bc-9954-57cdd73d8f5c.tmp
[2024-11-12T09:48:31.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.68.ee6985d3-ab55-41bc-9954-57cdd73d8f5c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/68
[2024-11-12T09:48:31.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO ManifestFileCommitProtocol: Committed batch 68
[2024-11-12T09:48:31.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO FileFormatWriter: Write Job 7dfce9c6-fc4d-4a3d-be49-c7c885ba7c21 committed. Elapsed time: 65 ms.
[2024-11-12T09:48:31.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO FileFormatWriter: Finished processing stats for write job 7dfce9c6-fc4d-4a3d-be49-c7c885ba7c21.
[2024-11-12T09:48:31.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/68 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.68.421c7851-2cee-46b7-96c8-ef18843e44ab.tmp
[2024-11-12T09:48:31.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.68.421c7851-2cee-46b7-96c8-ef18843e44ab.tmp to hdfs://namenode:9000/spark_checkpoint/commits/68
[2024-11-12T09:48:31.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:31.863+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:30.914Z",
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "batchId" : 68,
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0141987829614605,
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0559662090813096,
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:31.864+0000] {spark_submit.py:495} INFO - "addBatch" : 803,
[2024-11-12T09:48:31.873+0000] {spark_submit.py:495} INFO - "commitOffsets" : 59,
[2024-11-12T09:48:31.874+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:48:31.874+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:48:31.874+0000] {spark_submit.py:495} INFO - "queryPlanning" : 21,
[2024-11-12T09:48:31.875+0000] {spark_submit.py:495} INFO - "triggerExecution" : 947,
[2024-11-12T09:48:31.875+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:48:31.875+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:31.875+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:31.875+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:31.876+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:31.877+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:31.877+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:31.877+0000] {spark_submit.py:495} INFO - "0" : 654
[2024-11-12T09:48:31.877+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:31.877+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:31.878+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:31.878+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:31.878+0000] {spark_submit.py:495} INFO - "0" : 655
[2024-11-12T09:48:31.878+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:31.878+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:31.878+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - "0" : 655
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0141987829614605,
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0559662090813096,
[2024-11-12T09:48:31.879+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:31.880+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:31.880+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:31.881+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:31.881+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:31.881+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:31.882+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:31.882+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:31.883+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:31.883+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:31.883+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:31.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/69 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.69.330ea277-4ec5-49e9-b48f-3570337381ef.tmp
[2024-11-12T09:48:31.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.69.330ea277-4ec5-49e9-b48f-3570337381ef.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/69
[2024-11-12T09:48:31.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO MicroBatchExecution: Committed offsets for batch 69. Metadata OffsetSeqMetadata(0,1731404911877,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:31.950+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.953+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.978+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:31.987+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 67, 68, 68
[2024-11-12T09:48:31.989+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:32.029+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:32.031+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Got job 69 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:32.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:32.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:32.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:32.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:32.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:32.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:32.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:32.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:32.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:32.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0
[2024-11-12T09:48:32.081+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 69) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:32.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:32.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 69) in 707 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:32.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool
[2024-11-12T09:48:32.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.754 s
[2024-11-12T09:48:32.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:32.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
[2024-11-12T09:48:32.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO DAGScheduler: Job 69 finished: start at NativeMethodAccessorImpl.java:0, took 0.759054 s
[2024-11-12T09:48:32.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO FileFormatWriter: Start to commit write Job 60b3e047-7ffc-422b-a51e-aed6445c6945.
[2024-11-12T09:48:32.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:32 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/69.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.69.compact.a952b4ad-815f-4e4b-b3a8-69c303f30e28.tmp
[2024-11-12T09:48:33.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.69.compact.a952b4ad-815f-4e4b-b3a8-69c303f30e28.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/69.compact
[2024-11-12T09:48:33.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO ManifestFileCommitProtocol: Committed batch 69
[2024-11-12T09:48:33.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO FileFormatWriter: Write Job 60b3e047-7ffc-422b-a51e-aed6445c6945 committed. Elapsed time: 258 ms.
[2024-11-12T09:48:33.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO FileFormatWriter: Finished processing stats for write job 60b3e047-7ffc-422b-a51e-aed6445c6945.
[2024-11-12T09:48:33.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/69 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.69.7713b1d1-b631-4fbf-b270-3ce8a501096b.tmp
[2024-11-12T09:48:33.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.69.7713b1d1-b631-4fbf-b270-3ce8a501096b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/69
[2024-11-12T09:48:33.109+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:33.110+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:33.110+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:33.110+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:33.111+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:31.862Z",
[2024-11-12T09:48:33.113+0000] {spark_submit.py:495} INFO - "batchId" : 69,
[2024-11-12T09:48:33.114+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:33.114+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0548523206751055,
[2024-11-12T09:48:33.114+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8025682182985554,
[2024-11-12T09:48:33.114+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:33.114+0000] {spark_submit.py:495} INFO - "addBatch" : 1088,
[2024-11-12T09:48:33.114+0000] {spark_submit.py:495} INFO - "commitOffsets" : 61,
[2024-11-12T09:48:33.115+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:48:33.115+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:48:33.115+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1246,
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "walCommit" : 49
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "0" : 655
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.116+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - "0" : 656
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - "0" : 656
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.117+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:33.118+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0548523206751055,
[2024-11-12T09:48:33.118+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8025682182985554,
[2024-11-12T09:48:33.118+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:33.118+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:33.118+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:33.118+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:33.119+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.119+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:33.119+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:33.119+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:33.119+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:33.119+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.120+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/70 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.70.ae5e1bc7-5afa-4ebb-b60b-8e60231438a8.tmp
[2024-11-12T09:48:33.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.70.ae5e1bc7-5afa-4ebb-b60b-8e60231438a8.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/70
[2024-11-12T09:48:33.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO MicroBatchExecution: Committed offsets for batch 70. Metadata OffsetSeqMetadata(0,1731404913121,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:33.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:33.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:33.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:33.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:33.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 67, 68, 68, 69
[2024-11-12T09:48:33.281+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:33.324+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:33.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Got job 70 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:33.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Final stage: ResultStage 70 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:33.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:33.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:33.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[284] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:33.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:33.382+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.5 MiB)
[2024-11-12T09:48:33.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO BlockManagerInfo: Removed broadcast_69_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:33.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:33.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:33.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[284] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:33.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:33.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0
[2024-11-12T09:48:33.395+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 70) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:33.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO BlockManagerInfo: Removed broadcast_68_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:33.424+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:48:33.440+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:33.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 70) in 410 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:33.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool
[2024-11-12T09:48:33.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: ResultStage 70 (start at NativeMethodAccessorImpl.java:0) finished in 0.473 s
[2024-11-12T09:48:33.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:33.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
[2024-11-12T09:48:33.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO DAGScheduler: Job 70 finished: start at NativeMethodAccessorImpl.java:0, took 0.481020 s
[2024-11-12T09:48:33.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO FileFormatWriter: Start to commit write Job 08a9b10b-57b5-4717-9415-bb7e1e945857.
[2024-11-12T09:48:33.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/70 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.70.9b247e67-852f-4217-b859-477d9b724adb.tmp
[2024-11-12T09:48:33.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.70.9b247e67-852f-4217-b859-477d9b724adb.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/70
[2024-11-12T09:48:33.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO ManifestFileCommitProtocol: Committed batch 70
[2024-11-12T09:48:33.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO FileFormatWriter: Write Job 08a9b10b-57b5-4717-9415-bb7e1e945857 committed. Elapsed time: 68 ms.
[2024-11-12T09:48:33.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO FileFormatWriter: Finished processing stats for write job 08a9b10b-57b5-4717-9415-bb7e1e945857.
[2024-11-12T09:48:33.903+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/70 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.70.78df2924-a900-4946-9be4-8d45819125d6.tmp
[2024-11-12T09:48:33.954+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.70.78df2924-a900-4946-9be4-8d45819125d6.tmp to hdfs://namenode:9000/spark_checkpoint/commits/70
[2024-11-12T09:48:33.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:33.960+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:33.961+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:33.961+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:33.109Z",
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "batchId" : 70,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8019246190858058,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.183431952662722,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "addBatch" : 632,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "commitOffsets" : 79,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:33.962+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "queryPlanning" : 50,
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "triggerExecution" : 845,
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "walCommit" : 68
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - "0" : 656
[2024-11-12T09:48:33.963+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - "0" : 657
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:33.964+0000] {spark_submit.py:495} INFO - "0" : 657
[2024-11-12T09:48:33.965+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.967+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8019246190858058,
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.183431952662722,
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:33.968+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:33.969+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:33.969+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.969+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:33.976+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/71 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.71.eadd3607-2fee-45f1-bb22-60305f2d7386.tmp
[2024-11-12T09:48:34.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.71.eadd3607-2fee-45f1-bb22-60305f2d7386.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/71
[2024-11-12T09:48:34.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO MicroBatchExecution: Committed offsets for batch 71. Metadata OffsetSeqMetadata(0,1731404913963,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:34.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:34.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:34.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:34.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:34.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 68, 69, 70, 70
[2024-11-12T09:48:34.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:34.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:34.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Got job 71 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:34.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:34.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:34.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:34.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[288] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:34.164+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:34.174+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:34.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:34.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:34.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[288] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:34.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0
[2024-11-12T09:48:34.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 71) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:34.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:34.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 71) in 631 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:34.817+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool
[2024-11-12T09:48:34.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.679 s
[2024-11-12T09:48:34.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:34.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
[2024-11-12T09:48:34.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO DAGScheduler: Job 71 finished: start at NativeMethodAccessorImpl.java:0, took 0.684694 s
[2024-11-12T09:48:34.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO FileFormatWriter: Start to commit write Job 17e8d9d9-0173-41b8-b086-fd535d5421a7.
[2024-11-12T09:48:34.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/71 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.71.917ea703-8b5e-4c5f-b400-3570517d9823.tmp
[2024-11-12T09:48:34.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.71.917ea703-8b5e-4c5f-b400-3570517d9823.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/71
[2024-11-12T09:48:34.878+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO ManifestFileCommitProtocol: Committed batch 71
[2024-11-12T09:48:34.879+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO FileFormatWriter: Write Job 17e8d9d9-0173-41b8-b086-fd535d5421a7 committed. Elapsed time: 60 ms.
[2024-11-12T09:48:34.880+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO FileFormatWriter: Finished processing stats for write job 17e8d9d9-0173-41b8-b086-fd535d5421a7.
[2024-11-12T09:48:34.886+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/71 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.71.b4dc7eec-8754-4ab7-a977-18feddd2344a.tmp
[2024-11-12T09:48:34.943+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.71.b4dc7eec-8754-4ab7-a977-18feddd2344a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/71
[2024-11-12T09:48:34.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:34.945+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:34.945+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:34.946+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:34.946+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:33.955Z",
[2024-11-12T09:48:34.947+0000] {spark_submit.py:495} INFO - "batchId" : 71,
[2024-11-12T09:48:34.947+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:34.947+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1820330969267139,
[2024-11-12T09:48:34.947+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0121457489878543,
[2024-11-12T09:48:34.947+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:34.947+0000] {spark_submit.py:495} INFO - "addBatch" : 827,
[2024-11-12T09:48:34.948+0000] {spark_submit.py:495} INFO - "commitOffsets" : 65,
[2024-11-12T09:48:34.949+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:34.949+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:48:34.950+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:48:34.950+0000] {spark_submit.py:495} INFO - "triggerExecution" : 988,
[2024-11-12T09:48:34.950+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:48:34.950+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:34.950+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:34.951+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:34.951+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:34.951+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:34.952+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:34.957+0000] {spark_submit.py:495} INFO - "0" : 657
[2024-11-12T09:48:34.958+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - "0" : 658
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:34.959+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "0" : 658
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1820330969267139,
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0121457489878543,
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:34.960+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:34.961+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:34.961+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:34.961+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:34.961+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:34.962+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:34.962+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:34.962+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:34.962+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:34.962+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:34.962+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/72 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.72.63d92615-3c9b-4688-be42-2c7798eea56e.tmp
[2024-11-12T09:48:35.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.72.63d92615-3c9b-4688-be42-2c7798eea56e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/72
[2024-11-12T09:48:35.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO MicroBatchExecution: Committed offsets for batch 72. Metadata OffsetSeqMetadata(0,1731404914949,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:35.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:35.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:35.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:35.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:35.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 70, 71, 71
[2024-11-12T09:48:35.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:35.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:35.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Got job 72 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:35.174+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:35.174+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:35.174+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:35.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[292] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:35.218+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:35.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:48:35.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:35.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:35.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[292] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:35.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0
[2024-11-12T09:48:35.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 72) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:35.281+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:35.830+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 72) in 592 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:35.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool
[2024-11-12T09:48:35.839+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.655 s
[2024-11-12T09:48:35.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:35.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished
[2024-11-12T09:48:35.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO DAGScheduler: Job 72 finished: start at NativeMethodAccessorImpl.java:0, took 0.662722 s
[2024-11-12T09:48:35.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO FileFormatWriter: Start to commit write Job a8f85682-af61-4648-890b-544b5f2d19f3.
[2024-11-12T09:48:35.849+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/72 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.72.bcbbdd3c-04a6-47a0-9c09-155b6ceff1e1.tmp
[2024-11-12T09:48:35.909+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.72.bcbbdd3c-04a6-47a0-9c09-155b6ceff1e1.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/72
[2024-11-12T09:48:35.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO ManifestFileCommitProtocol: Committed batch 72
[2024-11-12T09:48:35.922+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO FileFormatWriter: Write Job a8f85682-af61-4648-890b-544b5f2d19f3 committed. Elapsed time: 75 ms.
[2024-11-12T09:48:35.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO FileFormatWriter: Finished processing stats for write job a8f85682-af61-4648-890b-544b5f2d19f3.
[2024-11-12T09:48:35.941+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/72 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.72.4598fe8f-40ea-45ed-90e2-59f40cc33803.tmp
[2024-11-12T09:48:36.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.72.4598fe8f-40ea-45ed-90e2-59f40cc33803.tmp to hdfs://namenode:9000/spark_checkpoint/commits/72
[2024-11-12T09:48:36.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:36.008+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:36.009+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:36.009+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:36.010+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:34.944Z",
[2024-11-12T09:48:36.010+0000] {spark_submit.py:495} INFO - "batchId" : 72,
[2024-11-12T09:48:36.011+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:36.011+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0111223458038423,
[2024-11-12T09:48:36.011+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9451795841209829,
[2024-11-12T09:48:36.011+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:36.012+0000] {spark_submit.py:495} INFO - "addBatch" : 848,
[2024-11-12T09:48:36.012+0000] {spark_submit.py:495} INFO - "commitOffsets" : 82,
[2024-11-12T09:48:36.012+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:36.012+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:48:36.013+0000] {spark_submit.py:495} INFO - "queryPlanning" : 38,
[2024-11-12T09:48:36.014+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1058,
[2024-11-12T09:48:36.014+0000] {spark_submit.py:495} INFO - "walCommit" : 75
[2024-11-12T09:48:36.014+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:36.014+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:36.018+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:36.020+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:36.022+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:36.022+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:36.023+0000] {spark_submit.py:495} INFO - "0" : 658
[2024-11-12T09:48:36.023+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:36.023+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:36.023+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:36.024+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:36.024+0000] {spark_submit.py:495} INFO - "0" : 659
[2024-11-12T09:48:36.024+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:36.024+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:36.025+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:36.025+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:36.025+0000] {spark_submit.py:495} INFO - "0" : 659
[2024-11-12T09:48:36.026+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:36.026+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:36.027+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:36.027+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0111223458038423,
[2024-11-12T09:48:36.027+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9451795841209829,
[2024-11-12T09:48:36.027+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:36.027+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:36.028+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:36.028+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:36.028+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:36.028+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:36.029+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:36.029+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:36.029+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:36.029+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:36.029+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:36.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/73 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.73.bbc35b5e-17bf-4b23-90b6-f4bfb61583bf.tmp
[2024-11-12T09:48:36.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.73.bbc35b5e-17bf-4b23-90b6-f4bfb61583bf.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/73
[2024-11-12T09:48:36.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO MicroBatchExecution: Committed offsets for batch 73. Metadata OffsetSeqMetadata(0,1731404916019,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:36.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:36.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:36.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:36.153+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:36.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 71, 72, 72
[2024-11-12T09:48:36.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:36.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:36.231+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Got job 73 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:36.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Final stage: ResultStage 73 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:36.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:36.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:36.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:36.276+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:48:36.282+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:48:36.284+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:36.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:36.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[296] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:36.290+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0
[2024-11-12T09:48:36.296+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 73) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:36.341+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:36.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 73) in 568 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:36.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool
[2024-11-12T09:48:36.863+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: ResultStage 73 (start at NativeMethodAccessorImpl.java:0) finished in 0.626 s
[2024-11-12T09:48:36.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:36.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished
[2024-11-12T09:48:36.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO DAGScheduler: Job 73 finished: start at NativeMethodAccessorImpl.java:0, took 0.639837 s
[2024-11-12T09:48:36.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO FileFormatWriter: Start to commit write Job e7586fed-c73c-4456-b1ba-e858ee32998b.
[2024-11-12T09:48:36.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/73 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.73.4805e1c0-daa7-485b-8868-3906966e50c6.tmp
[2024-11-12T09:48:36.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.73.4805e1c0-daa7-485b-8868-3906966e50c6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/73
[2024-11-12T09:48:36.939+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO ManifestFileCommitProtocol: Committed batch 73
[2024-11-12T09:48:36.940+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO FileFormatWriter: Write Job e7586fed-c73c-4456-b1ba-e858ee32998b committed. Elapsed time: 73 ms.
[2024-11-12T09:48:36.940+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO FileFormatWriter: Finished processing stats for write job e7586fed-c73c-4456-b1ba-e858ee32998b.
[2024-11-12T09:48:36.956+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/73 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.73.1721dd78-b24c-436c-a47a-8fb362abd816.tmp
[2024-11-12T09:48:37.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.73.1721dd78-b24c-436c-a47a-8fb362abd816.tmp to hdfs://namenode:9000/spark_checkpoint/commits/73
[2024-11-12T09:48:37.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:37.009+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:37.009+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:37.009+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:37.011+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:36.004Z",
[2024-11-12T09:48:37.012+0000] {spark_submit.py:495} INFO - "batchId" : 73,
[2024-11-12T09:48:37.012+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:37.013+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9433962264150942,
[2024-11-12T09:48:37.013+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9970089730807579,
[2024-11-12T09:48:37.013+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:37.014+0000] {spark_submit.py:495} INFO - "addBatch" : 803,
[2024-11-12T09:48:37.014+0000] {spark_submit.py:495} INFO - "commitOffsets" : 67,
[2024-11-12T09:48:37.014+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:37.015+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:48:37.015+0000] {spark_submit.py:495} INFO - "queryPlanning" : 33,
[2024-11-12T09:48:37.015+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1003,
[2024-11-12T09:48:37.015+0000] {spark_submit.py:495} INFO - "walCommit" : 81
[2024-11-12T09:48:37.016+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:37.016+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:37.016+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:37.017+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:37.017+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:37.017+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:37.017+0000] {spark_submit.py:495} INFO - "0" : 659
[2024-11-12T09:48:37.018+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:37.018+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:37.018+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:37.018+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:37.018+0000] {spark_submit.py:495} INFO - "0" : 660
[2024-11-12T09:48:37.020+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:37.025+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:37.026+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:37.026+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:37.026+0000] {spark_submit.py:495} INFO - "0" : 660
[2024-11-12T09:48:37.027+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:37.027+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:37.027+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:37.028+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9433962264150942,
[2024-11-12T09:48:37.028+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9970089730807579,
[2024-11-12T09:48:37.030+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:37.030+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:37.030+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:37.031+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:37.031+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:37.031+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:37.031+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:37.032+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:37.032+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:37.032+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:37.032+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:37.039+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/74 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.74.c92c2919-4df7-42c2-ab27-d5b0266341c3.tmp
[2024-11-12T09:48:37.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.74.c92c2919-4df7-42c2-ab27-d5b0266341c3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/74
[2024-11-12T09:48:37.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO MicroBatchExecution: Committed offsets for batch 74. Metadata OffsetSeqMetadata(0,1731404917025,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:37.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:37.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:37.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:37.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:37.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 72, 73, 73
[2024-11-12T09:48:37.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:37.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:37.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Got job 74 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:37.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Final stage: ResultStage 74 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:37.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:37.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:37.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[300] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:37.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:48:37.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_72_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:37.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.4 MiB)
[2024-11-12T09:48:37.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:37.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:37.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[300] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:37.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0
[2024-11-12T09:48:37.351+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:37.351+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 74) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:37.356+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_71_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:37.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:37.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_73_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:37.429+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:37.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:37.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_70_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:37.463+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:37.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 74) in 541 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:37.886+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool
[2024-11-12T09:48:37.886+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: ResultStage 74 (start at NativeMethodAccessorImpl.java:0) finished in 0.620 s
[2024-11-12T09:48:37.886+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:37.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
[2024-11-12T09:48:37.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO DAGScheduler: Job 74 finished: start at NativeMethodAccessorImpl.java:0, took 0.636070 s
[2024-11-12T09:48:37.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO FileFormatWriter: Start to commit write Job ae9403b2-d7fc-4c12-a713-b42dcea71a4f.
[2024-11-12T09:48:37.909+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/74 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.74.cf6af129-b207-4afb-b4a1-a6a45f516259.tmp
[2024-11-12T09:48:37.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.74.cf6af129-b207-4afb-b4a1-a6a45f516259.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/74
[2024-11-12T09:48:37.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO ManifestFileCommitProtocol: Committed batch 74
[2024-11-12T09:48:37.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO FileFormatWriter: Write Job ae9403b2-d7fc-4c12-a713-b42dcea71a4f committed. Elapsed time: 96 ms.
[2024-11-12T09:48:37.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO FileFormatWriter: Finished processing stats for write job ae9403b2-d7fc-4c12-a713-b42dcea71a4f.
[2024-11-12T09:48:37.996+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/74 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.74.14c52c24-2f4b-4ede-bad9-ee0a533ab56d.tmp
[2024-11-12T09:48:38.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.74.14c52c24-2f4b-4ede-bad9-ee0a533ab56d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/74
[2024-11-12T09:48:38.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:38.068+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:38.070+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:38.070+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:38.070+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:37.008Z",
[2024-11-12T09:48:38.071+0000] {spark_submit.py:495} INFO - "batchId" : 74,
[2024-11-12T09:48:38.071+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:38.072+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9960159362549801,
[2024-11-12T09:48:38.073+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9451795841209829,
[2024-11-12T09:48:38.073+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:38.073+0000] {spark_submit.py:495} INFO - "addBatch" : 820,
[2024-11-12T09:48:38.074+0000] {spark_submit.py:495} INFO - "commitOffsets" : 83,
[2024-11-12T09:48:38.074+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:38.074+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:48:38.075+0000] {spark_submit.py:495} INFO - "queryPlanning" : 66,
[2024-11-12T09:48:38.075+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1058,
[2024-11-12T09:48:38.076+0000] {spark_submit.py:495} INFO - "walCommit" : 69
[2024-11-12T09:48:38.076+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:38.077+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:38.077+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:38.077+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:38.078+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:38.078+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:38.083+0000] {spark_submit.py:495} INFO - "0" : 660
[2024-11-12T09:48:38.083+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:38.084+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:38.084+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:38.084+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:38.084+0000] {spark_submit.py:495} INFO - "0" : 661
[2024-11-12T09:48:38.085+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:38.085+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:38.085+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:38.088+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "0" : 661
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9960159362549801,
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9451795841209829,
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:38.089+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:38.090+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:38.090+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:38.090+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:38.090+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:38.090+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:38.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/75 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.75.8a2570c5-1fd6-44bc-ae31-fa69d473d67b.tmp
[2024-11-12T09:48:38.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.75.8a2570c5-1fd6-44bc-ae31-fa69d473d67b.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/75
[2024-11-12T09:48:38.151+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO MicroBatchExecution: Committed offsets for batch 75. Metadata OffsetSeqMetadata(0,1731404918074,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:38.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:38.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:38.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:38.221+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:38.233+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 73, 74, 74
[2024-11-12T09:48:38.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:38.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:38.301+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Got job 75 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:38.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Final stage: ResultStage 75 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:38.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:38.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:38.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[304] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:38.342+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:38.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:38.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:38.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:38.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[304] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:38.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0
[2024-11-12T09:48:38.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 75) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:38.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:38.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 75) in 521 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:38.887+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool
[2024-11-12T09:48:38.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: ResultStage 75 (start at NativeMethodAccessorImpl.java:0) finished in 0.580 s
[2024-11-12T09:48:38.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:38.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
[2024-11-12T09:48:38.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO DAGScheduler: Job 75 finished: start at NativeMethodAccessorImpl.java:0, took 0.592492 s
[2024-11-12T09:48:38.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO FileFormatWriter: Start to commit write Job 763f9a9e-2b52-47bd-b990-c438a89a33c8.
[2024-11-12T09:48:38.911+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/75 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.75.a0b2c93d-ffc2-473d-b6df-13e9527cecc4.tmp
[2024-11-12T09:48:38.980+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.75.a0b2c93d-ffc2-473d-b6df-13e9527cecc4.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/75
[2024-11-12T09:48:38.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO ManifestFileCommitProtocol: Committed batch 75
[2024-11-12T09:48:38.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO FileFormatWriter: Write Job 763f9a9e-2b52-47bd-b990-c438a89a33c8 committed. Elapsed time: 91 ms.
[2024-11-12T09:48:38.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:38 INFO FileFormatWriter: Finished processing stats for write job 763f9a9e-2b52-47bd-b990-c438a89a33c8.
[2024-11-12T09:48:39.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/75 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.75.6f9fb5e3-ed00-45b1-ad08-624b9b8e57bc.tmp
[2024-11-12T09:48:39.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.75.6f9fb5e3-ed00-45b1-ad08-624b9b8e57bc.tmp to hdfs://namenode:9000/spark_checkpoint/commits/75
[2024-11-12T09:48:39.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:39.474+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:39.475+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:39.476+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:39.476+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:38.068Z",
[2024-11-12T09:48:39.476+0000] {spark_submit.py:495} INFO - "batchId" : 75,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9433962264150942,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7142857142857143,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "addBatch" : 789,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "commitOffsets" : 484,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:48:39.477+0000] {spark_submit.py:495} INFO - "queryPlanning" : 39,
[2024-11-12T09:48:39.479+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1400,
[2024-11-12T09:48:39.479+0000] {spark_submit.py:495} INFO - "walCommit" : 77
[2024-11-12T09:48:39.479+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:39.480+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:39.480+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:39.480+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:39.480+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:39.481+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:39.481+0000] {spark_submit.py:495} INFO - "0" : 661
[2024-11-12T09:48:39.481+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:39.482+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:39.482+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:39.482+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:39.482+0000] {spark_submit.py:495} INFO - "0" : 662
[2024-11-12T09:48:39.483+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:39.483+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:39.483+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:39.483+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:39.483+0000] {spark_submit.py:495} INFO - "0" : 662
[2024-11-12T09:48:39.484+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:39.484+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:39.484+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:39.484+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9433962264150942,
[2024-11-12T09:48:39.484+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7142857142857143,
[2024-11-12T09:48:39.484+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:39.485+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:39.485+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:39.485+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:39.486+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:39.486+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:39.487+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:39.487+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:39.487+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:39.487+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:39.487+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:39.495+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/76 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.76.b33d810d-d4d7-4c31-815e-74b2e6242bcc.tmp
[2024-11-12T09:48:39.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.76.b33d810d-d4d7-4c31-815e-74b2e6242bcc.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/76
[2024-11-12T09:48:39.548+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO MicroBatchExecution: Committed offsets for batch 76. Metadata OffsetSeqMetadata(0,1731404919482,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:39.576+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:39.579+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:39.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:39.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:39.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 74, 75, 75
[2024-11-12T09:48:39.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:39.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:39.674+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Got job 76 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:39.676+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Final stage: ResultStage 76 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:39.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:39.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:39.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[308] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:39.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:39.705+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:48:39.707+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:39.708+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:39.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[308] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:39.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks resource profile 0
[2024-11-12T09:48:39.710+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 76) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:39.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:39.932+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 76) in 223 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:39.935+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool
[2024-11-12T09:48:39.941+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: ResultStage 76 (start at NativeMethodAccessorImpl.java:0) finished in 0.258 s
[2024-11-12T09:48:39.946+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:39.946+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished
[2024-11-12T09:48:39.948+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO DAGScheduler: Job 76 finished: start at NativeMethodAccessorImpl.java:0, took 0.274438 s
[2024-11-12T09:48:39.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO FileFormatWriter: Start to commit write Job af88c649-8a63-43b1-b06b-f5953c0b0d42.
[2024-11-12T09:48:39.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/76 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.76.918e9a85-e7e0-437c-9493-01ed0e24d752.tmp
[2024-11-12T09:48:40.039+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.76.918e9a85-e7e0-437c-9493-01ed0e24d752.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/76
[2024-11-12T09:48:40.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO ManifestFileCommitProtocol: Committed batch 76
[2024-11-12T09:48:40.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO FileFormatWriter: Write Job af88c649-8a63-43b1-b06b-f5953c0b0d42 committed. Elapsed time: 93 ms.
[2024-11-12T09:48:40.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO FileFormatWriter: Finished processing stats for write job af88c649-8a63-43b1-b06b-f5953c0b0d42.
[2024-11-12T09:48:40.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/76 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.76.1c72c319-074a-4d76-8ca1-19c05b5b5c10.tmp
[2024-11-12T09:48:40.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.76.1c72c319-074a-4d76-8ca1-19c05b5b5c10.tmp to hdfs://namenode:9000/spark_checkpoint/commits/76
[2024-11-12T09:48:40.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:40.142+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:40.143+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:40.143+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:40.143+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:39.472Z",
[2024-11-12T09:48:40.143+0000] {spark_submit.py:495} INFO - "batchId" : 76,
[2024-11-12T09:48:40.143+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:40.143+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7122507122507123,
[2024-11-12T09:48:40.144+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4992503748125936,
[2024-11-12T09:48:40.144+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:40.144+0000] {spark_submit.py:495} INFO - "addBatch" : 452,
[2024-11-12T09:48:40.144+0000] {spark_submit.py:495} INFO - "commitOffsets" : 99,
[2024-11-12T09:48:40.144+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:40.144+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:48:40.146+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:48:40.146+0000] {spark_submit.py:495} INFO - "triggerExecution" : 667,
[2024-11-12T09:48:40.146+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - "0" : 662
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.147+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - "0" : 663
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - "0" : 663
[2024-11-12T09:48:40.148+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.149+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.149+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:40.151+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7122507122507123,
[2024-11-12T09:48:40.151+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4992503748125936,
[2024-11-12T09:48:40.153+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:40.156+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:40.159+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:40.159+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:40.164+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.165+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:40.166+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:40.166+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:40.167+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:40.167+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.177+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.179+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/77 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.77.5320a26c-d3c3-41e5-982e-b9e3dd24cb5f.tmp
[2024-11-12T09:48:40.228+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.77.5320a26c-d3c3-41e5-982e-b9e3dd24cb5f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/77
[2024-11-12T09:48:40.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO MicroBatchExecution: Committed offsets for batch 77. Metadata OffsetSeqMetadata(0,1731404920152,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:40.260+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:40.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:40.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:40.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:40.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76
[2024-11-12T09:48:40.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:40.356+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:40.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Got job 77 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:40.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Final stage: ResultStage 77 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:40.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:40.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:40.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[312] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:40.379+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:48:40.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:48:40.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:40.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:40.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[312] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:40.389+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0
[2024-11-12T09:48:40.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 77) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:40.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:40.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 77) in 469 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:40.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool
[2024-11-12T09:48:40.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: ResultStage 77 (start at NativeMethodAccessorImpl.java:0) finished in 0.502 s
[2024-11-12T09:48:40.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:40.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished
[2024-11-12T09:48:40.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO DAGScheduler: Job 77 finished: start at NativeMethodAccessorImpl.java:0, took 0.504648 s
[2024-11-12T09:48:40.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO FileFormatWriter: Start to commit write Job 0d9d8ead-9029-4006-b5c5-e3e31ae764a4.
[2024-11-12T09:48:40.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/77 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.77.6d9aff67-08aa-4bfa-b6f3-20d6ca034abd.tmp
[2024-11-12T09:48:40.909+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.77.6d9aff67-08aa-4bfa-b6f3-20d6ca034abd.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/77
[2024-11-12T09:48:40.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO ManifestFileCommitProtocol: Committed batch 77
[2024-11-12T09:48:40.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO FileFormatWriter: Write Job 0d9d8ead-9029-4006-b5c5-e3e31ae764a4 committed. Elapsed time: 48 ms.
[2024-11-12T09:48:40.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO FileFormatWriter: Finished processing stats for write job 0d9d8ead-9029-4006-b5c5-e3e31ae764a4.
[2024-11-12T09:48:40.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/77 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.77.b0a05670-fda0-4823-8044-0a893f4dc7ed.tmp
[2024-11-12T09:48:40.951+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.77.b0a05670-fda0-4823-8044-0a893f4dc7ed.tmp to hdfs://namenode:9000/spark_checkpoint/commits/77
[2024-11-12T09:48:40.954+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:40.955+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:40.956+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:40.956+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:40.956+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:40.141Z",
[2024-11-12T09:48:40.956+0000] {spark_submit.py:495} INFO - "batchId" : 77,
[2024-11-12T09:48:40.957+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:40.957+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.4947683109118086,
[2024-11-12T09:48:40.957+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2345679012345678,
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "addBatch" : 641,
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "commitOffsets" : 41,
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:48:40.958+0000] {spark_submit.py:495} INFO - "triggerExecution" : 810,
[2024-11-12T09:48:40.959+0000] {spark_submit.py:495} INFO - "walCommit" : 77
[2024-11-12T09:48:40.959+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.959+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:40.960+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:40.960+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:40.960+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:40.961+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:40.961+0000] {spark_submit.py:495} INFO - "0" : 663
[2024-11-12T09:48:40.961+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.961+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.961+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:40.961+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "0" : 664
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "0" : 664
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.4947683109118086,
[2024-11-12T09:48:40.962+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2345679012345678,
[2024-11-12T09:48:40.963+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:40.963+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:40.963+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:40.963+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:40.964+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.964+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:40.964+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:40.965+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:40.965+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:40.965+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.965+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:40.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/78 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.78.8ad99484-2a70-455f-9587-6d4860955ee2.tmp
[2024-11-12T09:48:41.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.78.8ad99484-2a70-455f-9587-6d4860955ee2.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/78
[2024-11-12T09:48:41.027+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO MicroBatchExecution: Committed offsets for batch 78. Metadata OffsetSeqMetadata(0,1731404920964,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:41.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:41.059+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:41.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:41.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:41.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 76, 77, 77
[2024-11-12T09:48:41.109+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:41.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:41.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Got job 78 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:41.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Final stage: ResultStage 78 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:41.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:41.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:41.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[316] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:41.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:48:41.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:48:41.198+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:41.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_75_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:41.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:41.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[316] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:41.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks resource profile 0
[2024-11-12T09:48:41.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:41.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 78) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:41.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_76_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:41.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:41.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:41.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_74_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:41.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:41.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_77_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:41.282+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:41.855+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 78) in 651 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool
[2024-11-12T09:48:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: ResultStage 78 (start at NativeMethodAccessorImpl.java:0) finished in 0.710 s
[2024-11-12T09:48:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished
[2024-11-12T09:48:41.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO DAGScheduler: Job 78 finished: start at NativeMethodAccessorImpl.java:0, took 0.715820 s
[2024-11-12T09:48:41.857+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO FileFormatWriter: Start to commit write Job a30e180e-1195-4159-945e-0b7453969b18.
[2024-11-12T09:48:41.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/78 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.78.9e8b74fe-e50a-411b-a338-e493ca3c6cd8.tmp
[2024-11-12T09:48:41.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.78.9e8b74fe-e50a-411b-a338-e493ca3c6cd8.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/78
[2024-11-12T09:48:41.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO ManifestFileCommitProtocol: Committed batch 78
[2024-11-12T09:48:41.899+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO FileFormatWriter: Write Job a30e180e-1195-4159-945e-0b7453969b18 committed. Elapsed time: 41 ms.
[2024-11-12T09:48:41.899+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO FileFormatWriter: Finished processing stats for write job a30e180e-1195-4159-945e-0b7453969b18.
[2024-11-12T09:48:41.908+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/78 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.78.8c3429a0-fa46-4460-947c-b8ab2a000463.tmp
[2024-11-12T09:48:41.942+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.78.8c3429a0-fa46-4460-947c-b8ab2a000463.tmp to hdfs://namenode:9000/spark_checkpoint/commits/78
[2024-11-12T09:48:41.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:41.945+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:41.945+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:41.945+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:41.945+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:40.954Z",
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "batchId" : 78,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2300123001230012,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0121457489878543,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "addBatch" : 835,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "commitOffsets" : 44,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "queryPlanning" : 32,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "triggerExecution" : 988,
[2024-11-12T09:48:41.946+0000] {spark_submit.py:495} INFO - "walCommit" : 59
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - "0" : 664
[2024-11-12T09:48:41.947+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:41.948+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:41.948+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:41.948+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:41.948+0000] {spark_submit.py:495} INFO - "0" : 665
[2024-11-12T09:48:41.948+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:41.949+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:41.949+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:41.949+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:41.949+0000] {spark_submit.py:495} INFO - "0" : 665
[2024-11-12T09:48:41.949+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2300123001230012,
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0121457489878543,
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:41.950+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:41.951+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:41.951+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:41.952+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:41.953+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:41.953+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:41.954+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:41.955+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:41.963+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/79 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.79.acf38db3-0129-4311-a196-1865d83b5144.tmp
[2024-11-12T09:48:41.996+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.79.acf38db3-0129-4311-a196-1865d83b5144.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/79
[2024-11-12T09:48:41.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:41 INFO MicroBatchExecution: Committed offsets for batch 79. Metadata OffsetSeqMetadata(0,1731404921953,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:42.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:42.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:42.030+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:42.031+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:42.039+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 78
[2024-11-12T09:48:42.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:42.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:42.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Got job 79 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:42.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Final stage: ResultStage 79 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:42.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:42.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:42.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[320] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:42.092+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:42.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:42.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:42.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:42.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[320] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:42.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
[2024-11-12T09:48:42.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 79) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:42.120+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:42.759+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 79) in 656 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:42.760+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool
[2024-11-12T09:48:42.761+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: ResultStage 79 (start at NativeMethodAccessorImpl.java:0) finished in 0.683 s
[2024-11-12T09:48:42.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:42.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
[2024-11-12T09:48:42.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO DAGScheduler: Job 79 finished: start at NativeMethodAccessorImpl.java:0, took 0.686790 s
[2024-11-12T09:48:42.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO FileFormatWriter: Start to commit write Job 030c2e13-be8d-409c-9288-a6a641a9ff4d.
[2024-11-12T09:48:42.781+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/79.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.79.compact.ec7c3c3c-9ded-4728-a0ef-8cc6aabf7357.tmp
[2024-11-12T09:48:42.972+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.79.compact.ec7c3c3c-9ded-4728-a0ef-8cc6aabf7357.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/79.compact
[2024-11-12T09:48:42.973+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO ManifestFileCommitProtocol: Committed batch 79
[2024-11-12T09:48:42.974+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO FileFormatWriter: Write Job 030c2e13-be8d-409c-9288-a6a641a9ff4d committed. Elapsed time: 211 ms.
[2024-11-12T09:48:42.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO FileFormatWriter: Finished processing stats for write job 030c2e13-be8d-409c-9288-a6a641a9ff4d.
[2024-11-12T09:48:42.990+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/79 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.79.1526f4e2-1ba9-481f-90bf-178f9f650ff0.tmp
[2024-11-12T09:48:43.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.79.1526f4e2-1ba9-481f-90bf-178f9f650ff0.tmp to hdfs://namenode:9000/spark_checkpoint/commits/79
[2024-11-12T09:48:43.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:43.050+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:41.944Z",
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "batchId" : 79,
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0101010101010102,
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9057971014492753,
[2024-11-12T09:48:43.051+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:43.052+0000] {spark_submit.py:495} INFO - "addBatch" : 954,
[2024-11-12T09:48:43.052+0000] {spark_submit.py:495} INFO - "commitOffsets" : 71,
[2024-11-12T09:48:43.052+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:43.052+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "queryPlanning" : 25,
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1104,
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "walCommit" : 43
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:43.053+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:43.054+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:43.054+0000] {spark_submit.py:495} INFO - "0" : 665
[2024-11-12T09:48:43.054+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.055+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.055+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:43.055+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:43.055+0000] {spark_submit.py:495} INFO - "0" : 666
[2024-11-12T09:48:43.056+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.056+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.056+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:43.057+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:43.058+0000] {spark_submit.py:495} INFO - "0" : 666
[2024-11-12T09:48:43.058+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.058+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.058+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:43.058+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0101010101010102,
[2024-11-12T09:48:43.059+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9057971014492753,
[2024-11-12T09:48:43.059+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:43.059+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:43.059+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:43.061+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:43.061+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.061+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:43.061+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:43.063+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:43.064+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:43.064+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.064+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.078+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/80 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.80.1ce2d8a5-bbdc-4c60-ada6-d101cc53c7c2.tmp
[2024-11-12T09:48:43.125+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.80.1ce2d8a5-bbdc-4c60-ada6-d101cc53c7c2.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/80
[2024-11-12T09:48:43.125+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO MicroBatchExecution: Committed offsets for batch 80. Metadata OffsetSeqMetadata(0,1731404923057,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:43.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:43.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:43.164+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:43.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:43.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 78, 79
[2024-11-12T09:48:43.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:43.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:43.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Got job 80 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:43.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Final stage: ResultStage 80 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:43.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:43.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:43.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:43.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:43.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:48:43.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:43.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:43.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[324] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:43.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks resource profile 0
[2024-11-12T09:48:43.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 80) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:43.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:43.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 80) in 603 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:43.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool
[2024-11-12T09:48:43.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: ResultStage 80 (start at NativeMethodAccessorImpl.java:0) finished in 0.646 s
[2024-11-12T09:48:43.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:43.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished
[2024-11-12T09:48:43.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO DAGScheduler: Job 80 finished: start at NativeMethodAccessorImpl.java:0, took 0.648739 s
[2024-11-12T09:48:43.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO FileFormatWriter: Start to commit write Job 9a3db7c8-1e33-4fea-8664-f048871cb36a.
[2024-11-12T09:48:43.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/80 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.80.30b69c53-6f22-46a7-900e-cfab239dd9bd.tmp
[2024-11-12T09:48:43.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.80.30b69c53-6f22-46a7-900e-cfab239dd9bd.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/80
[2024-11-12T09:48:43.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO ManifestFileCommitProtocol: Committed batch 80
[2024-11-12T09:48:43.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO FileFormatWriter: Write Job 9a3db7c8-1e33-4fea-8664-f048871cb36a committed. Elapsed time: 45 ms.
[2024-11-12T09:48:43.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO FileFormatWriter: Finished processing stats for write job 9a3db7c8-1e33-4fea-8664-f048871cb36a.
[2024-11-12T09:48:43.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/80 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.80.bddbaba0-09ad-41e7-be62-cb68de2a185c.tmp
[2024-11-12T09:48:43.968+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.80.bddbaba0-09ad-41e7-be62-cb68de2a185c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/80
[2024-11-12T09:48:43.970+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:43.970+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:43.970+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:43.970+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:43.970+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:43.049Z",
[2024-11-12T09:48:43.970+0000] {spark_submit.py:495} INFO - "batchId" : 80,
[2024-11-12T09:48:43.971+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:43.971+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9049773755656109,
[2024-11-12T09:48:43.971+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.088139281828074,
[2024-11-12T09:48:43.971+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:43.976+0000] {spark_submit.py:495} INFO - "addBatch" : 769,
[2024-11-12T09:48:43.976+0000] {spark_submit.py:495} INFO - "commitOffsets" : 48,
[2024-11-12T09:48:43.977+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:43.979+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:48:43.980+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:48:43.980+0000] {spark_submit.py:495} INFO - "triggerExecution" : 919,
[2024-11-12T09:48:43.980+0000] {spark_submit.py:495} INFO - "walCommit" : 67
[2024-11-12T09:48:43.980+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.981+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:43.981+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:43.982+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:43.982+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:43.982+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:43.982+0000] {spark_submit.py:495} INFO - "0" : 666
[2024-11-12T09:48:43.982+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.983+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.983+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:43.983+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - "0" : 667
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - "0" : 667
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:43.984+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:43.985+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9049773755656109,
[2024-11-12T09:48:43.985+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.088139281828074,
[2024-11-12T09:48:43.985+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:43.985+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:43.985+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:43.985+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:43.986+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.986+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:43.986+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:43.986+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:43.987+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:43.987+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.987+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:43.989+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/81 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.81.4ebeee55-eba2-43ab-ac4f-3c0c34db3c84.tmp
[2024-11-12T09:48:44.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.81.4ebeee55-eba2-43ab-ac4f-3c0c34db3c84.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/81
[2024-11-12T09:48:44.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO MicroBatchExecution: Committed offsets for batch 81. Metadata OffsetSeqMetadata(0,1731404923980,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:44.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:44.076+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:44.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:44.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:44.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 78, 79, 80, 80
[2024-11-12T09:48:44.124+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:44.162+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:44.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Got job 81 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:44.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Final stage: ResultStage 81 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:44.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:44.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:44.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[328] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:44.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:48:44.209+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:48:44.210+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:44.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:44.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[328] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:44.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0
[2024-11-12T09:48:44.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 81) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:44.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:44.881+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 81) in 665 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:44.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool
[2024-11-12T09:48:44.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: ResultStage 81 (start at NativeMethodAccessorImpl.java:0) finished in 0.713 s
[2024-11-12T09:48:44.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:44.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
[2024-11-12T09:48:44.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO DAGScheduler: Job 81 finished: start at NativeMethodAccessorImpl.java:0, took 0.722150 s
[2024-11-12T09:48:44.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO FileFormatWriter: Start to commit write Job 8a98ba71-dccf-4071-91ba-a264139ea88c.
[2024-11-12T09:48:44.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/81 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.81.2aa7b7a1-9ab1-45fd-b0b4-50e40cbcd2eb.tmp
[2024-11-12T09:48:44.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.81.2aa7b7a1-9ab1-45fd-b0b4-50e40cbcd2eb.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/81
[2024-11-12T09:48:44.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO ManifestFileCommitProtocol: Committed batch 81
[2024-11-12T09:48:44.922+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO FileFormatWriter: Write Job 8a98ba71-dccf-4071-91ba-a264139ea88c committed. Elapsed time: 37 ms.
[2024-11-12T09:48:44.922+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO FileFormatWriter: Finished processing stats for write job 8a98ba71-dccf-4071-91ba-a264139ea88c.
[2024-11-12T09:48:44.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/81 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.81.a0a884f2-afeb-4d7f-9bb4-f5e5547d75e3.tmp
[2024-11-12T09:48:45.360+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.81.a0a884f2-afeb-4d7f-9bb4-f5e5547d75e3.tmp to hdfs://namenode:9000/spark_checkpoint/commits/81
[2024-11-12T09:48:45.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:45.369+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:45.369+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:45.369+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:45.370+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:43.969Z",
[2024-11-12T09:48:45.370+0000] {spark_submit.py:495} INFO - "batchId" : 81,
[2024-11-12T09:48:45.370+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0869565217391304,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7189072609633357,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "addBatch" : 838,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "commitOffsets" : 438,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "queryPlanning" : 34,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1391,
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "walCommit" : 64
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:45.371+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:45.372+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:45.372+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:45.373+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:45.373+0000] {spark_submit.py:495} INFO - "0" : 667
[2024-11-12T09:48:45.373+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:45.373+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:45.373+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:45.374+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:45.374+0000] {spark_submit.py:495} INFO - "0" : 668
[2024-11-12T09:48:45.374+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:45.374+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:45.375+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:45.375+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:45.375+0000] {spark_submit.py:495} INFO - "0" : 668
[2024-11-12T09:48:45.375+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:45.375+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0869565217391304,
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7189072609633357,
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:45.376+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:45.377+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:45.378+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/82 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.82.c05d8846-7734-4afc-bd19-0f8cd4a6eb61.tmp
[2024-11-12T09:48:45.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.82.c05d8846-7734-4afc-bd19-0f8cd4a6eb61.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/82
[2024-11-12T09:48:45.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO MicroBatchExecution: Committed offsets for batch 82. Metadata OffsetSeqMetadata(0,1731404925373,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:45.440+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:45.442+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:45.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:45.464+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:45.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 80, 81, 81
[2024-11-12T09:48:45.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:45.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:45.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Got job 82 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:45.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Final stage: ResultStage 82 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:45.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:45.507+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:45.507+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[332] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:45.523+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:48:45.530+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:48:45.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:45.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:45.532+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[332] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:45.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0
[2024-11-12T09:48:45.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 82) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:45.561+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:45.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 82) in 365 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:45.899+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool
[2024-11-12T09:48:45.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: ResultStage 82 (start at NativeMethodAccessorImpl.java:0) finished in 0.397 s
[2024-11-12T09:48:45.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:45.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished
[2024-11-12T09:48:45.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO DAGScheduler: Job 82 finished: start at NativeMethodAccessorImpl.java:0, took 0.399417 s
[2024-11-12T09:48:45.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO FileFormatWriter: Start to commit write Job 52363a1f-90b8-4668-893a-8f21ba5a8ca5.
[2024-11-12T09:48:45.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/82 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.82.d2cdb53b-74d3-478c-9d1f-ffdff71be9b2.tmp
[2024-11-12T09:48:45.958+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.82.d2cdb53b-74d3-478c-9d1f-ffdff71be9b2.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/82
[2024-11-12T09:48:45.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO ManifestFileCommitProtocol: Committed batch 82
[2024-11-12T09:48:45.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO FileFormatWriter: Write Job 52363a1f-90b8-4668-893a-8f21ba5a8ca5 committed. Elapsed time: 58 ms.
[2024-11-12T09:48:45.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO FileFormatWriter: Finished processing stats for write job 52363a1f-90b8-4668-893a-8f21ba5a8ca5.
[2024-11-12T09:48:45.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/82 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.82.fc858203-85c2-4d3d-a68f-d7f57b7d75f9.tmp
[2024-11-12T09:48:46.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.82.fc858203-85c2-4d3d-a68f-d7f57b7d75f9.tmp to hdfs://namenode:9000/spark_checkpoint/commits/82
[2024-11-12T09:48:46.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:45.362Z",
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "batchId" : 82,
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7178750897343862,
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.5313935681470137,
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "addBatch" : 515,
[2024-11-12T09:48:46.019+0000] {spark_submit.py:495} INFO - "commitOffsets" : 56,
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "queryPlanning" : 25,
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "triggerExecution" : 653,
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "walCommit" : 45
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:46.020+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "0" : 668
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "0" : 669
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - "0" : 669
[2024-11-12T09:48:46.021+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.022+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.022+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:46.022+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7178750897343862,
[2024-11-12T09:48:46.023+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.5313935681470137,
[2024-11-12T09:48:46.023+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:46.023+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:46.023+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:46.023+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:46.024+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.025+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:46.025+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:46.025+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:46.026+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:46.027+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.027+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/83 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.83.05e5794b-ea7f-4432-bf1d-9565f012357d.tmp
[2024-11-12T09:48:46.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.83.05e5794b-ea7f-4432-bf1d-9565f012357d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/83
[2024-11-12T09:48:46.081+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO MicroBatchExecution: Committed offsets for batch 83. Metadata OffsetSeqMetadata(0,1731404926027,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:46.098+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:46.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:46.118+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:46.120+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:46.128+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 81, 82, 82
[2024-11-12T09:48:46.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:46.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:46.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Got job 83 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:46.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Final stage: ResultStage 83 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:46.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:46.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:46.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[336] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:46.209+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:48:46.210+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_79_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:46.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:46.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:48:46.218+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:46.219+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:46.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[336] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:46.221+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks resource profile 0
[2024-11-12T09:48:46.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 83) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:46.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_78_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:46.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:46.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:46.256+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:46.257+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_82_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:46.276+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_80_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:46.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:46.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_81_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:46.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:46.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 83) in 651 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:46.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool
[2024-11-12T09:48:46.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: ResultStage 83 (start at NativeMethodAccessorImpl.java:0) finished in 0.700 s
[2024-11-12T09:48:46.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:46.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished
[2024-11-12T09:48:46.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO DAGScheduler: Job 83 finished: start at NativeMethodAccessorImpl.java:0, took 0.707136 s
[2024-11-12T09:48:46.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO FileFormatWriter: Start to commit write Job 97fcd64b-d2c5-4fa2-ba2d-134b0b288d14.
[2024-11-12T09:48:46.890+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/83 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.83.bd8df7ce-0c47-4e32-b1d1-724cae1e7869.tmp
[2024-11-12T09:48:46.919+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.83.bd8df7ce-0c47-4e32-b1d1-724cae1e7869.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/83
[2024-11-12T09:48:46.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO ManifestFileCommitProtocol: Committed batch 83
[2024-11-12T09:48:46.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO FileFormatWriter: Write Job 97fcd64b-d2c5-4fa2-ba2d-134b0b288d14 committed. Elapsed time: 44 ms.
[2024-11-12T09:48:46.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO FileFormatWriter: Finished processing stats for write job 97fcd64b-d2c5-4fa2-ba2d-134b0b288d14.
[2024-11-12T09:48:46.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/83 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.83.47c964f2-0c66-401d-8216-60b746044b6e.tmp
[2024-11-12T09:48:46.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.83.47c964f2-0c66-401d-8216-60b746044b6e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/83
[2024-11-12T09:48:46.962+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:46.963+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:46.964+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:46.964+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:46.018Z",
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "batchId" : 83,
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.524390243902439,
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0615711252653928,
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:46.965+0000] {spark_submit.py:495} INFO - "addBatch" : 815,
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "commitOffsets" : 40,
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "triggerExecution" : 942,
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "walCommit" : 52
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.966+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "0" : 669
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - "0" : 670
[2024-11-12T09:48:46.967+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.968+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.968+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:46.968+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:46.968+0000] {spark_submit.py:495} INFO - "0" : 670
[2024-11-12T09:48:46.968+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.968+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:46.969+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:46.969+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.524390243902439,
[2024-11-12T09:48:46.969+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0615711252653928,
[2024-11-12T09:48:46.970+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:46.972+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:46.972+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:46.972+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.973+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:46.978+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/84 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.84.6eca3da7-3230-462c-af45-3c5c30235a29.tmp
[2024-11-12T09:48:47.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.84.6eca3da7-3230-462c-af45-3c5c30235a29.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/84
[2024-11-12T09:48:47.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO MicroBatchExecution: Committed offsets for batch 84. Metadata OffsetSeqMetadata(0,1731404926966,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:47.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:47.037+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:47.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:47.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:47.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 82, 83, 83
[2024-11-12T09:48:47.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:47.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:47.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Got job 84 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:47.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Final stage: ResultStage 84 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:47.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:47.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:47.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:47.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:47.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:47.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:47.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:47.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[340] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:47.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks resource profile 0
[2024-11-12T09:48:47.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 84) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:47.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:47.775+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 84) in 653 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:47.775+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool
[2024-11-12T09:48:47.776+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: ResultStage 84 (start at NativeMethodAccessorImpl.java:0) finished in 0.689 s
[2024-11-12T09:48:47.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:47.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished
[2024-11-12T09:48:47.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO DAGScheduler: Job 84 finished: start at NativeMethodAccessorImpl.java:0, took 0.694353 s
[2024-11-12T09:48:47.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO FileFormatWriter: Start to commit write Job 652d5a8e-ce89-4d37-99c4-aeb088d94fff.
[2024-11-12T09:48:47.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/84 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.84.e401aa44-8ffe-4ea6-97c2-0432f6809f0b.tmp
[2024-11-12T09:48:47.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.84.e401aa44-8ffe-4ea6-97c2-0432f6809f0b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/84
[2024-11-12T09:48:47.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO ManifestFileCommitProtocol: Committed batch 84
[2024-11-12T09:48:47.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO FileFormatWriter: Write Job 652d5a8e-ce89-4d37-99c4-aeb088d94fff committed. Elapsed time: 56 ms.
[2024-11-12T09:48:47.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO FileFormatWriter: Finished processing stats for write job 652d5a8e-ce89-4d37-99c4-aeb088d94fff.
[2024-11-12T09:48:47.839+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/84 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.84.bc422c4a-fd12-4ba9-a97e-52aeaeb08d4d.tmp
[2024-11-12T09:48:47.894+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.84.bc422c4a-fd12-4ba9-a97e-52aeaeb08d4d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/84
[2024-11-12T09:48:47.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:47.895+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:46.962Z",
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "batchId" : 84,
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0593220338983051,
[2024-11-12T09:48:47.896+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0729613733905579,
[2024-11-12T09:48:47.897+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:47.897+0000] {spark_submit.py:495} INFO - "addBatch" : 792,
[2024-11-12T09:48:47.897+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-12T09:48:47.898+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:47.899+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:48:47.899+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:48:47.900+0000] {spark_submit.py:495} INFO - "triggerExecution" : 932,
[2024-11-12T09:48:47.900+0000] {spark_submit.py:495} INFO - "walCommit" : 50
[2024-11-12T09:48:47.901+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:47.901+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:47.901+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:47.901+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:47.901+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:47.902+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:47.902+0000] {spark_submit.py:495} INFO - "0" : 670
[2024-11-12T09:48:47.902+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:47.903+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:47.903+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:47.903+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:47.903+0000] {spark_submit.py:495} INFO - "0" : 671
[2024-11-12T09:48:47.903+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "0" : 671
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0593220338983051,
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0729613733905579,
[2024-11-12T09:48:47.904+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:47.911+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:47.912+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:47.912+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:47.913+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:47.918+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/85 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.85.9aec25f8-6b47-4ccb-a423-e48a362c6510.tmp
[2024-11-12T09:48:47.963+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.85.9aec25f8-6b47-4ccb-a423-e48a362c6510.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/85
[2024-11-12T09:48:47.965+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:47 INFO MicroBatchExecution: Committed offsets for batch 85. Metadata OffsetSeqMetadata(0,1731404927900,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:48.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:48.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:48.031+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:48.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:48.058+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 83, 84, 84
[2024-11-12T09:48:48.061+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:48.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:48.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Got job 85 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:48.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Final stage: ResultStage 85 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:48.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:48.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:48.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[344] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:48.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:48.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:48:48.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:48.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:48.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[344] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:48.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0
[2024-11-12T09:48:48.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 85) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:48.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:48.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 85) in 691 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:48.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool
[2024-11-12T09:48:48.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: ResultStage 85 (start at NativeMethodAccessorImpl.java:0) finished in 0.750 s
[2024-11-12T09:48:48.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:48.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished
[2024-11-12T09:48:48.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO DAGScheduler: Job 85 finished: start at NativeMethodAccessorImpl.java:0, took 0.762881 s
[2024-11-12T09:48:48.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO FileFormatWriter: Start to commit write Job 08c19990-e8ba-4296-8906-66eca4a6905b.
[2024-11-12T09:48:48.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/85 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.85.094314a2-33c1-4fb0-a9b8-9463ea5b5065.tmp
[2024-11-12T09:48:48.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.85.094314a2-33c1-4fb0-a9b8-9463ea5b5065.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/85
[2024-11-12T09:48:48.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO ManifestFileCommitProtocol: Committed batch 85
[2024-11-12T09:48:48.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO FileFormatWriter: Write Job 08c19990-e8ba-4296-8906-66eca4a6905b committed. Elapsed time: 52 ms.
[2024-11-12T09:48:48.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO FileFormatWriter: Finished processing stats for write job 08c19990-e8ba-4296-8906-66eca4a6905b.
[2024-11-12T09:48:48.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/85 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.85.62e7850b-7331-4169-ba6e-3c74725c606b.tmp
[2024-11-12T09:48:48.978+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.85.62e7850b-7331-4169-ba6e-3c74725c606b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/85
[2024-11-12T09:48:48.979+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:48.979+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:48.979+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:47.895Z",
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "batchId" : 85,
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0718113612004287,
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9233610341643583,
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "addBatch" : 907,
[2024-11-12T09:48:48.980+0000] {spark_submit.py:495} INFO - "commitOffsets" : 54,
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - "queryPlanning" : 48,
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1083,
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - "walCommit" : 62
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:48.981+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "0" : 671
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - "0" : 672
[2024-11-12T09:48:48.982+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:48.983+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:48.983+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:48.983+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:48.983+0000] {spark_submit.py:495} INFO - "0" : 672
[2024-11-12T09:48:48.984+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:48.984+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:48.985+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:48.985+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0718113612004287,
[2024-11-12T09:48:48.986+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9233610341643583,
[2024-11-12T09:48:48.986+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:48.986+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:48.986+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:48.986+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:48.987+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:48.987+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:48.987+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:48.988+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:48.988+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:48.988+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:48.988+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:48.999+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/86 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.86.d0869172-50e5-4b23-b9a9-40eaf3fd9631.tmp
[2024-11-12T09:48:49.039+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.86.d0869172-50e5-4b23-b9a9-40eaf3fd9631.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/86
[2024-11-12T09:48:49.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO MicroBatchExecution: Committed offsets for batch 86. Metadata OffsetSeqMetadata(0,1731404928985,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:49.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:49.059+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:49.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:49.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:49.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 84, 85, 85
[2024-11-12T09:48:49.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:49.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:49.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Got job 86 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:49.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Final stage: ResultStage 86 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:49.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:49.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:49.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[348] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:49.159+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:48:49.163+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:48:49.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:49.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:49.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[348] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:49.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0
[2024-11-12T09:48:49.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 86) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:49.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:49.808+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 86) in 640 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:49.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: ResultStage 86 (start at NativeMethodAccessorImpl.java:0) finished in 0.675 s
[2024-11-12T09:48:49.811+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:49.811+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool
[2024-11-12T09:48:49.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished
[2024-11-12T09:48:49.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO DAGScheduler: Job 86 finished: start at NativeMethodAccessorImpl.java:0, took 0.682313 s
[2024-11-12T09:48:49.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO FileFormatWriter: Start to commit write Job ac9332c6-7d54-49db-b197-582a1eea0c94.
[2024-11-12T09:48:49.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/86 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.86.4d17fc25-1ded-4c22-bf6f-46145758fb1b.tmp
[2024-11-12T09:48:49.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.86.4d17fc25-1ded-4c22-bf6f-46145758fb1b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/86
[2024-11-12T09:48:49.871+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO ManifestFileCommitProtocol: Committed batch 86
[2024-11-12T09:48:49.872+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO FileFormatWriter: Write Job ac9332c6-7d54-49db-b197-582a1eea0c94 committed. Elapsed time: 58 ms.
[2024-11-12T09:48:49.872+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO FileFormatWriter: Finished processing stats for write job ac9332c6-7d54-49db-b197-582a1eea0c94.
[2024-11-12T09:48:49.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/86 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.86.13dc66c7-52d5-4d91-b908-136602828084.tmp
[2024-11-12T09:48:49.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.86.13dc66c7-52d5-4d91-b908-136602828084.tmp to hdfs://namenode:9000/spark_checkpoint/commits/86
[2024-11-12T09:48:49.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:49.927+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:49.927+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:48.979Z",
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "batchId" : 86,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9225092250922509,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0582010582010584,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "addBatch" : 800,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "commitOffsets" : 53,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "queryPlanning" : 20,
[2024-11-12T09:48:49.928+0000] {spark_submit.py:495} INFO - "triggerExecution" : 945,
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - "0" : 672
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:49.929+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - "0" : 673
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - "0" : 673
[2024-11-12T09:48:49.930+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9225092250922509,
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0582010582010584,
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:49.931+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:49.932+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:49.932+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:49.932+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:49.932+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:49.932+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:49.939+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/87 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.87.c4bb2267-32c9-4b59-a1c4-6e44169a2954.tmp
[2024-11-12T09:48:49.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.87.c4bb2267-32c9-4b59-a1c4-6e44169a2954.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/87
[2024-11-12T09:48:49.978+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO MicroBatchExecution: Committed offsets for batch 87. Metadata OffsetSeqMetadata(0,1731404929930,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:49.989+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:49.990+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86
[2024-11-12T09:48:50.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:50.051+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:50.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Got job 87 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:50.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Final stage: ResultStage 87 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:50.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:50.058+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:50.059+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[352] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:50.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:48:50.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:48:50.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:50.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:50.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[352] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:50.090+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks resource profile 0
[2024-11-12T09:48:50.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 87) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:50.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:50.766+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 87) in 677 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:50.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool
[2024-11-12T09:48:50.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: ResultStage 87 (start at NativeMethodAccessorImpl.java:0) finished in 0.714 s
[2024-11-12T09:48:50.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:50.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
[2024-11-12T09:48:50.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO DAGScheduler: Job 87 finished: start at NativeMethodAccessorImpl.java:0, took 0.719533 s
[2024-11-12T09:48:50.771+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO FileFormatWriter: Start to commit write Job f95358c0-4227-4c44-9eb0-52988b5e3c23.
[2024-11-12T09:48:50.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/87 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.87.821e5565-18d6-49b5-bfdc-ac84adf34460.tmp
[2024-11-12T09:48:50.831+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.87.821e5565-18d6-49b5-bfdc-ac84adf34460.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/87
[2024-11-12T09:48:50.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO ManifestFileCommitProtocol: Committed batch 87
[2024-11-12T09:48:50.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO FileFormatWriter: Write Job f95358c0-4227-4c44-9eb0-52988b5e3c23 committed. Elapsed time: 61 ms.
[2024-11-12T09:48:50.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO FileFormatWriter: Finished processing stats for write job f95358c0-4227-4c44-9eb0-52988b5e3c23.
[2024-11-12T09:48:50.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/87 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.87.07c46966-447c-41e2-8483-871178fbfb8e.tmp
[2024-11-12T09:48:50.879+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.87.07c46966-447c-41e2-8483-871178fbfb8e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/87
[2024-11-12T09:48:50.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:50.889+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:50.889+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:50.890+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:50.890+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:49.927Z",
[2024-11-12T09:48:50.890+0000] {spark_submit.py:495} INFO - "batchId" : 87,
[2024-11-12T09:48:50.890+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:50.890+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0548523206751055,
[2024-11-12T09:48:50.890+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.050420168067227,
[2024-11-12T09:48:50.892+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:50.893+0000] {spark_submit.py:495} INFO - "addBatch" : 839,
[2024-11-12T09:48:50.893+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:48:50.893+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:50.893+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:48:50.894+0000] {spark_submit.py:495} INFO - "queryPlanning" : 13,
[2024-11-12T09:48:50.894+0000] {spark_submit.py:495} INFO - "triggerExecution" : 952,
[2024-11-12T09:48:50.894+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:48:50.894+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "0" : 673
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - "0" : 674
[2024-11-12T09:48:50.895+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:50.896+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:50.896+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:50.896+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:50.897+0000] {spark_submit.py:495} INFO - "0" : 674
[2024-11-12T09:48:50.897+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:50.897+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:50.897+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:50.897+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0548523206751055,
[2024-11-12T09:48:50.898+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.050420168067227,
[2024-11-12T09:48:50.898+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:50.898+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:50.898+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:50.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:50.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/88 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.88.c7c1ef99-50d0-4a6d-be70-818432ef449a.tmp
[2024-11-12T09:48:50.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.88.c7c1ef99-50d0-4a6d-be70-818432ef449a.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/88
[2024-11-12T09:48:50.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO MicroBatchExecution: Committed offsets for batch 88. Metadata OffsetSeqMetadata(0,1731404930893,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:50.957+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:50.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:50 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 87
[2024-11-12T09:48:51.000+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:51.037+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:51.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Got job 88 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:51.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Final stage: ResultStage 88 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:51.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:51.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:51.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[356] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:51.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:48:51.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_84_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:51.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:48:51.076+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:51.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:51.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[356] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:51.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0
[2024-11-12T09:48:51.079+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:51.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 88) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:51.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_86_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:51.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:51.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:51.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_85_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:51.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:51.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_87_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:51.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:51.159+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_83_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:51.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:51.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 88) in 652 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:51.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool
[2024-11-12T09:48:51.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: ResultStage 88 (start at NativeMethodAccessorImpl.java:0) finished in 0.696 s
[2024-11-12T09:48:51.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:51.738+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished
[2024-11-12T09:48:51.738+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO DAGScheduler: Job 88 finished: start at NativeMethodAccessorImpl.java:0, took 0.701419 s
[2024-11-12T09:48:51.738+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO FileFormatWriter: Start to commit write Job 7c8e2fa0-168d-4c3f-97c0-ddc7d8b0203c.
[2024-11-12T09:48:51.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/88 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.88.847a9312-813b-4bc9-b5dd-873bb3c7dc42.tmp
[2024-11-12T09:48:51.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.88.847a9312-813b-4bc9-b5dd-873bb3c7dc42.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/88
[2024-11-12T09:48:51.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO ManifestFileCommitProtocol: Committed batch 88
[2024-11-12T09:48:51.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO FileFormatWriter: Write Job 7c8e2fa0-168d-4c3f-97c0-ddc7d8b0203c committed. Elapsed time: 48 ms.
[2024-11-12T09:48:51.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO FileFormatWriter: Finished processing stats for write job 7c8e2fa0-168d-4c3f-97c0-ddc7d8b0203c.
[2024-11-12T09:48:51.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/88 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.88.12d58830-6a7d-44ed-b9b6-589bf0732702.tmp
[2024-11-12T09:48:51.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.88.12d58830-6a7d-44ed-b9b6-589bf0732702.tmp to hdfs://namenode:9000/spark_checkpoint/commits/88
[2024-11-12T09:48:51.839+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:51.840+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:51.841+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:51.841+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:50.885Z",
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "batchId" : 88,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0438413361169103,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.050420168067227,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "addBatch" : 821,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "commitOffsets" : 50,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:48:51.842+0000] {spark_submit.py:495} INFO - "queryPlanning" : 26,
[2024-11-12T09:48:51.843+0000] {spark_submit.py:495} INFO - "triggerExecution" : 952,
[2024-11-12T09:48:51.843+0000] {spark_submit.py:495} INFO - "walCommit" : 44
[2024-11-12T09:48:51.843+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "0" : 674
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - "0" : 675
[2024-11-12T09:48:51.844+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - "0" : 675
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0438413361169103,
[2024-11-12T09:48:51.845+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.050420168067227,
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:51.846+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:51.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/89 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.89.4a2e8442-95fc-4777-8b6a-39311446ec4d.tmp
[2024-11-12T09:48:51.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.89.4a2e8442-95fc-4777-8b6a-39311446ec4d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/89
[2024-11-12T09:48:51.929+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO MicroBatchExecution: Committed offsets for batch 89. Metadata OffsetSeqMetadata(0,1731404931890,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:51.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:51.948+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:51.958+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:51.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:51.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 87, 88, 88
[2024-11-12T09:48:51.973+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:52.004+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:52.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Got job 89 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:52.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Final stage: ResultStage 89 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:52.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:52.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:52.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[360] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:52.020+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:52.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:52.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:52.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:52.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[360] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:52.027+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0
[2024-11-12T09:48:52.027+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 89) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:52.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:52.676+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 89) in 648 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:52.676+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool
[2024-11-12T09:48:52.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: ResultStage 89 (start at NativeMethodAccessorImpl.java:0) finished in 0.670 s
[2024-11-12T09:48:52.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:52.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
[2024-11-12T09:48:52.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO DAGScheduler: Job 89 finished: start at NativeMethodAccessorImpl.java:0, took 0.673749 s
[2024-11-12T09:48:52.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO FileFormatWriter: Start to commit write Job 55a2bbd2-990b-4d7c-9365-1437d7974274.
[2024-11-12T09:48:52.687+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/89.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.89.compact.a89365f4-239c-459d-a53c-4c3e9849b91a.tmp
[2024-11-12T09:48:52.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.89.compact.a89365f4-239c-459d-a53c-4c3e9849b91a.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/89.compact
[2024-11-12T09:48:52.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO ManifestFileCommitProtocol: Committed batch 89
[2024-11-12T09:48:52.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO FileFormatWriter: Write Job 55a2bbd2-990b-4d7c-9365-1437d7974274 committed. Elapsed time: 221 ms.
[2024-11-12T09:48:52.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO FileFormatWriter: Finished processing stats for write job 55a2bbd2-990b-4d7c-9365-1437d7974274.
[2024-11-12T09:48:52.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/89 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.89.f03479f0-3258-4c41-9cbe-634e6f31459f.tmp
[2024-11-12T09:48:52.990+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.89.f03479f0-3258-4c41-9cbe-634e6f31459f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/89
[2024-11-12T09:48:52.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:52.996+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:52.998+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:52.999+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:52.999+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:51.884Z",
[2024-11-12T09:48:52.999+0000] {spark_submit.py:495} INFO - "batchId" : 89,
[2024-11-12T09:48:52.999+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:53.000+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-11-12T09:48:53.000+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9049773755656109,
[2024-11-12T09:48:53.000+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:53.000+0000] {spark_submit.py:495} INFO - "addBatch" : 953,
[2024-11-12T09:48:53.001+0000] {spark_submit.py:495} INFO - "commitOffsets" : 85,
[2024-11-12T09:48:53.001+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:53.001+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:48:53.001+0000] {spark_submit.py:495} INFO - "queryPlanning" : 21,
[2024-11-12T09:48:53.002+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1105,
[2024-11-12T09:48:53.002+0000] {spark_submit.py:495} INFO - "walCommit" : 37
[2024-11-12T09:48:53.003+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:53.003+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:53.003+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:53.003+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:53.004+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:53.005+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:53.005+0000] {spark_submit.py:495} INFO - "0" : 675
[2024-11-12T09:48:53.005+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:53.005+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:53.005+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:53.006+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:53.007+0000] {spark_submit.py:495} INFO - "0" : 676
[2024-11-12T09:48:53.007+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:53.007+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:53.007+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:53.008+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:53.008+0000] {spark_submit.py:495} INFO - "0" : 676
[2024-11-12T09:48:53.008+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:53.008+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:53.008+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:53.008+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-11-12T09:48:53.009+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9049773755656109,
[2024-11-12T09:48:53.009+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:53.010+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:53.010+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:53.010+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:53.010+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:53.011+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:53.011+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:53.011+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:53.011+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:53.012+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:53.012+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:53.013+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/90 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.90.467b7781-7d95-44b3-8c2d-1878d8f93b17.tmp
[2024-11-12T09:48:53.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.90.467b7781-7d95-44b3-8c2d-1878d8f93b17.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/90
[2024-11-12T09:48:53.057+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO MicroBatchExecution: Committed offsets for batch 90. Metadata OffsetSeqMetadata(0,1731404933003,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:53.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:53.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:53.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:53.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:53.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 87, 88, 88, 89
[2024-11-12T09:48:53.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:53.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:53.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Got job 90 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:53.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Final stage: ResultStage 90 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:53.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:53.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:53.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[364] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:53.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:48:53.198+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:48:53.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:53.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:53.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[364] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:53.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks resource profile 0
[2024-11-12T09:48:53.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 90) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:53.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:53.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 90) in 671 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:53.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool
[2024-11-12T09:48:53.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: ResultStage 90 (start at NativeMethodAccessorImpl.java:0) finished in 0.705 s
[2024-11-12T09:48:53.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:53.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
[2024-11-12T09:48:53.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO DAGScheduler: Job 90 finished: start at NativeMethodAccessorImpl.java:0, took 0.708603 s
[2024-11-12T09:48:53.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO FileFormatWriter: Start to commit write Job 77aea00c-cfe1-4cef-a397-afe138eb3181.
[2024-11-12T09:48:53.894+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/90 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.90.1f66f0ee-2bac-41e6-9839-bcf320c7ba94.tmp
[2024-11-12T09:48:53.935+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.90.1f66f0ee-2bac-41e6-9839-bcf320c7ba94.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/90
[2024-11-12T09:48:53.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO ManifestFileCommitProtocol: Committed batch 90
[2024-11-12T09:48:53.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO FileFormatWriter: Write Job 77aea00c-cfe1-4cef-a397-afe138eb3181 committed. Elapsed time: 56 ms.
[2024-11-12T09:48:53.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO FileFormatWriter: Finished processing stats for write job 77aea00c-cfe1-4cef-a397-afe138eb3181.
[2024-11-12T09:48:53.952+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/90 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.90.0077c065-fffc-4ecd-a375-57c0428f436f.tmp
[2024-11-12T09:48:53.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.90.0077c065-fffc-4ecd-a375-57c0428f436f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/90
[2024-11-12T09:48:53.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:53.994+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:53.994+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:53.994+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:52.991Z",
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "batchId" : 90,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.903342366757001,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9990009990009991,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "addBatch" : 839,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "commitOffsets" : 58,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:53.995+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:48:53.996+0000] {spark_submit.py:495} INFO - "queryPlanning" : 35,
[2024-11-12T09:48:53.996+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1001,
[2024-11-12T09:48:53.998+0000] {spark_submit.py:495} INFO - "walCommit" : 53
[2024-11-12T09:48:53.998+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:53.999+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:54.000+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:54.001+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:54.001+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:54.002+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:54.003+0000] {spark_submit.py:495} INFO - "0" : 676
[2024-11-12T09:48:54.004+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.004+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.004+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:54.004+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:54.004+0000] {spark_submit.py:495} INFO - "0" : 677
[2024-11-12T09:48:54.005+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.005+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.005+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:54.006+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:54.006+0000] {spark_submit.py:495} INFO - "0" : 677
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.903342366757001,
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9990009990009991,
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:54.007+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:54.009+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:54.009+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:54.009+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.009+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:54.013+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:54.014+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:54.014+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:54.014+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.015+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.015+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/91 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.91.16fa8513-c071-49c2-9983-378d226ae559.tmp
[2024-11-12T09:48:54.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.91.16fa8513-c071-49c2-9983-378d226ae559.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/91
[2024-11-12T09:48:54.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO MicroBatchExecution: Committed offsets for batch 91. Metadata OffsetSeqMetadata(0,1731404934000,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:54.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:54.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:54.125+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:54.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:54.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 88, 89, 90, 90
[2024-11-12T09:48:54.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:54.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:54.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Got job 91 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:54.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Final stage: ResultStage 91 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:54.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:54.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:54.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:54.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:48:54.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:48:54.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:54.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:54.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:54.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0
[2024-11-12T09:48:54.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 91) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:54.221+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:54.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 91) in 649 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:54.847+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool
[2024-11-12T09:48:54.849+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: ResultStage 91 (start at NativeMethodAccessorImpl.java:0) finished in 0.678 s
[2024-11-12T09:48:54.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:54.855+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished
[2024-11-12T09:48:54.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO DAGScheduler: Job 91 finished: start at NativeMethodAccessorImpl.java:0, took 0.680476 s
[2024-11-12T09:48:54.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO FileFormatWriter: Start to commit write Job 942f848d-2dd7-4263-9ae2-4f853f108433.
[2024-11-12T09:48:54.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/91 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.91.84b14c8b-b16c-434f-8780-99a13820cf45.tmp
[2024-11-12T09:48:54.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.91.84b14c8b-b16c-434f-8780-99a13820cf45.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/91
[2024-11-12T09:48:54.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO ManifestFileCommitProtocol: Committed batch 91
[2024-11-12T09:48:54.910+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO FileFormatWriter: Write Job 942f848d-2dd7-4263-9ae2-4f853f108433 committed. Elapsed time: 61 ms.
[2024-11-12T09:48:54.913+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO FileFormatWriter: Finished processing stats for write job 942f848d-2dd7-4263-9ae2-4f853f108433.
[2024-11-12T09:48:54.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/91 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.91.9bc53b9c-e96b-412f-9206-4d602c81bfd4.tmp
[2024-11-12T09:48:54.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.91.9bc53b9c-e96b-412f-9206-4d602c81bfd4.tmp to hdfs://namenode:9000/spark_checkpoint/commits/91
[2024-11-12T09:48:54.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:53.993Z",
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "batchId" : 91,
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.998003992015968,
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0245901639344261,
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:54.972+0000] {spark_submit.py:495} INFO - "addBatch" : 800,
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "commitOffsets" : 59,
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "queryPlanning" : 23,
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "triggerExecution" : 976,
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "walCommit" : 80
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:54.973+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:54.974+0000] {spark_submit.py:495} INFO - "0" : 677
[2024-11-12T09:48:54.974+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.974+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.974+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:54.974+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:54.975+0000] {spark_submit.py:495} INFO - "0" : 678
[2024-11-12T09:48:54.975+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.975+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.975+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:54.975+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:54.976+0000] {spark_submit.py:495} INFO - "0" : 678
[2024-11-12T09:48:54.976+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.976+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:54.976+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:54.976+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.998003992015968,
[2024-11-12T09:48:54.977+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0245901639344261,
[2024-11-12T09:48:54.977+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:54.977+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:54.977+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:54.977+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:54.978+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.978+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:54.978+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:54.979+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:54.980+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:54.980+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.980+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:54.988+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/92 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.92.b00daed4-729d-4cfd-b324-bf522a36c2ab.tmp
[2024-11-12T09:48:55.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.92.b00daed4-729d-4cfd-b324-bf522a36c2ab.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/92
[2024-11-12T09:48:55.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO MicroBatchExecution: Committed offsets for batch 92. Metadata OffsetSeqMetadata(0,1731404934974,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:55.059+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:55.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:55.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:55.088+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:55.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 90, 91, 91
[2024-11-12T09:48:55.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:55.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:55.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Got job 92 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:55.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Final stage: ResultStage 92 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:55.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:55.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:55.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[372] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:55.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:48:55.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:48:55.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:55.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:55.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[372] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:55.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0
[2024-11-12T09:48:55.180+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 92) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:55.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:55.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 92) in 642 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:55.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool
[2024-11-12T09:48:55.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: ResultStage 92 (start at NativeMethodAccessorImpl.java:0) finished in 0.690 s
[2024-11-12T09:48:55.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:55.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
[2024-11-12T09:48:55.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO DAGScheduler: Job 92 finished: start at NativeMethodAccessorImpl.java:0, took 0.692260 s
[2024-11-12T09:48:55.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO FileFormatWriter: Start to commit write Job 8607a398-5c88-4538-8770-f85356f7bdb3.
[2024-11-12T09:48:55.831+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/92 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.92.da7bee52-54af-4502-b8fb-122063c10828.tmp
[2024-11-12T09:48:55.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.92.da7bee52-54af-4502-b8fb-122063c10828.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/92
[2024-11-12T09:48:55.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO ManifestFileCommitProtocol: Committed batch 92
[2024-11-12T09:48:55.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO FileFormatWriter: Write Job 8607a398-5c88-4538-8770-f85356f7bdb3 committed. Elapsed time: 58 ms.
[2024-11-12T09:48:55.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO FileFormatWriter: Finished processing stats for write job 8607a398-5c88-4538-8770-f85356f7bdb3.
[2024-11-12T09:48:55.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/92 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.92.f5c723b5-02d3-4fac-9335-caeb38073383.tmp
[2024-11-12T09:48:55.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.92.f5c723b5-02d3-4fac-9335-caeb38073383.tmp to hdfs://namenode:9000/spark_checkpoint/commits/92
[2024-11-12T09:48:55.933+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:55.934+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:55.934+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:54.971Z",
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "batchId" : 92,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0224948875255624,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0427528675703859,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "addBatch" : 812,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:55.935+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "triggerExecution" : 959,
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "walCommit" : 67
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "0" : 678
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:55.936+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - "0" : 679
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - "0" : 679
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:55.937+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:55.938+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:55.938+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0224948875255624,
[2024-11-12T09:48:55.939+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0427528675703859,
[2024-11-12T09:48:55.942+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:55.943+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:55.943+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:55.943+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:55.944+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:55.944+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:55.945+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:55.945+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:55.945+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:55.946+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:55.946+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:55.956+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/93 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.93.ea91171f-d3dd-4b2b-9026-4d83bb285adb.tmp
[2024-11-12T09:48:56.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.93.ea91171f-d3dd-4b2b-9026-4d83bb285adb.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/93
[2024-11-12T09:48:56.002+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:55 INFO MicroBatchExecution: Committed offsets for batch 93. Metadata OffsetSeqMetadata(0,1731404935945,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:56.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:56.017+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:56.030+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:56.031+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:56.038+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 91, 92, 92
[2024-11-12T09:48:56.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:56.076+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:56.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Got job 93 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:56.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Final stage: ResultStage 93 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:56.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:56.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:56.077+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[376] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:56.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:48:56.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:48:56.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:48:56.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:56.106+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[376] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:56.106+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0
[2024-11-12T09:48:56.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 93) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:56.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:48:56.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 93) in 662 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:56.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool
[2024-11-12T09:48:56.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: ResultStage 93 (start at NativeMethodAccessorImpl.java:0) finished in 0.692 s
[2024-11-12T09:48:56.771+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:56.772+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
[2024-11-12T09:48:56.772+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO DAGScheduler: Job 93 finished: start at NativeMethodAccessorImpl.java:0, took 0.696071 s
[2024-11-12T09:48:56.773+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO FileFormatWriter: Start to commit write Job 05373d5e-eee1-4e5e-aa46-fdaa7f6ff223.
[2024-11-12T09:48:56.778+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/93 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.93.bfae5fa0-591d-40f2-af36-788f5ed4361e.tmp
[2024-11-12T09:48:57.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.93.bfae5fa0-591d-40f2-af36-788f5ed4361e.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/93
[2024-11-12T09:48:57.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO ManifestFileCommitProtocol: Committed batch 93
[2024-11-12T09:48:57.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO FileFormatWriter: Write Job 05373d5e-eee1-4e5e-aa46-fdaa7f6ff223 committed. Elapsed time: 454 ms.
[2024-11-12T09:48:57.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO FileFormatWriter: Finished processing stats for write job 05373d5e-eee1-4e5e-aa46-fdaa7f6ff223.
[2024-11-12T09:48:57.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/93 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.93.33529bfe-b6e9-45f1-a3ef-a4e06c0d94a0.tmp
[2024-11-12T09:48:57.277+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.93.33529bfe-b6e9-45f1-a3ef-a4e06c0d94a0.tmp to hdfs://namenode:9000/spark_checkpoint/commits/93
[2024-11-12T09:48:57.285+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:57.286+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:57.287+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:57.287+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:57.287+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:55.933Z",
[2024-11-12T09:48:57.287+0000] {spark_submit.py:495} INFO - "batchId" : 93,
[2024-11-12T09:48:57.287+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:57.288+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0395010395010396,
[2024-11-12T09:48:57.288+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.744047619047619,
[2024-11-12T09:48:57.288+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:57.288+0000] {spark_submit.py:495} INFO - "addBatch" : 1207,
[2024-11-12T09:48:57.288+0000] {spark_submit.py:495} INFO - "commitOffsets" : 50,
[2024-11-12T09:48:57.289+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:57.289+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:48:57.289+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:48:57.289+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1344,
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:57.290+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - "0" : 679
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - "0" : 680
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:57.291+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:57.292+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:57.292+0000] {spark_submit.py:495} INFO - "0" : 680
[2024-11-12T09:48:57.292+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:57.292+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:57.293+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:57.294+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0395010395010396,
[2024-11-12T09:48:57.294+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.744047619047619,
[2024-11-12T09:48:57.294+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:57.295+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:57.295+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:57.295+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:57.296+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:57.296+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:57.296+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:57.296+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:57.297+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:57.298+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:57.298+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:57.309+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/94 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.94.c3a11d8c-1fd3-40e7-84aa-58a64c8049d1.tmp
[2024-11-12T09:48:57.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.94.c3a11d8c-1fd3-40e7-84aa-58a64c8049d1.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/94
[2024-11-12T09:48:57.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO MicroBatchExecution: Committed offsets for batch 94. Metadata OffsetSeqMetadata(0,1731404937295,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:57.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:57.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:57.400+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:57.401+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:57.416+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 92, 93, 93
[2024-11-12T09:48:57.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:57.455+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:57.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO DAGScheduler: Got job 94 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:57.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO DAGScheduler: Final stage: ResultStage 94 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:57.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:57.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:57.462+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[380] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:57.490+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:48:57.494+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:48:57.494+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:48:57.495+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:57.495+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[380] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:57.496+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks resource profile 0
[2024-11-12T09:48:57.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 94) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:57.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:57 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:48:58.003+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 94) in 505 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:58.004+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool
[2024-11-12T09:48:58.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: ResultStage 94 (start at NativeMethodAccessorImpl.java:0) finished in 0.543 s
[2024-11-12T09:48:58.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:58.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished
[2024-11-12T09:48:58.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Job 94 finished: start at NativeMethodAccessorImpl.java:0, took 0.554611 s
[2024-11-12T09:48:58.012+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO FileFormatWriter: Start to commit write Job 1c4c6ce6-7960-4760-b60c-7b2c5c5c1291.
[2024-11-12T09:48:58.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/94 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.94.4a596a05-ef56-49fd-86a2-7defb7b58b39.tmp
[2024-11-12T09:48:58.080+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.94.4a596a05-ef56-49fd-86a2-7defb7b58b39.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/94
[2024-11-12T09:48:58.081+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO ManifestFileCommitProtocol: Committed batch 94
[2024-11-12T09:48:58.081+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO FileFormatWriter: Write Job 1c4c6ce6-7960-4760-b60c-7b2c5c5c1291 committed. Elapsed time: 68 ms.
[2024-11-12T09:48:58.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO FileFormatWriter: Finished processing stats for write job 1c4c6ce6-7960-4760-b60c-7b2c5c5c1291.
[2024-11-12T09:48:58.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/94 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.94.5170eaec-d91e-4f7c-818a-08ec5017f105.tmp
[2024-11-12T09:48:58.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.94.5170eaec-d91e-4f7c-818a-08ec5017f105.tmp to hdfs://namenode:9000/spark_checkpoint/commits/94
[2024-11-12T09:48:58.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:58.134+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:58.134+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:58.135+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:58.135+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:57.284Z",
[2024-11-12T09:48:58.135+0000] {spark_submit.py:495} INFO - "batchId" : 94,
[2024-11-12T09:48:58.135+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:58.135+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7401924500370096,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.179245283018868,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "addBatch" : 691,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "commitOffsets" : 50,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "queryPlanning" : 26,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "triggerExecution" : 848,
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "walCommit" : 69
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:58.136+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:58.137+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:58.137+0000] {spark_submit.py:495} INFO - "0" : 680
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - "0" : 681
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:58.138+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "0" : 681
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7401924500370096,
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.179245283018868,
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:58.139+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:58.140+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:58.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/95 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.95.2cb03f9a-bce4-4237-83af-09a51c0fbaf1.tmp
[2024-11-12T09:48:58.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.95.2cb03f9a-bce4-4237-83af-09a51c0fbaf1.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/95
[2024-11-12T09:48:58.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO MicroBatchExecution: Committed offsets for batch 95. Metadata OffsetSeqMetadata(0,1731404938139,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:58.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:58.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:58.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:58.228+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:58.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 93, 94, 94
[2024-11-12T09:48:58.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:58.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:58.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Got job 95 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:58.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Final stage: ResultStage 95 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:58.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:58.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:58.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:58.337+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_89_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:48:58.340+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:48:58.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:48:58.351+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:48:58.352+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:48:58.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:58.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:58.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0
[2024-11-12T09:48:58.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 95) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:58.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_93_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:48:58.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:58.422+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:48:58.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_90_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:58.439+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:48:58.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_92_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:58.464+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:48:58.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_94_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:58.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:48:58.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_91_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:58.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:58.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_88_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:58.545+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:58 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:48:59.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 95) in 723 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:48:59.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool
[2024-11-12T09:48:59.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: ResultStage 95 (start at NativeMethodAccessorImpl.java:0) finished in 0.807 s
[2024-11-12T09:48:59.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:48:59.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
[2024-11-12T09:48:59.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Job 95 finished: start at NativeMethodAccessorImpl.java:0, took 0.811895 s
[2024-11-12T09:48:59.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO FileFormatWriter: Start to commit write Job 8a2b9d3f-21d4-4490-add1-31edc26e96d2.
[2024-11-12T09:48:59.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/95 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.95.498c334e-c77a-4d2c-9a79-e400ace6d390.tmp
[2024-11-12T09:48:59.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.95.498c334e-c77a-4d2c-9a79-e400ace6d390.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/95
[2024-11-12T09:48:59.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO ManifestFileCommitProtocol: Committed batch 95
[2024-11-12T09:48:59.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO FileFormatWriter: Write Job 8a2b9d3f-21d4-4490-add1-31edc26e96d2 committed. Elapsed time: 98 ms.
[2024-11-12T09:48:59.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO FileFormatWriter: Finished processing stats for write job 8a2b9d3f-21d4-4490-add1-31edc26e96d2.
[2024-11-12T09:48:59.198+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/95 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.95.a846c6f3-b0e7-42d8-b78c-f6950bb1d065.tmp
[2024-11-12T09:48:59.265+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.95.a846c6f3-b0e7-42d8-b78c-f6950bb1d065.tmp to hdfs://namenode:9000/spark_checkpoint/commits/95
[2024-11-12T09:48:59.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:48:59.267+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:48:59.268+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:48:59.268+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:48:59.268+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:58.133Z",
[2024-11-12T09:48:59.268+0000] {spark_submit.py:495} INFO - "batchId" : 95,
[2024-11-12T09:48:59.274+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:59.275+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1778563015312131,
[2024-11-12T09:48:59.275+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.88339222614841,
[2024-11-12T09:48:59.276+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:48:59.277+0000] {spark_submit.py:495} INFO - "addBatch" : 978,
[2024-11-12T09:48:59.277+0000] {spark_submit.py:495} INFO - "commitOffsets" : 79,
[2024-11-12T09:48:59.277+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:48:59.277+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:48:59.278+0000] {spark_submit.py:495} INFO - "queryPlanning" : 12,
[2024-11-12T09:48:59.278+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1132,
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - "walCommit" : 55
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:48:59.279+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:59.280+0000] {spark_submit.py:495} INFO - "0" : 681
[2024-11-12T09:48:59.280+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:59.280+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "0" : 682
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "0" : 682
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:48:59.281+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:48:59.282+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1778563015312131,
[2024-11-12T09:48:59.282+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.88339222614841,
[2024-11-12T09:48:59.282+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:48:59.282+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:48:59.283+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:48:59.283+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:48:59.284+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:59.284+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:48:59.284+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:48:59.284+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:48:59.286+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:48:59.286+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:59.286+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:48:59.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/96 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.96.b8c5f263-cd80-4b3b-a9f8-7214a19c2a15.tmp
[2024-11-12T09:48:59.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.96.b8c5f263-cd80-4b3b-a9f8-7214a19c2a15.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/96
[2024-11-12T09:48:59.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO MicroBatchExecution: Committed offsets for batch 96. Metadata OffsetSeqMetadata(0,1731404939285,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:48:59.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:59.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:59.399+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:59.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:48:59.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94, 95, 95
[2024-11-12T09:48:59.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:48:59.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:48:59.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Got job 96 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:48:59.442+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Final stage: ResultStage 96 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:48:59.442+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:48:59.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:48:59.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[388] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:48:59.469+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:48:59.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:48:59.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:48:59.472+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:48:59.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[388] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:48:59.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO TaskSchedulerImpl: Adding task set 96.0 with 1 tasks resource profile 0
[2024-11-12T09:48:59.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 96) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:48:59.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:48:59 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:49:00.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 96) in 550 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:49:00.030+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool
[2024-11-12T09:49:00.030+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: ResultStage 96 (start at NativeMethodAccessorImpl.java:0) finished in 0.582 s
[2024-11-12T09:49:00.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:49:00.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
[2024-11-12T09:49:00.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Job 96 finished: start at NativeMethodAccessorImpl.java:0, took 0.584831 s
[2024-11-12T09:49:00.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO FileFormatWriter: Start to commit write Job 263d9e4a-c66c-4401-b6f8-8c926274697e.
[2024-11-12T09:49:00.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/96 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.96.589bd066-201d-4600-a1c6-777c5c9d91a4.tmp
[2024-11-12T09:49:00.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.96.589bd066-201d-4600-a1c6-777c5c9d91a4.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/96
[2024-11-12T09:49:00.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO ManifestFileCommitProtocol: Committed batch 96
[2024-11-12T09:49:00.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO FileFormatWriter: Write Job 263d9e4a-c66c-4401-b6f8-8c926274697e committed. Elapsed time: 63 ms.
[2024-11-12T09:49:00.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO FileFormatWriter: Finished processing stats for write job 263d9e4a-c66c-4401-b6f8-8c926274697e.
[2024-11-12T09:49:00.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/96 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.96.80dedb1e-434d-4042-8c0a-8747a03e2bf0.tmp
[2024-11-12T09:49:00.156+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.96.80dedb1e-434d-4042-8c0a-8747a03e2bf0.tmp to hdfs://namenode:9000/spark_checkpoint/commits/96
[2024-11-12T09:49:00.162+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:00.162+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:00.163+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:00.163+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:00.164+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:48:59.267Z",
[2024-11-12T09:49:00.164+0000] {spark_submit.py:495} INFO - "batchId" : 96,
[2024-11-12T09:49:00.164+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:00.165+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8818342151675486,
[2024-11-12T09:49:00.165+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.124859392575928,
[2024-11-12T09:49:00.165+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:00.165+0000] {spark_submit.py:495} INFO - "addBatch" : 710,
[2024-11-12T09:49:00.165+0000] {spark_submit.py:495} INFO - "commitOffsets" : 62,
[2024-11-12T09:49:00.166+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:49:00.166+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-12T09:49:00.166+0000] {spark_submit.py:495} INFO - "queryPlanning" : 33,
[2024-11-12T09:49:00.167+0000] {spark_submit.py:495} INFO - "triggerExecution" : 889,
[2024-11-12T09:49:00.168+0000] {spark_submit.py:495} INFO - "walCommit" : 61
[2024-11-12T09:49:00.169+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:00.170+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:00.170+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:00.171+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:00.172+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:00.172+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:00.172+0000] {spark_submit.py:495} INFO - "0" : 682
[2024-11-12T09:49:00.173+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:00.173+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:00.173+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:00.173+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - "0" : 683
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - "0" : 683
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:00.174+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:00.175+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:00.175+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8818342151675486,
[2024-11-12T09:49:00.175+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.124859392575928,
[2024-11-12T09:49:00.175+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:00.176+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:00.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/97 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.97.4174efa5-f80e-49d8-8fda-43c22faeb621.tmp
[2024-11-12T09:49:00.233+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.97.4174efa5-f80e-49d8-8fda-43c22faeb621.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/97
[2024-11-12T09:49:00.233+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO MicroBatchExecution: Committed offsets for batch 97. Metadata OffsetSeqMetadata(0,1731404940167,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:49:00.252+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:00.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:00.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:00.273+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:00.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 95, 96, 96
[2024-11-12T09:49:00.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:49:00.331+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:49:00.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Got job 97 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:49:00.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Final stage: ResultStage 97 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:49:00.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:49:00.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:49:00.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[392] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:49:00.360+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:49:00.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:49:00.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:49:00.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:49:00.367+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[392] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:49:00.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks resource profile 0
[2024-11-12T09:49:00.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 97) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:49:00.400+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:00 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:49:01.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 97) in 670 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:49:01.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool
[2024-11-12T09:49:01.041+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: ResultStage 97 (start at NativeMethodAccessorImpl.java:0) finished in 0.706 s
[2024-11-12T09:49:01.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:49:01.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished
[2024-11-12T09:49:01.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Job 97 finished: start at NativeMethodAccessorImpl.java:0, took 0.710077 s
[2024-11-12T09:49:01.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO FileFormatWriter: Start to commit write Job d51dc990-4e85-431f-9735-1c15300c8b25.
[2024-11-12T09:49:01.051+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/97 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.97.4d916a72-53d9-4bb4-adfd-28532d0c60df.tmp
[2024-11-12T09:49:01.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.97.4d916a72-53d9-4bb4-adfd-28532d0c60df.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/97
[2024-11-12T09:49:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO ManifestFileCommitProtocol: Committed batch 97
[2024-11-12T09:49:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO FileFormatWriter: Write Job d51dc990-4e85-431f-9735-1c15300c8b25 committed. Elapsed time: 62 ms.
[2024-11-12T09:49:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO FileFormatWriter: Finished processing stats for write job d51dc990-4e85-431f-9735-1c15300c8b25.
[2024-11-12T09:49:01.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/97 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.97.f57ddf16-8d03-47c8-81c8-b4e4edab4a6b.tmp
[2024-11-12T09:49:01.158+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.97.f57ddf16-8d03-47c8-81c8-b4e4edab4a6b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/97
[2024-11-12T09:49:01.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:01.164+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:01.166+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:01.167+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:01.167+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:00.160Z",
[2024-11-12T09:49:01.167+0000] {spark_submit.py:495} INFO - "batchId" : 97,
[2024-11-12T09:49:01.168+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:01.168+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1198208286674132,
[2024-11-12T09:49:01.168+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0030090270812437,
[2024-11-12T09:49:01.168+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:01.169+0000] {spark_submit.py:495} INFO - "addBatch" : 845,
[2024-11-12T09:49:01.169+0000] {spark_submit.py:495} INFO - "commitOffsets" : 52,
[2024-11-12T09:49:01.169+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:49:01.170+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:49:01.170+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:49:01.170+0000] {spark_submit.py:495} INFO - "triggerExecution" : 997,
[2024-11-12T09:49:01.170+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:49:01.171+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:01.171+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:01.171+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:01.172+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:01.172+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:01.172+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:01.173+0000] {spark_submit.py:495} INFO - "0" : 683
[2024-11-12T09:49:01.174+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:01.177+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:01.178+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:01.178+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:01.179+0000] {spark_submit.py:495} INFO - "0" : 684
[2024-11-12T09:49:01.179+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:01.179+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:01.181+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:01.181+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:01.181+0000] {spark_submit.py:495} INFO - "0" : 684
[2024-11-12T09:49:01.181+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:01.181+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:01.181+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:01.182+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1198208286674132,
[2024-11-12T09:49:01.183+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0030090270812437,
[2024-11-12T09:49:01.183+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:01.183+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:01.183+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:01.183+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:01.183+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:01.184+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/98 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.98.2c4c0464-bc7a-49ee-abd7-34c26767c5e2.tmp
[2024-11-12T09:49:01.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.98.2c4c0464-bc7a-49ee-abd7-34c26767c5e2.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/98
[2024-11-12T09:49:01.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO MicroBatchExecution: Committed offsets for batch 98. Metadata OffsetSeqMetadata(0,1731404941169,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:49:01.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:01.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:01.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:01.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:01.290+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 96, 97, 97
[2024-11-12T09:49:01.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:49:01.327+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:49:01.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Got job 98 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:49:01.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Final stage: ResultStage 98 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:49:01.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:49:01.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:49:01.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[396] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:49:01.354+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:49:01.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:49:01.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:49:01.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:49:01.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[396] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:49:01.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks resource profile 0
[2024-11-12T09:49:01.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 98) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:49:01.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:01 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:49:02.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 98) in 689 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:49:02.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool
[2024-11-12T09:49:02.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: ResultStage 98 (start at NativeMethodAccessorImpl.java:0) finished in 0.727 s
[2024-11-12T09:49:02.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:49:02.058+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
[2024-11-12T09:49:02.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Job 98 finished: start at NativeMethodAccessorImpl.java:0, took 0.729821 s
[2024-11-12T09:49:02.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO FileFormatWriter: Start to commit write Job 26735691-b51d-454b-961e-47b453837d8b.
[2024-11-12T09:49:02.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/98 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.98.642bb35c-a153-4758-8177-52bd152ad7f7.tmp
[2024-11-12T09:49:02.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.98.642bb35c-a153-4758-8177-52bd152ad7f7.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/98
[2024-11-12T09:49:02.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO ManifestFileCommitProtocol: Committed batch 98
[2024-11-12T09:49:02.135+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO FileFormatWriter: Write Job 26735691-b51d-454b-961e-47b453837d8b committed. Elapsed time: 76 ms.
[2024-11-12T09:49:02.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO FileFormatWriter: Finished processing stats for write job 26735691-b51d-454b-961e-47b453837d8b.
[2024-11-12T09:49:02.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/98 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.98.25f18486-cf82-4002-b4fe-419f1127abb4.tmp
[2024-11-12T09:49:02.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.98.25f18486-cf82-4002-b4fe-419f1127abb4.tmp to hdfs://namenode:9000/spark_checkpoint/commits/98
[2024-11-12T09:49:02.217+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:02.218+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:02.219+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:02.219+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:02.219+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:01.160Z",
[2024-11-12T09:49:02.222+0000] {spark_submit.py:495} INFO - "batchId" : 98,
[2024-11-12T09:49:02.222+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:02.223+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0,
[2024-11-12T09:49:02.223+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9541984732824427,
[2024-11-12T09:49:02.223+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:02.223+0000] {spark_submit.py:495} INFO - "addBatch" : 869,
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "commitOffsets" : 69,
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1048,
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "walCommit" : 68
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:02.224+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:02.225+0000] {spark_submit.py:495} INFO - "0" : 684
[2024-11-12T09:49:02.225+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:02.226+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:02.226+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:02.226+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "0" : 685
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "0" : 685
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0,
[2024-11-12T09:49:02.227+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9541984732824427,
[2024-11-12T09:49:02.228+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:02.228+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:02.228+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:02.229+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:02.229+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:02.229+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:02.229+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:02.231+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:02.231+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:02.231+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:02.231+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:02.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/99 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.99.4380202f-c458-48a0-97d3-b957e036922c.tmp
[2024-11-12T09:49:02.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.99.4380202f-c458-48a0-97d3-b957e036922c.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/99
[2024-11-12T09:49:02.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO MicroBatchExecution: Committed offsets for batch 99. Metadata OffsetSeqMetadata(0,1731404942230,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:49:02.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:02.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:02.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:02.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:02.439+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 97, 98, 98
[2024-11-12T09:49:02.449+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:49:02.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:49:02.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Got job 99 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:49:02.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Final stage: ResultStage 99 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:49:02.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:49:02.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:49:02.511+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[400] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:49:02.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:49:02.545+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:49:02.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:49:02.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:49:02.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[400] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:49:02.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks resource profile 0
[2024-11-12T09:49:02.548+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 99) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:49:02.574+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:02 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:49:03.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 99) in 459 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:49:03.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool
[2024-11-12T09:49:03.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: ResultStage 99 (start at NativeMethodAccessorImpl.java:0) finished in 0.496 s
[2024-11-12T09:49:03.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:49:03.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
[2024-11-12T09:49:03.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Job 99 finished: start at NativeMethodAccessorImpl.java:0, took 0.502478 s
[2024-11-12T09:49:03.008+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO FileFormatWriter: Start to commit write Job dd1fbf3d-e0c2-4174-9c2c-26a96ec26e2a.
[2024-11-12T09:49:03.019+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/99.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.99.compact.62551eca-4a01-4739-a102-6e8d2225657b.tmp
[2024-11-12T09:49:03.602+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.99.compact.62551eca-4a01-4739-a102-6e8d2225657b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/99.compact
[2024-11-12T09:49:03.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO ManifestFileCommitProtocol: Committed batch 99
[2024-11-12T09:49:03.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO FileFormatWriter: Write Job dd1fbf3d-e0c2-4174-9c2c-26a96ec26e2a committed. Elapsed time: 593 ms.
[2024-11-12T09:49:03.608+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO FileFormatWriter: Finished processing stats for write job dd1fbf3d-e0c2-4174-9c2c-26a96ec26e2a.
[2024-11-12T09:49:03.619+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/99 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.99.518f5057-730b-4c9b-8fb8-b9bf1681de48.tmp
[2024-11-12T09:49:03.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.99.518f5057-730b-4c9b-8fb8-b9bf1681de48.tmp to hdfs://namenode:9000/spark_checkpoint/commits/99
[2024-11-12T09:49:03.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:02.215Z",
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "batchId" : 99,
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:03.661+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9478672985781991,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.693000693000693,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "addBatch" : 1227,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "commitOffsets" : 55,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:49:03.662+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1443,
[2024-11-12T09:49:03.663+0000] {spark_submit.py:495} INFO - "walCommit" : 116
[2024-11-12T09:49:03.663+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:03.663+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:03.663+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:03.664+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:03.664+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:03.664+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:03.664+0000] {spark_submit.py:495} INFO - "0" : 685
[2024-11-12T09:49:03.664+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:03.665+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:03.666+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:03.667+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:03.667+0000] {spark_submit.py:495} INFO - "0" : 686
[2024-11-12T09:49:03.667+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:03.667+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:03.667+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:03.667+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:03.668+0000] {spark_submit.py:495} INFO - "0" : 686
[2024-11-12T09:49:03.668+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:03.668+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:03.668+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:03.669+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9478672985781991,
[2024-11-12T09:49:03.669+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.693000693000693,
[2024-11-12T09:49:03.669+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:03.669+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:03.669+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:03.669+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:03.670+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:03.670+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:03.670+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:03.670+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:03.671+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:03.671+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:03.671+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:03.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/100 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.100.4634f544-6a68-4501-9f42-4aac95ddd471.tmp
[2024-11-12T09:49:03.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.100.4634f544-6a68-4501-9f42-4aac95ddd471.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/100
[2024-11-12T09:49:03.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO MicroBatchExecution: Committed offsets for batch 100. Metadata OffsetSeqMetadata(0,1731404943666,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:49:03.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:03.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:03.768+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:03.773+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:03.784+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 97, 98, 98, 99
[2024-11-12T09:49:03.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:49:03.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:49:03.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Got job 100 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:49:03.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Final stage: ResultStage 100 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:49:03.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:49:03.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:49:03.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[404] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:49:03.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:49:03.864+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:49:03.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:49:03.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:49:03.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[404] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:49:03.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks resource profile 0
[2024-11-12T09:49:03.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 100) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:49:03.904+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:03 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:49:04.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 100) in 177 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:49:04.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool
[2024-11-12T09:49:04.053+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: ResultStage 100 (start at NativeMethodAccessorImpl.java:0) finished in 0.229 s
[2024-11-12T09:49:04.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:49:04.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished
[2024-11-12T09:49:04.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Job 100 finished: start at NativeMethodAccessorImpl.java:0, took 0.230918 s
[2024-11-12T09:49:04.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO FileFormatWriter: Start to commit write Job c72cb8dc-a690-493d-9d3f-3c59c9df8c25.
[2024-11-12T09:49:04.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/100 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.100.6219468c-e5af-4bb7-a9f5-21647da271b3.tmp
[2024-11-12T09:49:04.117+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.100.6219468c-e5af-4bb7-a9f5-21647da271b3.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/100
[2024-11-12T09:49:04.118+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO ManifestFileCommitProtocol: Committed batch 100
[2024-11-12T09:49:04.120+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO FileFormatWriter: Write Job c72cb8dc-a690-493d-9d3f-3c59c9df8c25 committed. Elapsed time: 64 ms.
[2024-11-12T09:49:04.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO FileFormatWriter: Finished processing stats for write job c72cb8dc-a690-493d-9d3f-3c59c9df8c25.
[2024-11-12T09:49:04.126+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/100 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.100.954252c2-b782-4a7f-bf25-2b05dc801ade.tmp
[2024-11-12T09:49:04.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.100.954252c2-b782-4a7f-bf25-2b05dc801ade.tmp to hdfs://namenode:9000/spark_checkpoint/commits/100
[2024-11-12T09:49:04.572+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:04.573+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:04.573+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:04.573+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:03.660Z",
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "batchId" : 100,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6920415224913494,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.097694840834248,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "addBatch" : 362,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "commitOffsets" : 454,
[2024-11-12T09:49:04.574+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:49:04.575+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:49:04.575+0000] {spark_submit.py:495} INFO - "queryPlanning" : 32,
[2024-11-12T09:49:04.575+0000] {spark_submit.py:495} INFO - "triggerExecution" : 911,
[2024-11-12T09:49:04.575+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:49:04.577+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:04.577+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:04.577+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:04.577+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - "0" : 686
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - "0" : 687
[2024-11-12T09:49:04.578+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:04.579+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:04.579+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:04.579+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:04.580+0000] {spark_submit.py:495} INFO - "0" : 687
[2024-11-12T09:49:04.580+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:04.580+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:04.580+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:04.581+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6920415224913494,
[2024-11-12T09:49:04.581+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.097694840834248,
[2024-11-12T09:49:04.581+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:04.581+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:04.581+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:04.582+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:04.582+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:04.582+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:04.582+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:04.582+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:04.583+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:04.583+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:04.583+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:04.601+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/101 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.101.75e74bd9-c00f-41e2-ab99-8e915e9f8200.tmp
[2024-11-12T09:49:04.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.101.75e74bd9-c00f-41e2-ab99-8e915e9f8200.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/101
[2024-11-12T09:49:04.675+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO MicroBatchExecution: Committed offsets for batch 101. Metadata OffsetSeqMetadata(0,1731404944580,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:49:04.702+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:04.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:04.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:04.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:49:04.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 98, 99, 100, 100
[2024-11-12T09:49:04.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:49:04.778+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:49:04.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Got job 101 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:49:04.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Final stage: ResultStage 101 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:49:04.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:49:04.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:49:04.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[408] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:49:04.800+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:49:04.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:49:04.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:49:04.806+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:49:04.808+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[408] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:49:04.808+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks resource profile 0
[2024-11-12T09:49:04.811+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 101) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:49:04.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:04 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:49:05.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 101) in 665 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:49:05.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool
[2024-11-12T09:49:05.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO DAGScheduler: ResultStage 101 (start at NativeMethodAccessorImpl.java:0) finished in 0.696 s
[2024-11-12T09:49:05.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:49:05.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished
[2024-11-12T09:49:05.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO DAGScheduler: Job 101 finished: start at NativeMethodAccessorImpl.java:0, took 0.698210 s
[2024-11-12T09:49:05.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO FileFormatWriter: Start to commit write Job 630fc40c-88eb-4f07-ac05-3cbdeaba3855.
[2024-11-12T09:49:05.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/101 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.101.da093081-4691-4255-a4fa-ba7f955c65b4.tmp
[2024-11-12T09:49:05.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.101.da093081-4691-4255-a4fa-ba7f955c65b4.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/101
[2024-11-12T09:49:05.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO ManifestFileCommitProtocol: Committed batch 101
[2024-11-12T09:49:05.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO FileFormatWriter: Write Job 630fc40c-88eb-4f07-ac05-3cbdeaba3855 committed. Elapsed time: 49 ms.
[2024-11-12T09:49:05.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO FileFormatWriter: Finished processing stats for write job 630fc40c-88eb-4f07-ac05-3cbdeaba3855.
[2024-11-12T09:49:05.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/101 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.101.0b5a324c-63a7-49a8-adb2-f2f2f3e9cfee.tmp
[2024-11-12T09:49:05.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.101.0b5a324c-63a7-49a8-adb2-f2f2f3e9cfee.tmp to hdfs://namenode:9000/spark_checkpoint/commits/101
[2024-11-12T09:49:05.565+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:05.565+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:05.566+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:05.566+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:05.567+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:04.572Z",
[2024-11-12T09:49:05.567+0000] {spark_submit.py:495} INFO - "batchId" : 101,
[2024-11-12T09:49:05.567+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:05.567+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0964912280701753,
[2024-11-12T09:49:05.568+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0101010101010102,
[2024-11-12T09:49:05.568+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:05.569+0000] {spark_submit.py:495} INFO - "addBatch" : 818,
[2024-11-12T09:49:05.572+0000] {spark_submit.py:495} INFO - "commitOffsets" : 36,
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "triggerExecution" : 990,
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "walCommit" : 97
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - "0" : 687
[2024-11-12T09:49:05.573+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0964912280701753,
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0101010101010102,
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:05.574+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:05.575+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:13.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_96_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:49:13.740+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:49:13.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_95_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:49:13.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:49:13.762+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_99_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:49:13.773+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:49:13.782+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_101_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:49:13.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:49:13.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_98_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:49:13.803+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:49:13.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_100_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:49:13.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:49:13.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_97_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:49:13.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:13 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:49:15.578+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:15.579+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:15.579+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:15.579+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:15.579+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:15.570Z",
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:49:15.580+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:49:15.581+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:15.581+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:15.581+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:15.582+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:15.582+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:15.582+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:15.582+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:15.583+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:15.584+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:15.584+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:15.585+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:15.586+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:15.586+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:15.586+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:15.586+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:15.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:15.587+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:25.581+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:25.581+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:25.582+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:25.567Z",
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "triggerExecution" : 13
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:25.583+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:25.584+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:25.584+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:25.584+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:25.584+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:25.584+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:25.585+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:25.585+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:25.586+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:25.587+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:25.588+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:25.589+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:35.584+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:35.586+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:35.587+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:35.573Z",
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:35.589+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:49:35.590+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8
[2024-11-12T09:49:35.590+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:35.590+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:35.590+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:35.590+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:35.590+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:35.591+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:35.591+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:35.591+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:35.591+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:35.591+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:35.591+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:35.592+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:35.592+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:35.592+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:35.592+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:35.592+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:35.592+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:35.593+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:35.593+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:35.593+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:35.593+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:35.593+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:35.593+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:35.594+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:35.594+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:35.595+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:35.595+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:35.595+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:35.596+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:35.596+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:35.596+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:35.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:35.596+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:45.589+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:45.589+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:45.590+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:45.590+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:45.590+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:45.585Z",
[2024-11-12T09:49:45.591+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:49:45.591+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:45.592+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:45.592+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:45.593+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - "triggerExecution" : 3
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:45.596+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:45.597+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:45.597+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:45.597+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:45.598+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:45.598+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:45.599+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:45.600+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:45.600+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:45.601+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:45.602+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:45.603+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:45.603+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:45.603+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:45.603+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:45.603+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:45.604+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:55.593+0000] {spark_submit.py:495} INFO - 24/11/12 09:49:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:49:55.594+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:49:55.594+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:49:55.594+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:49:55.594+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:49:55.590Z",
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "latestOffset" : 1,
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "triggerExecution" : 2
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:55.595+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:49:55.596+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:49:55.598+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:49:55.598+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:49:55.599+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:55.599+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:55.599+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:55.599+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:49:55.600+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:49:55.601+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:49:55.602+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:49:55.602+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:49:55.602+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:49:55.602+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:05.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:50:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:50:05.593Z",
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:05.604+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:50:05.605+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:50:05.606+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:50:05.607+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:50:05.607+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:05.607+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:50:05.607+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:50:05.607+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:50:05.607+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:50:05.608+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:05.608+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:15.617+0000] {spark_submit.py:495} INFO - 24/11/12 09:50:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:50:15.618+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:50:15.618+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:50:15.618+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:50:15.618+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:50:15.607Z",
[2024-11-12T09:50:15.619+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:50:15.619+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:15.619+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:15.620+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:15.620+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:50:15.620+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:50:15.621+0000] {spark_submit.py:495} INFO - "triggerExecution" : 9
[2024-11-12T09:50:15.622+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:15.623+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:50:15.624+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:15.625+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:15.625+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:15.625+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:15.625+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:50:15.625+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:50:15.626+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:50:15.627+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:50:15.627+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:15.627+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:25.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:50:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:50:25.633+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:50:25.634+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:50:25.634+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:50:25.635+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:50:25.621Z",
[2024-11-12T09:50:25.635+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:50:25.635+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:25.636+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:25.636+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:25.636+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:50:25.636+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:50:25.636+0000] {spark_submit.py:495} INFO - "triggerExecution" : 10
[2024-11-12T09:50:25.637+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:25.637+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:50:25.637+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:50:25.637+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:50:25.638+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:50:25.638+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:25.638+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:25.638+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:25.638+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:25.639+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:50:25.639+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:25.639+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:25.639+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:25.640+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:25.640+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:50:25.640+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:25.641+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:25.641+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:25.641+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:25.642+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:25.642+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:25.642+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:25.643+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:50:25.643+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:50:25.643+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:50:25.643+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:50:25.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:25.644+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:50:25.644+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:50:25.644+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:50:25.645+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:50:25.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:25.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:35.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:50:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:50:35.634+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:50:35.635+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:50:35.635+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:50:35.635+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:50:35.626Z",
[2024-11-12T09:50:35.636+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:50:35.636+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:50:35.637+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:50:35.638+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:50:35.638+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:35.639+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:35.639+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:35.639+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:50:35.640+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:50:35.641+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:50:35.642+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:35.642+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:50:35.642+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:50:35.643+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:50:35.643+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:50:35.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:35.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:45.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:50:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:50:45.633+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:50:45.633+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:50:45.621Z",
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:50:45.634+0000] {spark_submit.py:495} INFO - "triggerExecution" : 10
[2024-11-12T09:50:45.635+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:45.635+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:50:45.635+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:45.636+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:45.637+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:45.637+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:45.638+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:45.639+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:55.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:50:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:50:55.637+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:50:55.638+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:50:55.638+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:50:55.629Z",
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:50:55.639+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:50:55.640+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:50:55.640+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:50:55.641+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:55.641+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:55.643+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:55.643+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:55.643+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:50:55.644+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:55.644+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:55.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:55.645+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:55.645+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:50:55.645+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:50:55.645+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:50:55.646+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:50:55.647+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:05.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:05.638+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:05.639+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:05.639+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:05.639+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:05.628Z",
[2024-11-12T09:51:05.640+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:51:05.640+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:05.640+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:05.640+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:05.641+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:05.641+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:51:05.641+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8
[2024-11-12T09:51:05.641+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:05.641+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:05.642+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:05.642+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:05.643+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:05.643+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:05.644+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:05.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:05.644+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:05.644+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:05.645+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:05.645+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:05.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:05.645+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:05.645+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:05.645+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:05.646+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:05.647+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:05.647+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:05.647+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:05.648+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:05.648+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:05.649+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:05.649+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:05.649+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:05.649+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:15.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:15.646+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:15.646+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:15.646+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:15.646+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:15.637Z",
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:15.647+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:15.648+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:15.649+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:25.648+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:25.649+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:25.649+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:25.650+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:25.650+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:25.638Z",
[2024-11-12T09:51:25.650+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:51:25.650+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:25.650+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:25.651+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:25.651+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:25.651+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:51:25.652+0000] {spark_submit.py:495} INFO - "triggerExecution" : 9
[2024-11-12T09:51:25.652+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:25.652+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:25.653+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:25.653+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:25.653+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:25.653+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:25.654+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:25.655+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:25.655+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:25.655+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:25.655+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:25.655+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:25.656+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:25.656+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:25.661+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:25.663+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:25.665+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:25.669+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:25.670+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:35.650+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:35.650+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:35.651+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:35.651+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:35.651+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:35.631Z",
[2024-11-12T09:51:35.651+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:51:35.651+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:35.652+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:35.652+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:35.652+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "triggerExecution" : 18
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:35.653+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:35.654+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:35.655+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:35.655+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:35.656+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:35.657+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:35.657+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:35.657+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:35.658+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:39.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/102 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.102.2c3245ff-e428-4197-896c-3a2a7312d747.tmp
[2024-11-12T09:51:39.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.102.2c3245ff-e428-4197-896c-3a2a7312d747.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/102
[2024-11-12T09:51:39.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO MicroBatchExecution: Committed offsets for batch 102. Metadata OffsetSeqMetadata(0,1731405099065,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:39.163+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:39.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:39.180+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:39.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:39.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 100, 101, 101
[2024-11-12T09:51:39.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:39.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:39.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Got job 102 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:39.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Final stage: ResultStage 102 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:39.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:39.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:39.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:39.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:51:39.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:51:39.261+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:39.262+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:39.262+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[412] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:39.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO TaskSchedulerImpl: Adding task set 102.0 with 1 tasks resource profile 0
[2024-11-12T09:51:39.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 102) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:39.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:39.915+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 102) in 649 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:39.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool
[2024-11-12T09:51:39.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: ResultStage 102 (start at NativeMethodAccessorImpl.java:0) finished in 0.675 s
[2024-11-12T09:51:39.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:39.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished
[2024-11-12T09:51:39.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO DAGScheduler: Job 102 finished: start at NativeMethodAccessorImpl.java:0, took 0.683060 s
[2024-11-12T09:51:39.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO FileFormatWriter: Start to commit write Job e410f7bc-1721-4f72-bc9d-607c227d2189.
[2024-11-12T09:51:39.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/102 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.102.d2a7a574-ffee-4d6a-9078-20d061692fa6.tmp
[2024-11-12T09:51:39.963+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.102.d2a7a574-ffee-4d6a-9078-20d061692fa6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/102
[2024-11-12T09:51:39.963+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO ManifestFileCommitProtocol: Committed batch 102
[2024-11-12T09:51:39.964+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO FileFormatWriter: Write Job e410f7bc-1721-4f72-bc9d-607c227d2189 committed. Elapsed time: 46 ms.
[2024-11-12T09:51:39.964+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO FileFormatWriter: Finished processing stats for write job e410f7bc-1721-4f72-bc9d-607c227d2189.
[2024-11-12T09:51:39.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/102 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.102.903af2b9-40cc-4f3e-b5f0-7292d404c8cd.tmp
[2024-11-12T09:51:40.031+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.102.903af2b9-40cc-4f3e-b5f0-7292d404c8cd.tmp to hdfs://namenode:9000/spark_checkpoint/commits/102
[2024-11-12T09:51:40.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:40.041+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:40.041+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:40.041+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:39.061Z",
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "batchId" : 102,
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0309278350515465,
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "addBatch" : 794,
[2024-11-12T09:51:40.042+0000] {spark_submit.py:495} INFO - "commitOffsets" : 67,
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "queryPlanning" : 23,
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "triggerExecution" : 970,
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "walCommit" : 79
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:40.043+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:40.044+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:40.045+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:40.045+0000] {spark_submit.py:495} INFO - "0" : 688
[2024-11-12T09:51:40.045+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:40.046+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:40.046+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:40.046+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:40.046+0000] {spark_submit.py:495} INFO - "0" : 689
[2024-11-12T09:51:40.046+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:40.047+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:40.047+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:40.048+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:40.048+0000] {spark_submit.py:495} INFO - "0" : 689
[2024-11-12T09:51:40.049+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:40.049+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:40.050+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:40.050+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-11-12T09:51:40.050+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0309278350515465,
[2024-11-12T09:51:40.051+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:40.052+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:40.052+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:40.052+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:40.053+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:40.053+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:40.053+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:40.054+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:40.054+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:40.054+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:40.055+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:40.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/103 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.103.70538011-56c9-46f4-b265-69f497072fd5.tmp
[2024-11-12T09:51:40.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.103.70538011-56c9-46f4-b265-69f497072fd5.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/103
[2024-11-12T09:51:40.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO MicroBatchExecution: Committed offsets for batch 103. Metadata OffsetSeqMetadata(0,1731405100080,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:40.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:40.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:40.154+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:40.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:40.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 101, 102, 102
[2024-11-12T09:51:40.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:40.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:40.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Got job 103 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:40.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Final stage: ResultStage 103 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:40.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:40.207+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:40.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[416] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:40.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:51:40.231+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:51:40.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:40.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:40.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[416] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:40.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks resource profile 0
[2024-11-12T09:51:40.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 103) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:40.256+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:40.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 103) in 628 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:40.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool
[2024-11-12T09:51:40.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: ResultStage 103 (start at NativeMethodAccessorImpl.java:0) finished in 0.659 s
[2024-11-12T09:51:40.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:40.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 103: Stage finished
[2024-11-12T09:51:40.867+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO DAGScheduler: Job 103 finished: start at NativeMethodAccessorImpl.java:0, took 0.661189 s
[2024-11-12T09:51:40.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO FileFormatWriter: Start to commit write Job 1f280e00-1310-400d-aa24-33195e674f81.
[2024-11-12T09:51:40.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/103 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.103.60c89fc9-c74e-4d89-9cf9-0e7ac609dc5a.tmp
[2024-11-12T09:51:40.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.103.60c89fc9-c74e-4d89-9cf9-0e7ac609dc5a.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/103
[2024-11-12T09:51:40.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO ManifestFileCommitProtocol: Committed batch 103
[2024-11-12T09:51:40.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO FileFormatWriter: Write Job 1f280e00-1310-400d-aa24-33195e674f81 committed. Elapsed time: 53 ms.
[2024-11-12T09:51:40.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO FileFormatWriter: Finished processing stats for write job 1f280e00-1310-400d-aa24-33195e674f81.
[2024-11-12T09:51:40.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/103 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.103.1d96ddf7-a75c-4daa-9cc9-96b18865337e.tmp
[2024-11-12T09:51:41.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.103.1d96ddf7-a75c-4daa-9cc9-96b18865337e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/103
[2024-11-12T09:51:41.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:41.388+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:41.391+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:41.391+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:41.392+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:40.074Z",
[2024-11-12T09:51:41.393+0000] {spark_submit.py:495} INFO - "batchId" : 103,
[2024-11-12T09:51:41.393+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:41.393+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 62.5,
[2024-11-12T09:51:41.393+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7692307692307692,
[2024-11-12T09:51:41.394+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:41.394+0000] {spark_submit.py:495} INFO - "addBatch" : 776,
[2024-11-12T09:51:41.394+0000] {spark_submit.py:495} INFO - "commitOffsets" : 451,
[2024-11-12T09:51:41.394+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:51:41.394+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:51:41.394+0000] {spark_submit.py:495} INFO - "queryPlanning" : 19,
[2024-11-12T09:51:41.395+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1300,
[2024-11-12T09:51:41.395+0000] {spark_submit.py:495} INFO - "walCommit" : 42
[2024-11-12T09:51:41.395+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:41.397+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:41.397+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:41.397+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:41.398+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:41.399+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:41.400+0000] {spark_submit.py:495} INFO - "0" : 689
[2024-11-12T09:51:41.400+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:41.401+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:41.401+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:41.401+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:41.402+0000] {spark_submit.py:495} INFO - "0" : 690
[2024-11-12T09:51:41.402+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:41.402+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:41.402+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:41.403+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:41.403+0000] {spark_submit.py:495} INFO - "0" : 690
[2024-11-12T09:51:41.405+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:41.407+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 62.5,
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7692307692307692,
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:41.408+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:41.409+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:41.409+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:41.409+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:41.409+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:41.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/104 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.104.09c72aa8-3b58-4165-adb9-135ac35659f8.tmp
[2024-11-12T09:51:41.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.104.09c72aa8-3b58-4165-adb9-135ac35659f8.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/104
[2024-11-12T09:51:41.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO MicroBatchExecution: Committed offsets for batch 104. Metadata OffsetSeqMetadata(0,1731405101393,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:41.512+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:41.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:41.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:41.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:41.558+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 102, 103, 103
[2024-11-12T09:51:41.564+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:41.602+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:41.603+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO DAGScheduler: Got job 104 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:41.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO DAGScheduler: Final stage: ResultStage 104 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:41.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:41.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:41.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[420] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:41.628+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:51:41.635+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:51:41.635+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:41.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:41.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[420] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:41.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks resource profile 0
[2024-11-12T09:51:41.637+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 104) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:41.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:41 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:42.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 104) in 498 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:42.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool
[2024-11-12T09:51:42.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: ResultStage 104 (start at NativeMethodAccessorImpl.java:0) finished in 0.532 s
[2024-11-12T09:51:42.138+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:42.138+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
[2024-11-12T09:51:42.138+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Job 104 finished: start at NativeMethodAccessorImpl.java:0, took 0.535736 s
[2024-11-12T09:51:42.138+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO FileFormatWriter: Start to commit write Job bea65ad7-ce45-47d6-8c84-99d0f96de0be.
[2024-11-12T09:51:42.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/104 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.104.2e68accb-7dc9-4e99-8619-67c29c62f220.tmp
[2024-11-12T09:51:42.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.104.2e68accb-7dc9-4e99-8619-67c29c62f220.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/104
[2024-11-12T09:51:42.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO ManifestFileCommitProtocol: Committed batch 104
[2024-11-12T09:51:42.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO FileFormatWriter: Write Job bea65ad7-ce45-47d6-8c84-99d0f96de0be committed. Elapsed time: 38 ms.
[2024-11-12T09:51:42.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO FileFormatWriter: Finished processing stats for write job bea65ad7-ce45-47d6-8c84-99d0f96de0be.
[2024-11-12T09:51:42.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/104 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.104.b89ef358-ecb7-4be0-bef8-0fd6775cdd21.tmp
[2024-11-12T09:51:42.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.104.b89ef358-ecb7-4be0-bef8-0fd6775cdd21.tmp to hdfs://namenode:9000/spark_checkpoint/commits/104
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:41.377Z",
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "batchId" : 104,
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7674597083653109,
[2024-11-12T09:51:42.227+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.179245283018868,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "addBatch" : 661,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "commitOffsets" : 48,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "queryPlanning" : 36,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "triggerExecution" : 848,
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "walCommit" : 82
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:42.228+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:42.229+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:42.229+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:42.229+0000] {spark_submit.py:495} INFO - "0" : 690
[2024-11-12T09:51:42.229+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:42.229+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:42.229+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "0" : 691
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "0" : 691
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7674597083653109,
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.179245283018868,
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:42.230+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:42.231+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:42.231+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:42.231+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:42.232+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:42.233+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:42.233+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:42.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/105 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.105.0f724b95-7408-408d-98cb-dc4d18da54ac.tmp
[2024-11-12T09:51:42.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.105.0f724b95-7408-408d-98cb-dc4d18da54ac.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/105
[2024-11-12T09:51:42.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO MicroBatchExecution: Committed offsets for batch 105. Metadata OffsetSeqMetadata(0,1731405102233,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:42.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:42.309+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:42.324+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:42.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:42.331+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 103, 104, 104
[2024-11-12T09:51:42.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:42.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:42.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Got job 105 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:42.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Final stage: ResultStage 105 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:42.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:42.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:42.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[424] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:42.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:51:42.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:51:42.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:42.414+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:42.414+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[424] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:42.415+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks resource profile 0
[2024-11-12T09:51:42.418+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 105) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:42.442+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:42 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:43.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 105) in 618 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:43.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool
[2024-11-12T09:51:43.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: ResultStage 105 (start at NativeMethodAccessorImpl.java:0) finished in 0.662 s
[2024-11-12T09:51:43.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:43.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished
[2024-11-12T09:51:43.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Job 105 finished: start at NativeMethodAccessorImpl.java:0, took 0.665188 s
[2024-11-12T09:51:43.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO FileFormatWriter: Start to commit write Job fc7ff085-46ae-4fcb-b5e5-57ef1df310c7.
[2024-11-12T09:51:43.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/105 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.105.cdbb13cc-89e4-47e3-b7ad-0cc52ee9c885.tmp
[2024-11-12T09:51:43.479+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.105.cdbb13cc-89e4-47e3-b7ad-0cc52ee9c885.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/105
[2024-11-12T09:51:43.480+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO ManifestFileCommitProtocol: Committed batch 105
[2024-11-12T09:51:43.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO FileFormatWriter: Write Job fc7ff085-46ae-4fcb-b5e5-57ef1df310c7 committed. Elapsed time: 444 ms.
[2024-11-12T09:51:43.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO FileFormatWriter: Finished processing stats for write job fc7ff085-46ae-4fcb-b5e5-57ef1df310c7.
[2024-11-12T09:51:43.492+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/105 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.105.67eb8ce2-e46f-48f4-b6b7-1b221c67d49f.tmp
[2024-11-12T09:51:43.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.105.67eb8ce2-e46f-48f4-b6b7-1b221c67d49f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/105
[2024-11-12T09:51:43.548+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:42.227Z",
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "batchId" : 105,
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1764705882352942,
[2024-11-12T09:51:43.549+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7581501137225171,
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "addBatch" : 1166,
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "commitOffsets" : 66,
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "queryPlanning" : 25,
[2024-11-12T09:51:43.550+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1319,
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:43.551+0000] {spark_submit.py:495} INFO - "0" : 691
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - "0" : 692
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - "0" : 692
[2024-11-12T09:51:43.552+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:43.553+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:43.553+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:43.553+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1764705882352942,
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7581501137225171,
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:43.554+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:43.555+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:43.555+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:43.555+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:43.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/106 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.106.74240d2b-f765-4ed0-a15e-b3d166899865.tmp
[2024-11-12T09:51:43.615+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.106.74240d2b-f765-4ed0-a15e-b3d166899865.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/106
[2024-11-12T09:51:43.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO MicroBatchExecution: Committed offsets for batch 106. Metadata OffsetSeqMetadata(0,1731405103569,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:43.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:43.639+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:43.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:43.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:43.671+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 104, 105, 105
[2024-11-12T09:51:43.673+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:43.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:43.712+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Got job 106 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:43.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Final stage: ResultStage 106 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:43.716+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:43.716+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:43.716+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:43.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:51:43.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_102_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:43.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:51:43.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:43.759+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:43.760+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[428] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:43.764+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks resource profile 0
[2024-11-12T09:51:43.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:43.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 106) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:43.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_103_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:43.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_103_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:43.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:43.817+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_104_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:43.827+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:43.842+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_105_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:43.850+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:43 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:44.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 106) in 378 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:44.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool
[2024-11-12T09:51:44.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: ResultStage 106 (start at NativeMethodAccessorImpl.java:0) finished in 0.430 s
[2024-11-12T09:51:44.159+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:44.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished
[2024-11-12T09:51:44.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Job 106 finished: start at NativeMethodAccessorImpl.java:0, took 0.437355 s
[2024-11-12T09:51:44.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO FileFormatWriter: Start to commit write Job 0a4cce92-f6c8-4114-bbf9-5639887a0843.
[2024-11-12T09:51:44.163+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/106 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.106.c307061b-771f-41e4-9e24-61544540f590.tmp
[2024-11-12T09:51:44.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.106.c307061b-771f-41e4-9e24-61544540f590.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/106
[2024-11-12T09:51:44.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO ManifestFileCommitProtocol: Committed batch 106
[2024-11-12T09:51:44.196+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO FileFormatWriter: Write Job 0a4cce92-f6c8-4114-bbf9-5639887a0843 committed. Elapsed time: 47 ms.
[2024-11-12T09:51:44.197+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO FileFormatWriter: Finished processing stats for write job 0a4cce92-f6c8-4114-bbf9-5639887a0843.
[2024-11-12T09:51:44.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/106 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.106.c34b1ac8-358f-4251-b1c9-f7b25e634216.tmp
[2024-11-12T09:51:44.231+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.106.c34b1ac8-358f-4251-b1c9-f7b25e634216.tmp to hdfs://namenode:9000/spark_checkpoint/commits/106
[2024-11-12T09:51:44.232+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:44.234+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:43.548Z",
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "batchId" : 106,
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.757002271006813,
[2024-11-12T09:51:44.235+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4641288433382136,
[2024-11-12T09:51:44.236+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:44.236+0000] {spark_submit.py:495} INFO - "addBatch" : 553,
[2024-11-12T09:51:44.236+0000] {spark_submit.py:495} INFO - "commitOffsets" : 37,
[2024-11-12T09:51:44.236+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:44.236+0000] {spark_submit.py:495} INFO - "latestOffset" : 21,
[2024-11-12T09:51:44.236+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:51:44.244+0000] {spark_submit.py:495} INFO - "triggerExecution" : 683,
[2024-11-12T09:51:44.247+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:51:44.249+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:44.249+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:44.249+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:44.250+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:44.250+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:44.250+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - "0" : 692
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - "0" : 693
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:44.251+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - "0" : 693
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.757002271006813,
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4641288433382136,
[2024-11-12T09:51:44.252+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:44.253+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:44.253+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:44.253+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:44.253+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:44.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/107 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.107.49a13d4a-6367-42a5-b040-be4f1ea2368e.tmp
[2024-11-12T09:51:44.296+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.107.49a13d4a-6367-42a5-b040-be4f1ea2368e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/107
[2024-11-12T09:51:44.296+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO MicroBatchExecution: Committed offsets for batch 107. Metadata OffsetSeqMetadata(0,1731405104249,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:44.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:44.314+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:44.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:44.329+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:44.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 105, 106, 106
[2024-11-12T09:51:44.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:44.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:44.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Got job 107 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:44.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Final stage: ResultStage 107 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:44.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:44.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:44.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[432] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:44.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:51:44.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:51:44.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:44.412+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:44.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[432] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:44.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks resource profile 0
[2024-11-12T09:51:44.415+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 107) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:44.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:44 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:45.065+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 107) in 650 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:45.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool
[2024-11-12T09:51:45.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: ResultStage 107 (start at NativeMethodAccessorImpl.java:0) finished in 0.684 s
[2024-11-12T09:51:45.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:45.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
[2024-11-12T09:51:45.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Job 107 finished: start at NativeMethodAccessorImpl.java:0, took 0.687856 s
[2024-11-12T09:51:45.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO FileFormatWriter: Start to commit write Job 85b436b9-8722-49a5-96d4-1902aba1277c.
[2024-11-12T09:51:45.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/107 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.107.fd5946d6-7c67-44a4-8ef0-66d3cb9c349b.tmp
[2024-11-12T09:51:45.124+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.107.fd5946d6-7c67-44a4-8ef0-66d3cb9c349b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/107
[2024-11-12T09:51:45.126+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO ManifestFileCommitProtocol: Committed batch 107
[2024-11-12T09:51:45.126+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO FileFormatWriter: Write Job 85b436b9-8722-49a5-96d4-1902aba1277c committed. Elapsed time: 56 ms.
[2024-11-12T09:51:45.126+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO FileFormatWriter: Finished processing stats for write job 85b436b9-8722-49a5-96d4-1902aba1277c.
[2024-11-12T09:51:45.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/107 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.107.9bb1c50d-b707-4661-9bde-74b009dbaea1.tmp
[2024-11-12T09:51:45.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.107.9bb1c50d-b707-4661-9bde-74b009dbaea1.tmp to hdfs://namenode:9000/spark_checkpoint/commits/107
[2024-11-12T09:51:45.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:45.171+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:45.171+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:45.172+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:45.172+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:44.232Z",
[2024-11-12T09:51:45.172+0000] {spark_submit.py:495} INFO - "batchId" : 107,
[2024-11-12T09:51:45.173+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:45.173+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.461988304093567,
[2024-11-12T09:51:45.173+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0672358591248665,
[2024-11-12T09:51:45.173+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:45.174+0000] {spark_submit.py:495} INFO - "addBatch" : 807,
[2024-11-12T09:51:45.174+0000] {spark_submit.py:495} INFO - "commitOffsets" : 43,
[2024-11-12T09:51:45.174+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:45.174+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:51:45.174+0000] {spark_submit.py:495} INFO - "queryPlanning" : 20,
[2024-11-12T09:51:45.174+0000] {spark_submit.py:495} INFO - "triggerExecution" : 937,
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "0" : 693
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:45.175+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "0" : 694
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "0" : 694
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:45.176+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.461988304093567,
[2024-11-12T09:51:45.177+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0672358591248665,
[2024-11-12T09:51:45.177+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:45.177+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:45.177+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:45.177+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:45.178+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:45.178+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:45.178+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:45.178+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:45.178+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:45.179+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:45.179+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:45.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/108 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.108.9cc345f7-fdb9-4631-bf46-9ab5add2a18b.tmp
[2024-11-12T09:51:45.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.108.9cc345f7-fdb9-4631-bf46-9ab5add2a18b.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/108
[2024-11-12T09:51:45.229+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO MicroBatchExecution: Committed offsets for batch 108. Metadata OffsetSeqMetadata(0,1731405105175,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:45.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:45.257+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:45.273+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:45.274+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:45.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 106, 107, 107
[2024-11-12T09:51:45.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:45.323+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:45.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Got job 108 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:45.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Final stage: ResultStage 108 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:45.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:45.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:45.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[436] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:45.343+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:51:45.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:51:45.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:45.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:45.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[436] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:45.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks resource profile 0
[2024-11-12T09:51:45.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 108) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:45.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:45.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 108) in 633 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:45.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool
[2024-11-12T09:51:45.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: ResultStage 108 (start at NativeMethodAccessorImpl.java:0) finished in 0.657 s
[2024-11-12T09:51:45.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:45.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
[2024-11-12T09:51:45.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO DAGScheduler: Job 108 finished: start at NativeMethodAccessorImpl.java:0, took 0.659629 s
[2024-11-12T09:51:45.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO FileFormatWriter: Start to commit write Job 4c4497dc-854d-4e6a-80b0-ce53526b7238.
[2024-11-12T09:51:45.989+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/108 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.108.0d087a86-6855-404e-a2d7-b340eca3e7cb.tmp
[2024-11-12T09:51:46.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.108.0d087a86-6855-404e-a2d7-b340eca3e7cb.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/108
[2024-11-12T09:51:46.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO ManifestFileCommitProtocol: Committed batch 108
[2024-11-12T09:51:46.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO FileFormatWriter: Write Job 4c4497dc-854d-4e6a-80b0-ce53526b7238 committed. Elapsed time: 50 ms.
[2024-11-12T09:51:46.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO FileFormatWriter: Finished processing stats for write job 4c4497dc-854d-4e6a-80b0-ce53526b7238.
[2024-11-12T09:51:46.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/108 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.108.33a72d66-bf94-4a41-aea4-f15b94ff13ec.tmp
[2024-11-12T09:51:46.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.108.33a72d66-bf94-4a41-aea4-f15b94ff13ec.tmp to hdfs://namenode:9000/spark_checkpoint/commits/108
[2024-11-12T09:51:46.084+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:46.084+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:46.084+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:46.085+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:46.086+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:45.170Z",
[2024-11-12T09:51:46.086+0000] {spark_submit.py:495} INFO - "batchId" : 108,
[2024-11-12T09:51:46.087+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:46.088+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0660980810234542,
[2024-11-12T09:51:46.088+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0964912280701753,
[2024-11-12T09:51:46.088+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:46.089+0000] {spark_submit.py:495} INFO - "addBatch" : 773,
[2024-11-12T09:51:46.089+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:51:46.089+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:46.089+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:51:46.089+0000] {spark_submit.py:495} INFO - "queryPlanning" : 30,
[2024-11-12T09:51:46.090+0000] {spark_submit.py:495} INFO - "triggerExecution" : 912,
[2024-11-12T09:51:46.090+0000] {spark_submit.py:495} INFO - "walCommit" : 52
[2024-11-12T09:51:46.090+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:46.090+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:46.090+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:46.091+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:46.091+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:46.091+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:46.091+0000] {spark_submit.py:495} INFO - "0" : 694
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - "0" : 695
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:46.092+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:46.093+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:46.094+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:46.094+0000] {spark_submit.py:495} INFO - "0" : 695
[2024-11-12T09:51:46.094+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:46.094+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:46.094+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0660980810234542,
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0964912280701753,
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:46.095+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:46.096+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:46.097+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:46.099+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:46.099+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:46.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/109 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.109.d9a077aa-be33-46bf-b162-59d5780ee794.tmp
[2024-11-12T09:51:46.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.109.d9a077aa-be33-46bf-b162-59d5780ee794.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/109
[2024-11-12T09:51:46.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO MicroBatchExecution: Committed offsets for batch 109. Metadata OffsetSeqMetadata(0,1731405106137,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:46.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:46.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:46.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:46.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:46.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 108
[2024-11-12T09:51:46.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:46.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:46.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Got job 109 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:46.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Final stage: ResultStage 109 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:46.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:46.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:46.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Submitting ResultStage 109 (MapPartitionsRDD[440] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:46.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:51:46.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:51:46.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:46.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:46.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[440] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:46.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks resource profile 0
[2024-11-12T09:51:46.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 109) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:46.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:46.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 109) in 617 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:46.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool
[2024-11-12T09:51:46.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: ResultStage 109 (start at NativeMethodAccessorImpl.java:0) finished in 0.651 s
[2024-11-12T09:51:46.899+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:46.899+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 109: Stage finished
[2024-11-12T09:51:46.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO DAGScheduler: Job 109 finished: start at NativeMethodAccessorImpl.java:0, took 0.654577 s
[2024-11-12T09:51:46.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO FileFormatWriter: Start to commit write Job df369616-63a9-4a65-a9ad-2ad9dea4f0d9.
[2024-11-12T09:51:46.904+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/109.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.109.compact.c056f011-7798-431c-8c77-b476df61235e.tmp
[2024-11-12T09:51:47.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.109.compact.c056f011-7798-431c-8c77-b476df61235e.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/109.compact
[2024-11-12T09:51:47.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO FileStreamSinkLog: Current compact batch id = 109 min compaction batch id to delete = 9
[2024-11-12T09:51:47.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO ManifestFileCommitProtocol: Committed batch 109
[2024-11-12T09:51:47.096+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO FileFormatWriter: Write Job df369616-63a9-4a65-a9ad-2ad9dea4f0d9 committed. Elapsed time: 203 ms.
[2024-11-12T09:51:47.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO FileFormatWriter: Finished processing stats for write job df369616-63a9-4a65-a9ad-2ad9dea4f0d9.
[2024-11-12T09:51:47.111+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/109 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.109.22ada73a-4be0-49ff-a1b6-a3d3bf0edcb1.tmp
[2024-11-12T09:51:47.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.109.22ada73a-4be0-49ff-a1b6-a3d3bf0edcb1.tmp to hdfs://namenode:9000/spark_checkpoint/commits/109
[2024-11-12T09:51:47.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:47.175+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:47.175+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:47.176+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:47.176+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:46.126Z",
[2024-11-12T09:51:47.176+0000] {spark_submit.py:495} INFO - "batchId" : 109,
[2024-11-12T09:51:47.176+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:47.177+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-11-12T09:51:47.177+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9551098376313276,
[2024-11-12T09:51:47.177+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:47.177+0000] {spark_submit.py:495} INFO - "addBatch" : 917,
[2024-11-12T09:51:47.183+0000] {spark_submit.py:495} INFO - "commitOffsets" : 76,
[2024-11-12T09:51:47.184+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:51:47.184+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - "queryPlanning" : 10,
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1047,
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - "walCommit" : 30
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:47.185+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:47.186+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:47.186+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:47.186+0000] {spark_submit.py:495} INFO - "0" : 695
[2024-11-12T09:51:47.186+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:47.186+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:47.186+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - "0" : 696
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - "0" : 696
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:47.187+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9551098376313276,
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:47.188+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:47.189+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:47.189+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:47.189+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:47.189+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:47.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/110 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.110.3682bb3a-8357-40cf-a7bb-3eb1098de34a.tmp
[2024-11-12T09:51:47.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.110.3682bb3a-8357-40cf-a7bb-3eb1098de34a.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/110
[2024-11-12T09:51:47.256+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO MicroBatchExecution: Committed offsets for batch 110. Metadata OffsetSeqMetadata(0,1731405107186,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:47.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:47.296+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:47.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:47.322+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:47.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 108, 109
[2024-11-12T09:51:47.335+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:47.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:47.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO DAGScheduler: Got job 110 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:47.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO DAGScheduler: Final stage: ResultStage 110 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:47.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:47.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:47.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[444] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:47.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:51:47.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:51:47.424+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:47.425+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:47.426+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_107_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:47.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:47.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[444] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:47.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO TaskSchedulerImpl: Adding task set 110.0 with 1 tasks resource profile 0
[2024-11-12T09:51:47.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 110) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:47.456+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_109_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:47.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_109_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:47.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_108_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:47.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:47.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:47.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_106_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:47.498+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:47 INFO BlockManagerInfo: Removed broadcast_106_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:48.092+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 110) in 658 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:48.093+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool
[2024-11-12T09:51:48.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: ResultStage 110 (start at NativeMethodAccessorImpl.java:0) finished in 0.720 s
[2024-11-12T09:51:48.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Job 110 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:48.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished
[2024-11-12T09:51:48.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Job 110 finished: start at NativeMethodAccessorImpl.java:0, took 0.728955 s
[2024-11-12T09:51:48.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO FileFormatWriter: Start to commit write Job c72ba651-910b-43f0-94b7-d95241c2c9d1.
[2024-11-12T09:51:48.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/110 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.110.ffbe43bf-a011-4b54-98b5-dc0ce55a0e32.tmp
[2024-11-12T09:51:48.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.110.ffbe43bf-a011-4b54-98b5-dc0ce55a0e32.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/110
[2024-11-12T09:51:48.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO FileStreamSinkLog: Current compact batch id = 110 min compaction batch id to delete = 9
[2024-11-12T09:51:48.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO ManifestFileCommitProtocol: Committed batch 110
[2024-11-12T09:51:48.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO FileFormatWriter: Write Job c72ba651-910b-43f0-94b7-d95241c2c9d1 committed. Elapsed time: 49 ms.
[2024-11-12T09:51:48.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO FileFormatWriter: Finished processing stats for write job c72ba651-910b-43f0-94b7-d95241c2c9d1.
[2024-11-12T09:51:48.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/110 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.110.97c277d2-3808-4cef-b853-21c0ef38740b.tmp
[2024-11-12T09:51:48.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.110.97c277d2-3808-4cef-b853-21c0ef38740b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/110
[2024-11-12T09:51:48.201+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:48.203+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:48.203+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:48.203+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:48.203+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:47.174Z",
[2024-11-12T09:51:48.203+0000] {spark_submit.py:495} INFO - "batchId" : 110,
[2024-11-12T09:51:48.203+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:48.204+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9541984732824427,
[2024-11-12T09:51:48.204+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9765625,
[2024-11-12T09:51:48.204+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "addBatch" : 849,
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "queryPlanning" : 42,
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1024,
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "walCommit" : 69
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:48.205+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "0" : 696
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:48.206+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - "0" : 697
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - "0" : 697
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:48.207+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9541984732824427,
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9765625,
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:48.208+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:48.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/111 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.111.6fe297d4-93f1-4f73-a459-c8605f088315.tmp
[2024-11-12T09:51:48.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.111.6fe297d4-93f1-4f73-a459-c8605f088315.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/111
[2024-11-12T09:51:48.249+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO MicroBatchExecution: Committed offsets for batch 111. Metadata OffsetSeqMetadata(0,1731405108206,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:48.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:48.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:48.281+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:48.282+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:48.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 110
[2024-11-12T09:51:48.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:48.331+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:48.333+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Got job 111 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:48.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Final stage: ResultStage 111 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:48.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:48.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:48.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[448] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:48.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:51:48.372+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:51:48.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:48.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:48.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[448] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:48.374+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks resource profile 0
[2024-11-12T09:51:48.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 111) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:48.402+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:48 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:49.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 111) in 1071 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:49.457+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool
[2024-11-12T09:51:49.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: ResultStage 111 (start at NativeMethodAccessorImpl.java:0) finished in 1.113 s
[2024-11-12T09:51:49.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Job 111 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:49.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
[2024-11-12T09:51:49.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Job 111 finished: start at NativeMethodAccessorImpl.java:0, took 1.122819 s
[2024-11-12T09:51:49.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO FileFormatWriter: Start to commit write Job adee4b8a-bda2-4041-a777-71e71a2ebc83.
[2024-11-12T09:51:49.465+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/111 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.111.a9abbf4f-fd2c-4942-a49a-6eeaa0eb3a3c.tmp
[2024-11-12T09:51:49.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.111.a9abbf4f-fd2c-4942-a49a-6eeaa0eb3a3c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/111
[2024-11-12T09:51:49.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO FileStreamSinkLog: Current compact batch id = 111 min compaction batch id to delete = 9
[2024-11-12T09:51:49.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO ManifestFileCommitProtocol: Committed batch 111
[2024-11-12T09:51:49.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO FileFormatWriter: Write Job adee4b8a-bda2-4041-a777-71e71a2ebc83 committed. Elapsed time: 49 ms.
[2024-11-12T09:51:49.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO FileFormatWriter: Finished processing stats for write job adee4b8a-bda2-4041-a777-71e71a2ebc83.
[2024-11-12T09:51:49.523+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/111 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.111.44da59e4-2080-4266-a9f1-a50f4bc6623a.tmp
[2024-11-12T09:51:49.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.111.44da59e4-2080-4266-a9f1-a50f4bc6623a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/111
[2024-11-12T09:51:49.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:49.564+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:49.564+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:49.564+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:48.201Z",
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "batchId" : 111,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9737098344693282,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7347538574577517,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "addBatch" : 1237,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "commitOffsets" : 54,
[2024-11-12T09:51:49.565+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:49.566+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:51:49.566+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:51:49.566+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1361,
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - "walCommit" : 39
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:49.567+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:49.568+0000] {spark_submit.py:495} INFO - "0" : 697
[2024-11-12T09:51:49.568+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:49.568+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:49.568+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:49.568+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:49.569+0000] {spark_submit.py:495} INFO - "0" : 698
[2024-11-12T09:51:49.569+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:49.569+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:49.569+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:49.569+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:49.570+0000] {spark_submit.py:495} INFO - "0" : 698
[2024-11-12T09:51:49.570+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:49.570+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:49.570+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:49.570+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9737098344693282,
[2024-11-12T09:51:49.571+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7347538574577517,
[2024-11-12T09:51:49.571+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:49.571+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:49.572+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:49.573+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:49.576+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/112 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.112.c4c6ad54-b54e-4b23-ba79-7b676e9afa3c.tmp
[2024-11-12T09:51:49.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.112.c4c6ad54-b54e-4b23-ba79-7b676e9afa3c.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/112
[2024-11-12T09:51:49.627+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO MicroBatchExecution: Committed offsets for batch 112. Metadata OffsetSeqMetadata(0,1731405109569,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:49.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:49.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:49.657+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:49.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:49.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 110, 111, 111
[2024-11-12T09:51:49.666+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:49.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:49.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Got job 112 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:49.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Final stage: ResultStage 112 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:49.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:49.697+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:49.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[452] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:49.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:51:49.724+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:51:49.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:49.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:49.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[452] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:49.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks resource profile 0
[2024-11-12T09:51:49.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 112) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:49.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:49 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:50.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 112) in 476 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:50.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool
[2024-11-12T09:51:50.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: ResultStage 112 (start at NativeMethodAccessorImpl.java:0) finished in 0.507 s
[2024-11-12T09:51:50.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:50.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished
[2024-11-12T09:51:50.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Job 112 finished: start at NativeMethodAccessorImpl.java:0, took 0.511188 s
[2024-11-12T09:51:50.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO FileFormatWriter: Start to commit write Job 7fd1beff-ebe4-443a-910a-e93b5faf1971.
[2024-11-12T09:51:50.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/112 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.112.0f66b2d9-e202-4bd8-9aef-2430c9460124.tmp
[2024-11-12T09:51:50.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.112.0f66b2d9-e202-4bd8-9aef-2430c9460124.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/112
[2024-11-12T09:51:50.246+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO FileStreamSinkLog: Current compact batch id = 112 min compaction batch id to delete = 9
[2024-11-12T09:51:50.247+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO ManifestFileCommitProtocol: Committed batch 112
[2024-11-12T09:51:50.248+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO FileFormatWriter: Write Job 7fd1beff-ebe4-443a-910a-e93b5faf1971 committed. Elapsed time: 41 ms.
[2024-11-12T09:51:50.248+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO FileFormatWriter: Finished processing stats for write job 7fd1beff-ebe4-443a-910a-e93b5faf1971.
[2024-11-12T09:51:50.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/112 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.112.6b45452c-9cab-45de-9877-9064261512ca.tmp
[2024-11-12T09:51:50.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.112.6b45452c-9cab-45de-9877-9064261512ca.tmp to hdfs://namenode:9000/spark_checkpoint/commits/112
[2024-11-12T09:51:50.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:50.309+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:50.309+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:50.309+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:50.309+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:49.563Z",
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "batchId" : 112,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7342143906020557,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3440860215053763,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "addBatch" : 605,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:51:50.310+0000] {spark_submit.py:495} INFO - "queryPlanning" : 14,
[2024-11-12T09:51:50.311+0000] {spark_submit.py:495} INFO - "triggerExecution" : 744,
[2024-11-12T09:51:50.311+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:51:50.311+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:50.311+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:50.312+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:50.312+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:50.312+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - "0" : 698
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - "0" : 699
[2024-11-12T09:51:50.313+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:50.314+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:50.314+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:50.314+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:50.314+0000] {spark_submit.py:495} INFO - "0" : 699
[2024-11-12T09:51:50.314+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:50.314+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:50.315+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:50.315+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7342143906020557,
[2024-11-12T09:51:50.315+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3440860215053763,
[2024-11-12T09:51:50.315+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:50.315+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:50.315+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:50.316+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:50.316+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:50.316+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:50.317+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:50.317+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:50.317+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:50.317+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:50.317+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:50.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/113 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.113.1488202b-b03c-4227-a4c9-3c2ae48dc767.tmp
[2024-11-12T09:51:50.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.113.1488202b-b03c-4227-a4c9-3c2ae48dc767.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/113
[2024-11-12T09:51:50.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO MicroBatchExecution: Committed offsets for batch 113. Metadata OffsetSeqMetadata(0,1731405110312,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:50.382+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:50.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:50.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:50.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:50.413+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 111, 112, 112
[2024-11-12T09:51:50.415+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:50.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:50.453+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Got job 113 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:50.453+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Final stage: ResultStage 113 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:50.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:50.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:50.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:50.479+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:51:50.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:51:50.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:50.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:50.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[456] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:50.483+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks resource profile 0
[2024-11-12T09:51:50.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 113) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:50.507+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:50 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:51.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 113) in 618 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:51.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool
[2024-11-12T09:51:51.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: ResultStage 113 (start at NativeMethodAccessorImpl.java:0) finished in 0.647 s
[2024-11-12T09:51:51.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:51.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
[2024-11-12T09:51:51.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Job 113 finished: start at NativeMethodAccessorImpl.java:0, took 0.651309 s
[2024-11-12T09:51:51.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO FileFormatWriter: Start to commit write Job e11c227f-a654-4efc-9e32-db60edd56acf.
[2024-11-12T09:51:51.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/113 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.113.31ebe6b3-63f1-4b65-94e0-885e63d86260.tmp
[2024-11-12T09:51:51.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.113.31ebe6b3-63f1-4b65-94e0-885e63d86260.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/113
[2024-11-12T09:51:51.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO FileStreamSinkLog: Current compact batch id = 113 min compaction batch id to delete = 9
[2024-11-12T09:51:51.147+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO ManifestFileCommitProtocol: Committed batch 113
[2024-11-12T09:51:51.152+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO FileFormatWriter: Write Job e11c227f-a654-4efc-9e32-db60edd56acf committed. Elapsed time: 40 ms.
[2024-11-12T09:51:51.152+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO FileFormatWriter: Finished processing stats for write job e11c227f-a654-4efc-9e32-db60edd56acf.
[2024-11-12T09:51:51.159+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/113 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.113.40222563-b399-41f4-87c6-044c5a3be574.tmp
[2024-11-12T09:51:51.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.113.40222563-b399-41f4-87c6-044c5a3be574.tmp to hdfs://namenode:9000/spark_checkpoint/commits/113
[2024-11-12T09:51:51.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:51.190+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:50.308Z",
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "batchId" : 113,
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.342281879194631,
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1350737797956867,
[2024-11-12T09:51:51.191+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "addBatch" : 757,
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "commitOffsets" : 44,
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "triggerExecution" : 881,
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "walCommit" : 55
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:51.193+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:51.194+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:51.194+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:51.194+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:51.194+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:51.194+0000] {spark_submit.py:495} INFO - "0" : 699
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - "0" : 700
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:51.195+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "0" : 700
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.342281879194631,
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1350737797956867,
[2024-11-12T09:51:51.196+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:51.197+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:51.197+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:51.197+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:51.197+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:51.198+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:51.198+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:51.198+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:51.198+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:51.198+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:51.198+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:51.200+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/114 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.114.2332f99b-d697-42e3-8c6c-c0130a009f49.tmp
[2024-11-12T09:51:51.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.114.2332f99b-d697-42e3-8c6c-c0130a009f49.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/114
[2024-11-12T09:51:51.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO MicroBatchExecution: Committed offsets for batch 114. Metadata OffsetSeqMetadata(0,1731405111194,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:51.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:51.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:51.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:51.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:51.284+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 112, 113, 113
[2024-11-12T09:51:51.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:51.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:51.314+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Got job 114 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:51.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Final stage: ResultStage 114 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:51.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:51.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:51.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[460] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:51.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:51:51.337+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:51:51.337+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:51.337+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:51.338+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[460] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:51.338+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks resource profile 0
[2024-11-12T09:51:51.339+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 114) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:51.356+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:51.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 114) in 643 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:51.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool
[2024-11-12T09:51:51.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: ResultStage 114 (start at NativeMethodAccessorImpl.java:0) finished in 0.668 s
[2024-11-12T09:51:51.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:51.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
[2024-11-12T09:51:51.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO DAGScheduler: Job 114 finished: start at NativeMethodAccessorImpl.java:0, took 0.669841 s
[2024-11-12T09:51:51.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:51 INFO FileFormatWriter: Start to commit write Job 4f642b10-3ea5-4713-8cbc-8cbf34b4f4de.
[2024-11-12T09:51:52.003+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/114 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.114.72db6714-02cf-48ca-b7b8-ecb18eedacb4.tmp
[2024-11-12T09:51:52.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.114.72db6714-02cf-48ca-b7b8-ecb18eedacb4.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/114
[2024-11-12T09:51:52.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO FileStreamSinkLog: Current compact batch id = 114 min compaction batch id to delete = 9
[2024-11-12T09:51:52.051+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO ManifestFileCommitProtocol: Committed batch 114
[2024-11-12T09:51:52.052+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO FileFormatWriter: Write Job 4f642b10-3ea5-4713-8cbc-8cbf34b4f4de committed. Elapsed time: 68 ms.
[2024-11-12T09:51:52.052+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO FileFormatWriter: Finished processing stats for write job 4f642b10-3ea5-4713-8cbc-8cbf34b4f4de.
[2024-11-12T09:51:52.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/114 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.114.6566b03b-b30b-43e6-b08b-9a9570112c28.tmp
[2024-11-12T09:51:52.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.114.6566b03b-b30b-43e6-b08b-9a9570112c28.tmp to hdfs://namenode:9000/spark_checkpoint/commits/114
[2024-11-12T09:51:52.117+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:52.118+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:52.119+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:52.119+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:52.120+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:51.190Z",
[2024-11-12T09:51:52.121+0000] {spark_submit.py:495} INFO - "batchId" : 114,
[2024-11-12T09:51:52.121+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:52.121+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1337868480725624,
[2024-11-12T09:51:52.121+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.079913606911447,
[2024-11-12T09:51:52.122+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:52.122+0000] {spark_submit.py:495} INFO - "addBatch" : 795,
[2024-11-12T09:51:52.122+0000] {spark_submit.py:495} INFO - "commitOffsets" : 64,
[2024-11-12T09:51:52.123+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:52.123+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:51:52.123+0000] {spark_submit.py:495} INFO - "queryPlanning" : 17,
[2024-11-12T09:51:52.123+0000] {spark_submit.py:495} INFO - "triggerExecution" : 926,
[2024-11-12T09:51:52.123+0000] {spark_submit.py:495} INFO - "walCommit" : 43
[2024-11-12T09:51:52.123+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:52.124+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:52.124+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:52.124+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - "0" : 700
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:52.125+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "0" : 701
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "0" : 701
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1337868480725624,
[2024-11-12T09:51:52.126+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.079913606911447,
[2024-11-12T09:51:52.127+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:52.127+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:52.127+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:52.127+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:52.127+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:52.127+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:52.128+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:52.128+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:52.128+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:52.135+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:52.136+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:52.179+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/115 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.115.85dc0360-7925-41c5-985e-4ed11691c071.tmp
[2024-11-12T09:51:52.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.115.85dc0360-7925-41c5-985e-4ed11691c071.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/115
[2024-11-12T09:51:52.216+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO MicroBatchExecution: Committed offsets for batch 115. Metadata OffsetSeqMetadata(0,1731405112169,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:52.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:52.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:52.252+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:52.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:52.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 113, 114, 114
[2024-11-12T09:51:52.273+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:52.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:52.310+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO DAGScheduler: Got job 115 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:52.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO DAGScheduler: Final stage: ResultStage 115 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:52.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:52.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:52.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO DAGScheduler: Submitting ResultStage 115 (MapPartitionsRDD[464] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:52.339+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:51:52.343+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_111_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:52.353+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:51:52.355+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:52.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:52.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:52.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[464] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:52.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks resource profile 0
[2024-11-12T09:51:52.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 115) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:52.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_114_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:52.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:52.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_110_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:52.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:52.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:52.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_113_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:52.397+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:52.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_112_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:52.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:52 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:53.402+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 115) in 1042 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:53.402+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool
[2024-11-12T09:51:53.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: ResultStage 115 (start at NativeMethodAccessorImpl.java:0) finished in 1.090 s
[2024-11-12T09:51:53.403+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:53.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 115: Stage finished
[2024-11-12T09:51:53.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Job 115 finished: start at NativeMethodAccessorImpl.java:0, took 1.095277 s
[2024-11-12T09:51:53.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO FileFormatWriter: Start to commit write Job e1d61e88-5c8d-4025-b504-aefba39d466c.
[2024-11-12T09:51:53.414+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/115 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.115.de7bc722-1e87-4237-a93b-05491c088271.tmp
[2024-11-12T09:51:53.448+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.115.de7bc722-1e87-4237-a93b-05491c088271.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/115
[2024-11-12T09:51:53.450+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO FileStreamSinkLog: Current compact batch id = 115 min compaction batch id to delete = 9
[2024-11-12T09:51:53.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO ManifestFileCommitProtocol: Committed batch 115
[2024-11-12T09:51:53.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO FileFormatWriter: Write Job e1d61e88-5c8d-4025-b504-aefba39d466c committed. Elapsed time: 45 ms.
[2024-11-12T09:51:53.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO FileFormatWriter: Finished processing stats for write job e1d61e88-5c8d-4025-b504-aefba39d466c.
[2024-11-12T09:51:53.465+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/115 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.115.b75be10f-9d1e-4ac2-9ffe-95fe4c02fcc5.tmp
[2024-11-12T09:51:53.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.115.b75be10f-9d1e-4ac2-9ffe-95fe4c02fcc5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/115
[2024-11-12T09:51:53.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:53.500+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:53.501+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:52.159Z",
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "batchId" : 115,
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7462686567164178,
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "addBatch" : 1210,
[2024-11-12T09:51:53.502+0000] {spark_submit.py:495} INFO - "commitOffsets" : 48,
[2024-11-12T09:51:53.503+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:53.503+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:51:53.504+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:51:53.504+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1340,
[2024-11-12T09:51:53.504+0000] {spark_submit.py:495} INFO - "walCommit" : 46
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "0" : 701
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:53.505+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:53.506+0000] {spark_submit.py:495} INFO - "0" : 702
[2024-11-12T09:51:53.506+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:53.506+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:53.506+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:53.506+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:53.506+0000] {spark_submit.py:495} INFO - "0" : 702
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7462686567164178,
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:53.507+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:53.508+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:53.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/116 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.116.f7cc0ad1-cb2b-4d70-a727-941571037f28.tmp
[2024-11-12T09:51:53.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.116.f7cc0ad1-cb2b-4d70-a727-941571037f28.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/116
[2024-11-12T09:51:53.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO MicroBatchExecution: Committed offsets for batch 116. Metadata OffsetSeqMetadata(0,1731405113503,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:53.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:53.564+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:53.572+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:53.573+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:53.580+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 114, 115, 115
[2024-11-12T09:51:53.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:53.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:53.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Got job 116 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:53.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Final stage: ResultStage 116 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:53.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:53.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:53.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[468] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:53.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:51:53.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:51:53.635+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:53.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:53.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[468] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:53.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO TaskSchedulerImpl: Adding task set 116.0 with 1 tasks resource profile 0
[2024-11-12T09:51:53.640+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 116) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:53.655+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:53 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:54.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 116) in 617 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:54.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool
[2024-11-12T09:51:54.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: ResultStage 116 (start at NativeMethodAccessorImpl.java:0) finished in 0.646 s
[2024-11-12T09:51:54.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:54.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished
[2024-11-12T09:51:54.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Job 116 finished: start at NativeMethodAccessorImpl.java:0, took 0.652063 s
[2024-11-12T09:51:54.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO FileFormatWriter: Start to commit write Job 8a4028d3-9393-405b-9541-dbabca2c22f7.
[2024-11-12T09:51:54.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/116 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.116.ac231452-ceba-49a2-9672-3e79fee220be.tmp
[2024-11-12T09:51:54.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.116.ac231452-ceba-49a2-9672-3e79fee220be.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/116
[2024-11-12T09:51:54.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO FileStreamSinkLog: Current compact batch id = 116 min compaction batch id to delete = 9
[2024-11-12T09:51:54.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO ManifestFileCommitProtocol: Committed batch 116
[2024-11-12T09:51:54.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO FileFormatWriter: Write Job 8a4028d3-9393-405b-9541-dbabca2c22f7 committed. Elapsed time: 55 ms.
[2024-11-12T09:51:54.324+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO FileFormatWriter: Finished processing stats for write job 8a4028d3-9393-405b-9541-dbabca2c22f7.
[2024-11-12T09:51:54.335+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/116 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.116.5bdbfd88-1271-4c7f-ad01-c671421492a7.tmp
[2024-11-12T09:51:54.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.116.5bdbfd88-1271-4c7f-ad01-c671421492a7.tmp to hdfs://namenode:9000/spark_checkpoint/commits/116
[2024-11-12T09:51:54.367+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:54.368+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:54.368+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:54.369+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:54.369+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:53.500Z",
[2024-11-12T09:51:54.369+0000] {spark_submit.py:495} INFO - "batchId" : 116,
[2024-11-12T09:51:54.369+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:54.369+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7457121551081283,
[2024-11-12T09:51:54.369+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1547344110854503,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "addBatch" : 758,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "commitOffsets" : 42,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "triggerExecution" : 866,
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - "walCommit" : 40
[2024-11-12T09:51:54.370+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - "0" : 702
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:54.371+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:54.372+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:54.372+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:54.372+0000] {spark_submit.py:495} INFO - "0" : 703
[2024-11-12T09:51:54.372+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "0" : 703
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7457121551081283,
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1547344110854503,
[2024-11-12T09:51:54.373+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:54.374+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:54.374+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:54.374+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:54.374+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:54.374+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:54.375+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:54.375+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:54.375+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:54.375+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:54.375+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:54.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/117 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.117.ea82d8dd-131c-46b7-be82-db8f72176fd5.tmp
[2024-11-12T09:51:54.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.117.ea82d8dd-131c-46b7-be82-db8f72176fd5.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/117
[2024-11-12T09:51:54.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO MicroBatchExecution: Committed offsets for batch 117. Metadata OffsetSeqMetadata(0,1731405114371,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:54.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:54.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:54.436+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:54.437+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:54.448+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 115, 116, 116
[2024-11-12T09:51:54.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:54.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:54.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Got job 117 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:54.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Final stage: ResultStage 117 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:54.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:54.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:54.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:54.494+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:51:54.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:51:54.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:54.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:54.504+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:54.504+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0
[2024-11-12T09:51:54.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 117) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:54.520+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:54 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:55.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 117) in 608 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:55.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool
[2024-11-12T09:51:55.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: ResultStage 117 (start at NativeMethodAccessorImpl.java:0) finished in 0.636 s
[2024-11-12T09:51:55.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Job 117 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:55.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
[2024-11-12T09:51:55.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Job 117 finished: start at NativeMethodAccessorImpl.java:0, took 0.639024 s
[2024-11-12T09:51:55.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO FileFormatWriter: Start to commit write Job fc134ce2-bbbe-4956-b7a4-2674b973ae22.
[2024-11-12T09:51:55.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/117 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.117.bbed4e05-4a6e-4bcd-b1de-391a40eb4548.tmp
[2024-11-12T09:51:55.157+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.117.bbed4e05-4a6e-4bcd-b1de-391a40eb4548.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/117
[2024-11-12T09:51:55.158+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO FileStreamSinkLog: Current compact batch id = 117 min compaction batch id to delete = 9
[2024-11-12T09:51:55.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO ManifestFileCommitProtocol: Committed batch 117
[2024-11-12T09:51:55.163+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO FileFormatWriter: Write Job fc134ce2-bbbe-4956-b7a4-2674b973ae22 committed. Elapsed time: 45 ms.
[2024-11-12T09:51:55.163+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO FileFormatWriter: Finished processing stats for write job fc134ce2-bbbe-4956-b7a4-2674b973ae22.
[2024-11-12T09:51:55.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/117 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.117.094494e1-a549-4be8-855b-a50b97bb795d.tmp
[2024-11-12T09:51:55.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.117.094494e1-a549-4be8-855b-a50b97bb795d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/117
[2024-11-12T09:51:55.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:55.215+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:55.216+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:55.216+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:55.216+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:54.367Z",
[2024-11-12T09:51:55.217+0000] {spark_submit.py:495} INFO - "batchId" : 117,
[2024-11-12T09:51:55.217+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:55.217+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1534025374855825,
[2024-11-12T09:51:55.217+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1820330969267139,
[2024-11-12T09:51:55.218+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:55.218+0000] {spark_submit.py:495} INFO - "addBatch" : 732,
[2024-11-12T09:51:55.218+0000] {spark_submit.py:495} INFO - "commitOffsets" : 51,
[2024-11-12T09:51:55.218+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:55.218+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:51:55.219+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:51:55.220+0000] {spark_submit.py:495} INFO - "triggerExecution" : 846,
[2024-11-12T09:51:55.221+0000] {spark_submit.py:495} INFO - "walCommit" : 35
[2024-11-12T09:51:55.221+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:55.222+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:55.223+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:55.223+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:55.225+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:55.225+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:55.225+0000] {spark_submit.py:495} INFO - "0" : 703
[2024-11-12T09:51:55.226+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:55.227+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:55.227+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:55.228+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:55.228+0000] {spark_submit.py:495} INFO - "0" : 704
[2024-11-12T09:51:55.229+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:55.229+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:55.229+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:55.235+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:55.236+0000] {spark_submit.py:495} INFO - "0" : 704
[2024-11-12T09:51:55.237+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:55.239+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:55.239+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:55.240+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1534025374855825,
[2024-11-12T09:51:55.240+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1820330969267139,
[2024-11-12T09:51:55.240+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:55.240+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:55.241+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:55.241+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:55.242+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:55.242+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:55.242+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:55.243+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:55.243+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:55.243+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:55.243+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:55.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/118 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.118.c489af16-72d4-4791-ad62-51ef71133476.tmp
[2024-11-12T09:51:55.272+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.118.c489af16-72d4-4791-ad62-51ef71133476.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/118
[2024-11-12T09:51:55.273+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO MicroBatchExecution: Committed offsets for batch 118. Metadata OffsetSeqMetadata(0,1731405115225,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:55.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:55.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:55.309+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:55.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:55.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 116, 117, 117
[2024-11-12T09:51:55.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:55.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:55.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Got job 118 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:55.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Final stage: ResultStage 118 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:55.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:55.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:55.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Submitting ResultStage 118 (MapPartitionsRDD[476] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:55.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:51:55.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:51:55.378+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:55.379+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:55.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[476] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:55.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO TaskSchedulerImpl: Adding task set 118.0 with 1 tasks resource profile 0
[2024-11-12T09:51:55.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 118) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:55.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:55 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:56.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 118) in 654 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:56.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool
[2024-11-12T09:51:56.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: ResultStage 118 (start at NativeMethodAccessorImpl.java:0) finished in 0.686 s
[2024-11-12T09:51:56.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Job 118 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:56.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 118: Stage finished
[2024-11-12T09:51:56.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Job 118 finished: start at NativeMethodAccessorImpl.java:0, took 0.696703 s
[2024-11-12T09:51:56.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO FileFormatWriter: Start to commit write Job b34d7c4e-6bdf-4548-a966-c5b3252d17a2.
[2024-11-12T09:51:56.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/118 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.118.12faf090-0ec6-4b25-ab6c-e485e3452c15.tmp
[2024-11-12T09:51:56.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.118.12faf090-0ec6-4b25-ab6c-e485e3452c15.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/118
[2024-11-12T09:51:56.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO FileStreamSinkLog: Current compact batch id = 118 min compaction batch id to delete = 9
[2024-11-12T09:51:56.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO ManifestFileCommitProtocol: Committed batch 118
[2024-11-12T09:51:56.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO FileFormatWriter: Write Job b34d7c4e-6bdf-4548-a966-c5b3252d17a2 committed. Elapsed time: 75 ms.
[2024-11-12T09:51:56.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO FileFormatWriter: Finished processing stats for write job b34d7c4e-6bdf-4548-a966-c5b3252d17a2.
[2024-11-12T09:51:56.132+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/118 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.118.72c882ae-f94b-4629-9c00-25fd0b579097.tmp
[2024-11-12T09:51:56.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.118.72c882ae-f94b-4629-9c00-25fd0b579097.tmp to hdfs://namenode:9000/spark_checkpoint/commits/118
[2024-11-12T09:51:56.180+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:56.182+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:56.184+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:56.187+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:56.189+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:55.215Z",
[2024-11-12T09:51:56.189+0000] {spark_submit.py:495} INFO - "batchId" : 118,
[2024-11-12T09:51:56.189+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:56.189+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.179245283018868,
[2024-11-12T09:51:56.190+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0416666666666667,
[2024-11-12T09:51:56.190+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:56.190+0000] {spark_submit.py:495} INFO - "addBatch" : 828,
[2024-11-12T09:51:56.190+0000] {spark_submit.py:495} INFO - "commitOffsets" : 54,
[2024-11-12T09:51:56.190+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:51:56.191+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:51:56.191+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:51:56.191+0000] {spark_submit.py:495} INFO - "triggerExecution" : 960,
[2024-11-12T09:51:56.191+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:51:56.191+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:56.191+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "0" : 704
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:56.192+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:56.193+0000] {spark_submit.py:495} INFO - "0" : 705
[2024-11-12T09:51:56.193+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:56.193+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "0" : 705
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.179245283018868,
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0416666666666667,
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:56.194+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:56.195+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:56.195+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:56.195+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:56.195+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:56.196+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:56.196+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:56.196+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:56.196+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:56.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/119 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.119.d2e6e760-d63f-40f5-a7fc-04c8c2bfec67.tmp
[2024-11-12T09:51:56.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.119.d2e6e760-d63f-40f5-a7fc-04c8c2bfec67.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/119
[2024-11-12T09:51:56.673+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO MicroBatchExecution: Committed offsets for batch 119. Metadata OffsetSeqMetadata(0,1731405116224,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:56.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:56.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:56.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:56.704+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:56.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 117, 118, 118
[2024-11-12T09:51:56.710+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:56.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:56.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Got job 119 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:56.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Final stage: ResultStage 119 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:56.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:56.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:56.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[480] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:56.761+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:51:56.764+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:51:56.764+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:56.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:56.766+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[480] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:56.766+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0
[2024-11-12T09:51:56.775+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 119) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:56.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:56 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:57.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 119) in 536 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:57.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool
[2024-11-12T09:51:57.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO DAGScheduler: ResultStage 119 (start at NativeMethodAccessorImpl.java:0) finished in 0.564 s
[2024-11-12T09:51:57.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO DAGScheduler: Job 119 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:57.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished
[2024-11-12T09:51:57.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO DAGScheduler: Job 119 finished: start at NativeMethodAccessorImpl.java:0, took 0.571730 s
[2024-11-12T09:51:57.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO FileFormatWriter: Start to commit write Job aa67da40-7da2-464c-9ef4-ed5540bb1b5a.
[2024-11-12T09:51:57.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/119.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.119.compact.11fb971b-ff34-45f3-960a-167d03b714e3.tmp
[2024-11-12T09:51:57.923+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.119.compact.11fb971b-ff34-45f3-960a-167d03b714e3.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/119.compact
[2024-11-12T09:51:57.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO FileStreamSinkLog: Current compact batch id = 119 min compaction batch id to delete = 19
[2024-11-12T09:51:57.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO ManifestFileCommitProtocol: Committed batch 119
[2024-11-12T09:51:57.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO FileFormatWriter: Write Job aa67da40-7da2-464c-9ef4-ed5540bb1b5a committed. Elapsed time: 614 ms.
[2024-11-12T09:51:57.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO FileFormatWriter: Finished processing stats for write job aa67da40-7da2-464c-9ef4-ed5540bb1b5a.
[2024-11-12T09:51:57.945+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/119 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.119.aedea728-f338-45aa-a241-f59e8c077bd0.tmp
[2024-11-12T09:51:58.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.119.aedea728-f338-45aa-a241-f59e8c077bd0.tmp to hdfs://namenode:9000/spark_checkpoint/commits/119
[2024-11-12T09:51:58.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:58.390+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:58.390+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:58.390+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:56.213Z",
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "batchId" : 119,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.4597701149425288,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "addBatch" : 1232,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "commitOffsets" : 458,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:51:58.391+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:51:58.392+0000] {spark_submit.py:495} INFO - "triggerExecution" : 2175,
[2024-11-12T09:51:58.392+0000] {spark_submit.py:495} INFO - "walCommit" : 448
[2024-11-12T09:51:58.393+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:58.393+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:58.393+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:58.393+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:58.397+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:58.397+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "0" : 705
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "0" : 706
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - "0" : 706
[2024-11-12T09:51:58.398+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 71.42857142857143,
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.4597701149425288,
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:58.399+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:58.400+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:58.400+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:58.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/120 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.120.1559d87a-c09f-47b4-bb1e-9ad0739b8a9f.tmp
[2024-11-12T09:51:58.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.120.1559d87a-c09f-47b4-bb1e-9ad0739b8a9f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/120
[2024-11-12T09:51:58.444+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO MicroBatchExecution: Committed offsets for batch 120. Metadata OffsetSeqMetadata(0,1731405118399,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:58.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:58.463+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:58.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:58.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:58.480+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 117, 118, 118, 119
[2024-11-12T09:51:58.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:58.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:58.520+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO DAGScheduler: Got job 120 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:58.521+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO DAGScheduler: Final stage: ResultStage 120 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:58.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:58.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:58.527+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[484] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:58.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:51:58.565+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:51:58.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:51:58.569+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_118_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:51:58.570+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:58.570+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[484] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:58.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks resource profile 0
[2024-11-12T09:51:58.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:58.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 120) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:58.585+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_117_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:58.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:58.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:51:58.610+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_116_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:58.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:51:58.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_119_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:58.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:58.641+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_115_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:58.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:58 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:51:59.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 120) in 681 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:51:59.252+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool
[2024-11-12T09:51:59.253+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: ResultStage 120 (start at NativeMethodAccessorImpl.java:0) finished in 0.730 s
[2024-11-12T09:51:59.253+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Job 120 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:51:59.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
[2024-11-12T09:51:59.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Job 120 finished: start at NativeMethodAccessorImpl.java:0, took 0.739849 s
[2024-11-12T09:51:59.256+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO FileFormatWriter: Start to commit write Job c32dfdfc-c650-43df-ae5f-42762f8bffaa.
[2024-11-12T09:51:59.261+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/120 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.120.7bdbf661-f7cc-4fe5-8fb5-a6ea6f42323c.tmp
[2024-11-12T09:51:59.299+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.120.7bdbf661-f7cc-4fe5-8fb5-a6ea6f42323c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/120
[2024-11-12T09:51:59.300+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO FileStreamSinkLog: Current compact batch id = 120 min compaction batch id to delete = 19
[2024-11-12T09:51:59.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO ManifestFileCommitProtocol: Committed batch 120
[2024-11-12T09:51:59.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO FileFormatWriter: Write Job c32dfdfc-c650-43df-ae5f-42762f8bffaa committed. Elapsed time: 58 ms.
[2024-11-12T09:51:59.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO FileFormatWriter: Finished processing stats for write job c32dfdfc-c650-43df-ae5f-42762f8bffaa.
[2024-11-12T09:51:59.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/120 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.120.e121bc92-2cb8-4677-bba8-9f2341951400.tmp
[2024-11-12T09:51:59.363+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.120.e121bc92-2cb8-4677-bba8-9f2341951400.tmp to hdfs://namenode:9000/spark_checkpoint/commits/120
[2024-11-12T09:51:59.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:51:59.378+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:51:59.378+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:51:59.381+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:51:59.381+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:58.390Z",
[2024-11-12T09:51:59.382+0000] {spark_submit.py:495} INFO - "batchId" : 120,
[2024-11-12T09:51:59.382+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:51:59.382+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9186954524575103,
[2024-11-12T09:51:59.383+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.055498458376156,
[2024-11-12T09:51:59.383+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:51:59.383+0000] {spark_submit.py:495} INFO - "addBatch" : 845,
[2024-11-12T09:51:59.383+0000] {spark_submit.py:495} INFO - "commitOffsets" : 50,
[2024-11-12T09:51:59.383+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:51:59.383+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - "triggerExecution" : 973,
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - "walCommit" : 44
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:51:59.384+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - "0" : 706
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - "0" : 708
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:59.385+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:59.386+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:51:59.386+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:51:59.386+0000] {spark_submit.py:495} INFO - "0" : 708
[2024-11-12T09:51:59.386+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:59.387+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:51:59.387+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:51:59.387+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9186954524575103,
[2024-11-12T09:51:59.387+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.055498458376156,
[2024-11-12T09:51:59.388+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:51:59.388+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:51:59.388+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:51:59.388+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:51:59.392+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:59.393+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:51:59.393+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:51:59.393+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:51:59.393+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:51:59.393+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:59.393+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:51:59.397+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/121 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.121.5895c6d1-4f8f-4f15-b7b9-5c9211d45166.tmp
[2024-11-12T09:51:59.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.121.5895c6d1-4f8f-4f15-b7b9-5c9211d45166.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/121
[2024-11-12T09:51:59.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO MicroBatchExecution: Committed offsets for batch 121. Metadata OffsetSeqMetadata(0,1731405119385,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:51:59.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:59.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:59.489+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:59.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:51:59.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 120
[2024-11-12T09:51:59.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:51:59.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:51:59.551+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Got job 121 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:51:59.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Final stage: ResultStage 121 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:51:59.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:51:59.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:51:59.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[488] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:51:59.572+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:51:59.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:51:59.584+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:51:59.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:51:59.591+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[488] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:51:59.591+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO TaskSchedulerImpl: Adding task set 121.0 with 1 tasks resource profile 0
[2024-11-12T09:51:59.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO TaskSetManager: Starting task 0.0 in stage 121.0 (TID 121) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:51:59.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:51:59 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:52:00.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO TaskSetManager: Finished task 0.0 in stage 121.0 (TID 121) in 1051 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:52:00.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool
[2024-11-12T09:52:00.645+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: ResultStage 121 (start at NativeMethodAccessorImpl.java:0) finished in 1.103 s
[2024-11-12T09:52:00.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:52:00.647+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 121: Stage finished
[2024-11-12T09:52:00.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Job 121 finished: start at NativeMethodAccessorImpl.java:0, took 1.110158 s
[2024-11-12T09:52:00.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO FileFormatWriter: Start to commit write Job 7e8204f8-1765-49d4-94f4-0821e10cb303.
[2024-11-12T09:52:00.671+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/121 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.121.1c30d720-11a3-4c05-be78-e97613c2751d.tmp
[2024-11-12T09:52:00.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.121.1c30d720-11a3-4c05-be78-e97613c2751d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/121
[2024-11-12T09:52:00.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO FileStreamSinkLog: Current compact batch id = 121 min compaction batch id to delete = 19
[2024-11-12T09:52:00.724+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO ManifestFileCommitProtocol: Committed batch 121
[2024-11-12T09:52:00.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO FileFormatWriter: Write Job 7e8204f8-1765-49d4-94f4-0821e10cb303 committed. Elapsed time: 74 ms.
[2024-11-12T09:52:00.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO FileFormatWriter: Finished processing stats for write job 7e8204f8-1765-49d4-94f4-0821e10cb303.
[2024-11-12T09:52:00.743+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/121 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.121.901473d1-7287-45fa-bfef-38f4a027717f.tmp
[2024-11-12T09:52:00.794+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.121.901473d1-7287-45fa-bfef-38f4a027717f.tmp to hdfs://namenode:9000/spark_checkpoint/commits/121
[2024-11-12T09:52:00.796+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:51:59.368Z",
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "batchId" : 121,
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0224948875255624,
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7012622720897616,
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "addBatch" : 1248,
[2024-11-12T09:52:00.797+0000] {spark_submit.py:495} INFO - "commitOffsets" : 70,
[2024-11-12T09:52:00.798+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:52:00.798+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:52:00.798+0000] {spark_submit.py:495} INFO - "queryPlanning" : 20,
[2024-11-12T09:52:00.798+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1426,
[2024-11-12T09:52:00.798+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:52:00.799+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:00.799+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:00.799+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:00.799+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:00.806+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:00.807+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:00.807+0000] {spark_submit.py:495} INFO - "0" : 708
[2024-11-12T09:52:00.807+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:00.808+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:00.808+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:00.808+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:00.808+0000] {spark_submit.py:495} INFO - "0" : 709
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - "0" : 709
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:52:00.809+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0224948875255624,
[2024-11-12T09:52:00.810+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7012622720897616,
[2024-11-12T09:52:00.810+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:00.810+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:00.811+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:00.811+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:00.811+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:00.811+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:00.811+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:00.812+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:00.812+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:00.812+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:00.813+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:00.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/122 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.122.0133a9db-6419-4056-8f5b-a0e2e7939398.tmp
[2024-11-12T09:52:00.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.122.0133a9db-6419-4056-8f5b-a0e2e7939398.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/122
[2024-11-12T09:52:00.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO MicroBatchExecution: Committed offsets for batch 122. Metadata OffsetSeqMetadata(0,1731405120809,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:52:00.889+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:00.890+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:00.908+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:00.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:00.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 120, 121, 121
[2024-11-12T09:52:00.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:52:00.958+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:52:00.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Got job 122 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:52:00.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Final stage: ResultStage 122 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:52:00.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:52:00.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:52:00.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[492] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:52:00.990+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:52:00.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:52:00.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:52:00.994+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO SparkContext: Created broadcast 122 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:52:00.995+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[492] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:52:00.995+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO TaskSchedulerImpl: Adding task set 122.0 with 1 tasks resource profile 0
[2024-11-12T09:52:00.995+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:00 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 122) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:52:01.018+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:52:01.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 122) in 757 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:52:01.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool
[2024-11-12T09:52:01.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO DAGScheduler: ResultStage 122 (start at NativeMethodAccessorImpl.java:0) finished in 0.791 s
[2024-11-12T09:52:01.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO DAGScheduler: Job 122 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:52:01.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished
[2024-11-12T09:52:01.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO DAGScheduler: Job 122 finished: start at NativeMethodAccessorImpl.java:0, took 0.794635 s
[2024-11-12T09:52:01.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO FileFormatWriter: Start to commit write Job 35575efa-a7a1-4da3-963b-43fcea7c465e.
[2024-11-12T09:52:01.760+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/122 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.122.be3fe074-6bd4-439a-a144-a2041c3f6080.tmp
[2024-11-12T09:52:01.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.122.be3fe074-6bd4-439a-a144-a2041c3f6080.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/122
[2024-11-12T09:52:01.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO FileStreamSinkLog: Current compact batch id = 122 min compaction batch id to delete = 19
[2024-11-12T09:52:01.817+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO ManifestFileCommitProtocol: Committed batch 122
[2024-11-12T09:52:01.818+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO FileFormatWriter: Write Job 35575efa-a7a1-4da3-963b-43fcea7c465e committed. Elapsed time: 63 ms.
[2024-11-12T09:52:01.818+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO FileFormatWriter: Finished processing stats for write job 35575efa-a7a1-4da3-963b-43fcea7c465e.
[2024-11-12T09:52:01.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/122 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.122.cef722b6-f6f5-4cfd-b3bc-865a5dc4b4a5.tmp
[2024-11-12T09:52:02.274+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.122.cef722b6-f6f5-4cfd-b3bc-865a5dc4b4a5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/122
[2024-11-12T09:52:02.279+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:00.796Z",
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "batchId" : 122,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.700280112044818,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6770480704129993,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "addBatch" : 919,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "commitOffsets" : 456,
[2024-11-12T09:52:02.280+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1477,
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "walCommit" : 51
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:02.281+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - "0" : 709
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - "0" : 710
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:02.282+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "0" : 710
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.700280112044818,
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6770480704129993,
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:02.283+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:02.284+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:02.284+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:02.284+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:02.285+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:02.285+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:02.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/123 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.123.115f5f17-880a-407c-a833-77e51ad5ecc9.tmp
[2024-11-12T09:52:02.360+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.123.115f5f17-880a-407c-a833-77e51ad5ecc9.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/123
[2024-11-12T09:52:02.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO MicroBatchExecution: Committed offsets for batch 123. Metadata OffsetSeqMetadata(0,1731405122293,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:52:02.382+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:02.384+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:02.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:02.408+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:02.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 121, 122, 122
[2024-11-12T09:52:02.426+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:52:02.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:52:02.487+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO DAGScheduler: Got job 123 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:52:02.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO DAGScheduler: Final stage: ResultStage 123 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:52:02.488+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:52:02.489+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:52:02.489+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[496] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:52:02.523+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:52:02.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:52:02.537+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:52:02.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:52:02.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[496] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:52:02.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks resource profile 0
[2024-11-12T09:52:02.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 123) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:52:02.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:02 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:52:03.249+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 123) in 707 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:52:03.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool
[2024-11-12T09:52:03.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO DAGScheduler: ResultStage 123 (start at NativeMethodAccessorImpl.java:0) finished in 0.760 s
[2024-11-12T09:52:03.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:52:03.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished
[2024-11-12T09:52:03.257+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO DAGScheduler: Job 123 finished: start at NativeMethodAccessorImpl.java:0, took 0.778171 s
[2024-11-12T09:52:03.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO FileFormatWriter: Start to commit write Job 6267d3a5-c378-4af7-bd46-1618e0564f21.
[2024-11-12T09:52:03.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/123 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.123.4413eaca-5f76-4aee-af64-c330834a4fa6.tmp
[2024-11-12T09:52:03.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.123.4413eaca-5f76-4aee-af64-c330834a4fa6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/123
[2024-11-12T09:52:03.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO FileStreamSinkLog: Current compact batch id = 123 min compaction batch id to delete = 19
[2024-11-12T09:52:03.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO ManifestFileCommitProtocol: Committed batch 123
[2024-11-12T09:52:03.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO FileFormatWriter: Write Job 6267d3a5-c378-4af7-bd46-1618e0564f21 committed. Elapsed time: 479 ms.
[2024-11-12T09:52:03.738+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO FileFormatWriter: Finished processing stats for write job 6267d3a5-c378-4af7-bd46-1618e0564f21.
[2024-11-12T09:52:03.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/123 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.123.6def89c2-3351-48c3-bb73-e9c71984c28a.tmp
[2024-11-12T09:52:03.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.123.6def89c2-3351-48c3-bb73-e9c71984c28a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/123
[2024-11-12T09:52:03.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:03.791+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:03.791+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:03.791+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:03.791+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:02.278Z",
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "batchId" : 123,
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.349527665317139,
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "addBatch" : 1350,
[2024-11-12T09:52:03.792+0000] {spark_submit.py:495} INFO - "commitOffsets" : 53,
[2024-11-12T09:52:03.793+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:52:03.793+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:52:03.793+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:52:03.793+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1512,
[2024-11-12T09:52:03.794+0000] {spark_submit.py:495} INFO - "walCommit" : 67
[2024-11-12T09:52:03.794+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:03.794+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:03.794+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:03.794+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:03.795+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:03.795+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:03.796+0000] {spark_submit.py:495} INFO - "0" : 710
[2024-11-12T09:52:03.796+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:03.796+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:03.797+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:03.797+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:03.797+0000] {spark_submit.py:495} INFO - "0" : 712
[2024-11-12T09:52:03.797+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:03.799+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - "0" : 712
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.349527665317139,
[2024-11-12T09:52:03.800+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:52:03.801+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:03.801+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:03.801+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:03.802+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:03.802+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:03.802+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:03.802+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:03.803+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:03.803+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:03.803+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:03.804+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:03.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:03 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/124 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.124.30c96e74-fbcb-4260-89c6-0c77d383d0ec.tmp
[2024-11-12T09:52:04.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.124.30c96e74-fbcb-4260-89c6-0c77d383d0ec.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/124
[2024-11-12T09:52:04.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO MicroBatchExecution: Committed offsets for batch 124. Metadata OffsetSeqMetadata(0,1731405123796,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:52:04.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:04.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:04.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:04.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:52:04.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 122, 123, 123
[2024-11-12T09:52:04.324+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:52:04.351+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:52:04.354+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO DAGScheduler: Got job 124 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:52:04.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO DAGScheduler: Final stage: ResultStage 124 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:52:04.357+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:52:04.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:52:04.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO DAGScheduler: Submitting ResultStage 124 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:52:04.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:52:04.390+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:52:04.392+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:52:04.393+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:52:04.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 124 (MapPartitionsRDD[500] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:52:04.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO TaskSchedulerImpl: Adding task set 124.0 with 1 tasks resource profile 0
[2024-11-12T09:52:04.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 124) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:52:04.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:04 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:52:05.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 124) in 649 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:52:05.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool
[2024-11-12T09:52:05.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO DAGScheduler: ResultStage 124 (start at NativeMethodAccessorImpl.java:0) finished in 0.688 s
[2024-11-12T09:52:05.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:52:05.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 124: Stage finished
[2024-11-12T09:52:05.057+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO DAGScheduler: Job 124 finished: start at NativeMethodAccessorImpl.java:0, took 0.703680 s
[2024-11-12T09:52:05.063+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO FileFormatWriter: Start to commit write Job 59d38c77-ef01-4287-a3c1-25029ee33690.
[2024-11-12T09:52:05.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/124 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.124.a2d25ca7-dcc6-454e-a04f-d00d90b4bb70.tmp
[2024-11-12T09:52:05.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.124.a2d25ca7-dcc6-454e-a04f-d00d90b4bb70.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/124
[2024-11-12T09:52:05.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO FileStreamSinkLog: Current compact batch id = 124 min compaction batch id to delete = 19
[2024-11-12T09:52:05.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO ManifestFileCommitProtocol: Committed batch 124
[2024-11-12T09:52:05.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO FileFormatWriter: Write Job 59d38c77-ef01-4287-a3c1-25029ee33690 committed. Elapsed time: 65 ms.
[2024-11-12T09:52:05.150+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO FileFormatWriter: Finished processing stats for write job 59d38c77-ef01-4287-a3c1-25029ee33690.
[2024-11-12T09:52:05.152+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/124 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.124.d4ff793f-3dec-4f23-9b1f-2119608be814.tmp
[2024-11-12T09:52:05.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.124.d4ff793f-3dec-4f23-9b1f-2119608be814.tmp to hdfs://namenode:9000/spark_checkpoint/commits/124
[2024-11-12T09:52:05.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:05.242+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:05.243+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:05.243+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:05.243+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:03.791Z",
[2024-11-12T09:52:05.243+0000] {spark_submit.py:495} INFO - "batchId" : 124,
[2024-11-12T09:52:05.243+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:52:05.244+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6609385327164574,
[2024-11-12T09:52:05.244+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6915629322268326,
[2024-11-12T09:52:05.244+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:05.247+0000] {spark_submit.py:495} INFO - "addBatch" : 827,
[2024-11-12T09:52:05.257+0000] {spark_submit.py:495} INFO - "commitOffsets" : 113,
[2024-11-12T09:52:05.267+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:52:05.269+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:52:05.274+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:52:05.274+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1446,
[2024-11-12T09:52:05.274+0000] {spark_submit.py:495} INFO - "walCommit" : 472
[2024-11-12T09:52:05.274+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:05.276+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:05.276+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:05.276+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:05.277+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:05.277+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:05.278+0000] {spark_submit.py:495} INFO - "0" : 712
[2024-11-12T09:52:05.278+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:05.279+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:05.279+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:05.279+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:05.279+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:05.280+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:05.281+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:05.281+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:05.281+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:05.282+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:05.283+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:05.283+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:05.283+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:52:05.283+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6609385327164574,
[2024-11-12T09:52:05.284+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6915629322268326,
[2024-11-12T09:52:05.284+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:05.284+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:05.284+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:05.284+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:05.284+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:05.285+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:05.290+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:05.296+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:05.296+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:05.297+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:05.297+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:15.240+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:15.240+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:15.241+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:15.241+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:15.241+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:15.235Z",
[2024-11-12T09:52:15.241+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:52:15.241+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:15.241+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:15.242+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:15.242+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:15.243+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:52:15.243+0000] {spark_submit.py:495} INFO - "triggerExecution" : 3
[2024-11-12T09:52:15.244+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:15.244+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:15.244+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:15.244+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:15.244+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:15.245+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:15.245+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:15.245+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:15.245+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:15.250+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:15.252+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:15.252+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:15.252+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:15.252+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:15.253+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:15.253+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:15.253+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:15.253+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:15.253+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:15.253+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:15.254+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:15.254+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:15.254+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:15.254+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:15.254+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:15.255+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:25.240Z",
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:25.245+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:25.246+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:52:25.246+0000] {spark_submit.py:495} INFO - "triggerExecution" : 3
[2024-11-12T09:52:25.246+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:25.246+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:25.246+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:25.246+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:25.247+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:25.248+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:25.249+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:25.249+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.249+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:25.249+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:25.249+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:25.250+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:25.251+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:25.251+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:25.251+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:25.252+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:25.252+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.252+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:25.252+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:25.252+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:25.253+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:25.254+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.254+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:25.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_122_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:52:25.595+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:52:25.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_121_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:52:25.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:52:25.624+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_120_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:52:25.628+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:52:25.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_124_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:52:25.648+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:52:25.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_123_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:52:25.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:25 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.4 MiB)
[2024-11-12T09:52:35.251+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:35.252+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:35.253+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:35.253+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:35.253+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:35.243Z",
[2024-11-12T09:52:35.253+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:52:35.263+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:35.264+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:35.265+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:35.265+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:35.266+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:52:35.266+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:35.267+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:35.268+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:35.269+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:35.270+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:35.270+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:35.271+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:35.271+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:35.271+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:35.272+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:35.273+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:35.273+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:35.274+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:35.274+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:35.274+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:35.275+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:35.275+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:35.275+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:45.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:45.268+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:45.269+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:45.271+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:45.272+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:45.257Z",
[2024-11-12T09:52:45.272+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:52:45.272+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:45.273+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:45.273+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:45.273+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:45.274+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:52:45.274+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:52:45.274+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:45.274+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:45.274+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:45.275+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:45.275+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:45.275+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:45.276+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:45.276+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:45.276+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:45.276+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:45.276+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:45.277+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:45.277+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:45.277+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:45.278+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:45.278+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:45.278+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:45.278+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:45.278+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:45.279+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:45.279+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:45.279+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:45.279+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:45.280+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:55.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:52:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:52:55.278+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:52:55.279+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:52:55.279+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:52:55.279+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:52:55.272Z",
[2024-11-12T09:52:55.280+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:52:55.280+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:55.281+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:55.282+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:55.282+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:52:55.282+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:52:55.282+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5
[2024-11-12T09:52:55.283+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:55.283+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:52:55.284+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:52:55.284+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:52:55.285+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:52:55.286+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:55.286+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:55.286+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:55.287+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:55.288+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:52:55.288+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:55.289+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:55.289+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:52:55.290+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:52:55.291+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:52:55.292+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:52:55.292+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:52:55.292+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:52:55.292+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:05.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:53:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:53:05.297+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:53:05.298+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:53:05.299+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:53:05.299+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:53:05.286Z",
[2024-11-12T09:53:05.299+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:53:05.299+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:05.299+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:05.300+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:05.300+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:53:05.300+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:53:05.301+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5
[2024-11-12T09:53:05.301+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:05.301+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:53:05.301+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:53:05.301+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:53:05.302+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:53:05.302+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:05.302+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:05.302+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:05.303+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:05.304+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:53:05.305+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:05.305+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:05.305+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:05.306+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:05.306+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:53:05.306+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:05.306+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:05.306+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:05.307+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:05.307+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:05.307+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:05.307+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:05.307+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:53:05.308+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:53:05.308+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:53:05.308+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:53:05.308+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:05.309+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:53:05.310+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:53:05.310+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:53:05.310+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:53:05.310+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:05.310+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:15.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:53:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:53:15.295+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:53:15.290Z",
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4
[2024-11-12T09:53:15.296+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:53:15.297+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:15.298+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:15.299+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:15.299+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:15.299+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:53:15.299+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:53:15.299+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:15.300+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:25.309+0000] {spark_submit.py:495} INFO - 24/11/12 09:53:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:53:25.310+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:53:25.310+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:53:25.311+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:53:25.311+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:53:25.302Z",
[2024-11-12T09:53:25.311+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:25.312+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:25.313+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:25.313+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:53:25.313+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:25.315+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:25.317+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:25.318+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:25.319+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:53:25.319+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:25.319+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:25.319+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:25.319+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:25.319+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:53:25.320+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:25.321+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:53:25.321+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:53:25.322+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:53:25.322+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:53:25.322+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:25.322+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:35.315+0000] {spark_submit.py:495} INFO - 24/11/12 09:53:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:53:35.316+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:53:35.317+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:53:35.317+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:53:35.317+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:53:35.277Z",
[2024-11-12T09:53:35.317+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:53:35.318+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:35.320+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:35.321+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:35.321+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:53:35.321+0000] {spark_submit.py:495} INFO - "latestOffset" : 34,
[2024-11-12T09:53:35.321+0000] {spark_submit.py:495} INFO - "triggerExecution" : 35
[2024-11-12T09:53:35.321+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:35.321+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:53:35.332+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:53:35.334+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:35.335+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:53:35.336+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:35.336+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:35.336+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:35.337+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:35.337+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:35.337+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:53:35.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:35.339+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:45.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:53:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:53:45.316+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:53:45.316+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:53:45.317+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:53:45.317+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:53:45.307Z",
[2024-11-12T09:53:45.317+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:53:45.317+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:45.317+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:45.318+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:45.319+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:53:45.320+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:45.323+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:45.323+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:45.323+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:45.323+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:53:45.324+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:53:45.325+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:45.325+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:55.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:53:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:53:55.331+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:53:55.332+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:53:55.332+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:53:55.333+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:53:55.313Z",
[2024-11-12T09:53:55.333+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:53:55.333+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:55.334+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:55.334+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:55.334+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:53:55.334+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:53:55.335+0000] {spark_submit.py:495} INFO - "triggerExecution" : 13
[2024-11-12T09:53:55.335+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:55.335+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:53:55.336+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:53:55.336+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:53:55.336+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:53:55.336+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:53:55.337+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:53:55.338+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:53:55.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:55.339+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:53:55.339+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:53:55.339+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:53:55.339+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:53:55.339+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:53:55.339+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:53:55.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:53:55.341+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:05.341+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:05.342+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:05.342+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:05.331Z",
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "latestOffset" : 3,
[2024-11-12T09:54:05.343+0000] {spark_submit.py:495} INFO - "triggerExecution" : 3
[2024-11-12T09:54:05.344+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:05.344+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:05.344+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:05.345+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:05.346+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:05.347+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:10.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/125 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.125.f0f52ef0-5ef8-4c3a-93d9-6e9649aa5ddb.tmp
[2024-11-12T09:54:10.439+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.125.f0f52ef0-5ef8-4c3a-93d9-6e9649aa5ddb.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/125
[2024-11-12T09:54:10.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO MicroBatchExecution: Committed offsets for batch 125. Metadata OffsetSeqMetadata(0,1731405250375,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:10.459+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:10.463+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:10.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:10.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:10.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123, 124, 124
[2024-11-12T09:54:10.489+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:10.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:10.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO DAGScheduler: Got job 125 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:10.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO DAGScheduler: Final stage: ResultStage 125 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:10.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:10.537+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:10.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[504] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:10.565+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 320.7 KiB, free 434.1 MiB)
[2024-11-12T09:54:10.579+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 434.0 MiB)
[2024-11-12T09:54:10.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:10.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:10.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[504] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:10.584+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks resource profile 0
[2024-11-12T09:54:10.585+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 125) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:10.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:11.218+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 125) in 632 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:11.219+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool
[2024-11-12T09:54:11.219+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: ResultStage 125 (start at NativeMethodAccessorImpl.java:0) finished in 0.682 s
[2024-11-12T09:54:11.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Job 125 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:11.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished
[2024-11-12T09:54:11.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Job 125 finished: start at NativeMethodAccessorImpl.java:0, took 0.685745 s
[2024-11-12T09:54:11.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO FileFormatWriter: Start to commit write Job 3eb15cba-872d-476d-a7ed-e72775165fc1.
[2024-11-12T09:54:11.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/125 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.125.448b58e4-655b-4e22-92f2-08d4cc9e9e1d.tmp
[2024-11-12T09:54:11.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.125.448b58e4-655b-4e22-92f2-08d4cc9e9e1d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/125
[2024-11-12T09:54:11.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO FileStreamSinkLog: Current compact batch id = 125 min compaction batch id to delete = 19
[2024-11-12T09:54:11.350+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO ManifestFileCommitProtocol: Committed batch 125
[2024-11-12T09:54:11.351+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO FileFormatWriter: Write Job 3eb15cba-872d-476d-a7ed-e72775165fc1 committed. Elapsed time: 130 ms.
[2024-11-12T09:54:11.352+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO FileFormatWriter: Finished processing stats for write job 3eb15cba-872d-476d-a7ed-e72775165fc1.
[2024-11-12T09:54:11.360+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/125 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.125.f451c728-71b9-4986-a6d3-087277841315.tmp
[2024-11-12T09:54:11.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.125.f451c728-71b9-4986-a6d3-087277841315.tmp to hdfs://namenode:9000/spark_checkpoint/commits/125
[2024-11-12T09:54:11.807+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:11.807+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:11.808+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:11.808+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:11.809+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:10.367Z",
[2024-11-12T09:54:11.809+0000] {spark_submit.py:495} INFO - "batchId" : 125,
[2024-11-12T09:54:11.809+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:11.810+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-11-12T09:54:11.810+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6954102920723227,
[2024-11-12T09:54:11.810+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:11.810+0000] {spark_submit.py:495} INFO - "addBatch" : 885,
[2024-11-12T09:54:11.810+0000] {spark_submit.py:495} INFO - "commitOffsets" : 455,
[2024-11-12T09:54:11.811+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:11.811+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:54:11.811+0000] {spark_submit.py:495} INFO - "queryPlanning" : 21,
[2024-11-12T09:54:11.811+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1438,
[2024-11-12T09:54:11.812+0000] {spark_submit.py:495} INFO - "walCommit" : 64
[2024-11-12T09:54:11.812+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:11.812+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:11.813+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:11.813+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:11.813+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:11.813+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:11.813+0000] {spark_submit.py:495} INFO - "0" : 713
[2024-11-12T09:54:11.814+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:11.814+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:11.814+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:11.814+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:11.814+0000] {spark_submit.py:495} INFO - "0" : 714
[2024-11-12T09:54:11.815+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:11.815+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:11.815+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:11.816+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:11.817+0000] {spark_submit.py:495} INFO - "0" : 714
[2024-11-12T09:54:11.817+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:11.817+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 66.66666666666667,
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6954102920723227,
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:11.818+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:11.819+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:11.819+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:11.819+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:11.819+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:11.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/126 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.126.7c5f8364-ba57-4d03-8574-f77c78be907e.tmp
[2024-11-12T09:54:11.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.126.7c5f8364-ba57-4d03-8574-f77c78be907e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/126
[2024-11-12T09:54:11.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO MicroBatchExecution: Committed offsets for batch 126. Metadata OffsetSeqMetadata(0,1731405251813,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:11.878+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:11.879+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:11.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:11.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:11.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 124, 125, 125
[2024-11-12T09:54:11.905+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:11.934+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:11.941+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Got job 126 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:11.941+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Final stage: ResultStage 126 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:11.942+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:11.942+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:11.943+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[508] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:11.967+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:11.972+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:11.973+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:11.974+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:11.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[508] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:11.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks resource profile 0
[2024-11-12T09:54:11.976+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:11 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 126) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:12.003+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:12.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 126) in 455 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:12.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool
[2024-11-12T09:54:12.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: ResultStage 126 (start at NativeMethodAccessorImpl.java:0) finished in 0.488 s
[2024-11-12T09:54:12.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Job 126 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:12.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished
[2024-11-12T09:54:12.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Job 126 finished: start at NativeMethodAccessorImpl.java:0, took 0.498443 s
[2024-11-12T09:54:12.433+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO FileFormatWriter: Start to commit write Job 8af84bba-f852-464c-b85b-32dc10a90ea9.
[2024-11-12T09:54:12.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/126 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.126.43bbff7a-1d82-4b38-9252-b503f31329be.tmp
[2024-11-12T09:54:12.494+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.126.43bbff7a-1d82-4b38-9252-b503f31329be.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/126
[2024-11-12T09:54:12.495+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO FileStreamSinkLog: Current compact batch id = 126 min compaction batch id to delete = 19
[2024-11-12T09:54:12.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO ManifestFileCommitProtocol: Committed batch 126
[2024-11-12T09:54:12.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO FileFormatWriter: Write Job 8af84bba-f852-464c-b85b-32dc10a90ea9 committed. Elapsed time: 65 ms.
[2024-11-12T09:54:12.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO FileFormatWriter: Finished processing stats for write job 8af84bba-f852-464c-b85b-32dc10a90ea9.
[2024-11-12T09:54:12.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/126 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.126.2e75d8e6-1209-4a95-8fbd-9a64aaf30961.tmp
[2024-11-12T09:54:12.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.126.2e75d8e6-1209-4a95-8fbd-9a64aaf30961.tmp to hdfs://namenode:9000/spark_checkpoint/commits/126
[2024-11-12T09:54:12.585+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:12.585+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:11.806Z",
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "batchId" : 126,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6949270326615705,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2886597938144329,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "addBatch" : 615,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "commitOffsets" : 84,
[2024-11-12T09:54:12.586+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:12.587+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:54:12.588+0000] {spark_submit.py:495} INFO - "queryPlanning" : 12,
[2024-11-12T09:54:12.588+0000] {spark_submit.py:495} INFO - "triggerExecution" : 776,
[2024-11-12T09:54:12.588+0000] {spark_submit.py:495} INFO - "walCommit" : 55
[2024-11-12T09:54:12.588+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:12.588+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:12.594+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:12.595+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:12.597+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:12.597+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:12.597+0000] {spark_submit.py:495} INFO - "0" : 714
[2024-11-12T09:54:12.597+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:12.598+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:12.598+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:12.598+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:12.598+0000] {spark_submit.py:495} INFO - "0" : 715
[2024-11-12T09:54:12.601+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:12.601+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:12.602+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:12.602+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:12.602+0000] {spark_submit.py:495} INFO - "0" : 715
[2024-11-12T09:54:12.602+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:12.602+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:12.602+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:12.603+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.6949270326615705,
[2024-11-12T09:54:12.603+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2886597938144329,
[2024-11-12T09:54:12.604+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:12.611+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:12.612+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:12.613+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:12.613+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:12.613+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:12.613+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:12.614+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:12.614+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:12.614+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:12.614+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:12.622+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/127 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.127.f13f31ee-72ec-422a-8ed1-2eabb3675d49.tmp
[2024-11-12T09:54:12.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.127.f13f31ee-72ec-422a-8ed1-2eabb3675d49.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/127
[2024-11-12T09:54:12.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO MicroBatchExecution: Committed offsets for batch 127. Metadata OffsetSeqMetadata(0,1731405252601,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:12.705+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:12.705+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:12.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:12.723+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:12.735+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 125, 126, 126
[2024-11-12T09:54:12.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:12.777+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:12.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Got job 127 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:12.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Final stage: ResultStage 127 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:12.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:12.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:12.781+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Submitting ResultStage 127 (MapPartitionsRDD[512] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:12.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:12.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:12.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:12.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:12.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[512] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:12.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO TaskSchedulerImpl: Adding task set 127.0 with 1 tasks resource profile 0
[2024-11-12T09:54:12.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 127) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:12.848+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:12 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:13.498+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 127) in 675 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:13.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool
[2024-11-12T09:54:13.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: ResultStage 127 (start at NativeMethodAccessorImpl.java:0) finished in 0.718 s
[2024-11-12T09:54:13.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:13.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 127: Stage finished
[2024-11-12T09:54:13.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Job 127 finished: start at NativeMethodAccessorImpl.java:0, took 0.724078 s
[2024-11-12T09:54:13.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO FileFormatWriter: Start to commit write Job 296a6636-3cd4-436b-aef3-7e2e3e2ac007.
[2024-11-12T09:54:13.524+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/127 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.127.258221cc-8ba3-4555-a6d6-d83e11caabff.tmp
[2024-11-12T09:54:13.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.127.258221cc-8ba3-4555-a6d6-d83e11caabff.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/127
[2024-11-12T09:54:13.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO FileStreamSinkLog: Current compact batch id = 127 min compaction batch id to delete = 19
[2024-11-12T09:54:13.589+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO ManifestFileCommitProtocol: Committed batch 127
[2024-11-12T09:54:13.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO FileFormatWriter: Write Job 296a6636-3cd4-436b-aef3-7e2e3e2ac007 committed. Elapsed time: 85 ms.
[2024-11-12T09:54:13.591+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO FileFormatWriter: Finished processing stats for write job 296a6636-3cd4-436b-aef3-7e2e3e2ac007.
[2024-11-12T09:54:13.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/127 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.127.9ac88a73-aac1-432d-bfa4-9cde92bebd8a.tmp
[2024-11-12T09:54:13.652+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.127.9ac88a73-aac1-432d-bfa4-9cde92bebd8a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/127
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:12.585Z",
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - "batchId" : 127,
[2024-11-12T09:54:13.654+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2836970474967908,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9372071227741331,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "addBatch" : 879,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "commitOffsets" : 65,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "queryPlanning" : 28,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1067,
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "walCommit" : 77
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:13.655+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "0" : 715
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - "0" : 716
[2024-11-12T09:54:13.656+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - "0" : 716
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:13.657+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2836970474967908,
[2024-11-12T09:54:13.658+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9372071227741331,
[2024-11-12T09:54:13.658+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:13.659+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:13.662+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:13.663+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:13.663+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:13.663+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:13.663+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:13.663+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:13.663+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:13.665+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:13.665+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:13.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/128 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.128.000932dc-b767-402a-9bba-0eb032120998.tmp
[2024-11-12T09:54:13.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.128.000932dc-b767-402a-9bba-0eb032120998.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/128
[2024-11-12T09:54:13.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO MicroBatchExecution: Committed offsets for batch 128. Metadata OffsetSeqMetadata(0,1731405253670,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:13.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:13.774+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:13.798+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:13.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:13.810+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 126, 127, 127
[2024-11-12T09:54:13.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:13.871+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:13.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Got job 128 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:13.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Final stage: ResultStage 128 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:13.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:13.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:13.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:13.903+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:13.911+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:13.912+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:13.912+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:13.913+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[516] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:13.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks resource profile 0
[2024-11-12T09:54:13.918+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 128) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:13.950+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:13 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:14.465+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 128) in 545 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:14.467+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool
[2024-11-12T09:54:14.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO DAGScheduler: ResultStage 128 (start at NativeMethodAccessorImpl.java:0) finished in 0.592 s
[2024-11-12T09:54:14.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:14.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished
[2024-11-12T09:54:14.468+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO DAGScheduler: Job 128 finished: start at NativeMethodAccessorImpl.java:0, took 0.597483 s
[2024-11-12T09:54:14.469+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO FileFormatWriter: Start to commit write Job bbeeba40-a78d-439f-bfe3-549dca6af056.
[2024-11-12T09:54:14.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/128 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.128.f60b6640-1cb8-43fa-b569-20a475a98ac7.tmp
[2024-11-12T09:54:14.522+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.128.f60b6640-1cb8-43fa-b569-20a475a98ac7.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/128
[2024-11-12T09:54:14.522+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO FileStreamSinkLog: Current compact batch id = 128 min compaction batch id to delete = 19
[2024-11-12T09:54:14.524+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO ManifestFileCommitProtocol: Committed batch 128
[2024-11-12T09:54:14.524+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO FileFormatWriter: Write Job bbeeba40-a78d-439f-bfe3-549dca6af056 committed. Elapsed time: 54 ms.
[2024-11-12T09:54:14.525+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO FileFormatWriter: Finished processing stats for write job bbeeba40-a78d-439f-bfe3-549dca6af056.
[2024-11-12T09:54:14.536+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/128 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.128.2ff2ddc0-bbe4-4621-b440-ef6a4e750180.tmp
[2024-11-12T09:54:14.994+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.128.2ff2ddc0-bbe4-4621-b440-ef6a4e750180.tmp to hdfs://namenode:9000/spark_checkpoint/commits/128
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:13.653Z",
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - "batchId" : 128,
[2024-11-12T09:54:14.996+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:14.997+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9363295880149812,
[2024-11-12T09:54:14.997+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7457121551081283,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "addBatch" : 743,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "commitOffsets" : 470,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "queryPlanning" : 29,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1341,
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "walCommit" : 73
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:14.998+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - "0" : 716
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - "0" : 717
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:14.999+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:15.000+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:15.003+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:15.003+0000] {spark_submit.py:495} INFO - "0" : 717
[2024-11-12T09:54:15.003+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:15.004+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:15.004+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:15.005+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9363295880149812,
[2024-11-12T09:54:15.007+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7457121551081283,
[2024-11-12T09:54:15.007+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:15.008+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:15.009+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:15.010+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:15.010+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:15.010+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:15.012+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:15.012+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:15.012+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:15.012+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:15.012+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:15.014+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/129 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.129.e9f6eeca-a934-4589-a397-80b15c60b716.tmp
[2024-11-12T09:54:15.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.129.e9f6eeca-a934-4589-a397-80b15c60b716.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/129
[2024-11-12T09:54:15.049+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO MicroBatchExecution: Committed offsets for batch 129. Metadata OffsetSeqMetadata(0,1731405255004,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:15.066+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:15.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:15.076+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:15.081+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:15.091+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 128, 128
[2024-11-12T09:54:15.092+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:15.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:15.117+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Got job 129 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:15.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Final stage: ResultStage 129 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:15.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:15.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:15.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[520] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:15.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:15.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:15.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:15.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:15.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[520] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:15.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks resource profile 0
[2024-11-12T09:54:15.154+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 129) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:15.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:15.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 129) in 716 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:15.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool
[2024-11-12T09:54:15.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: ResultStage 129 (start at NativeMethodAccessorImpl.java:0) finished in 0.754 s
[2024-11-12T09:54:15.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:15.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished
[2024-11-12T09:54:15.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO DAGScheduler: Job 129 finished: start at NativeMethodAccessorImpl.java:0, took 0.760534 s
[2024-11-12T09:54:15.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO FileFormatWriter: Start to commit write Job 30367d3b-fd6d-44dc-8967-571476769902.
[2024-11-12T09:54:15.890+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:15 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/129.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.129.compact.c86732a6-32a1-47c6-840c-9de6b7abd482.tmp
[2024-11-12T09:54:16.180+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.129.compact.c86732a6-32a1-47c6-840c-9de6b7abd482.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/129.compact
[2024-11-12T09:54:16.181+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileStreamSinkLog: Current compact batch id = 129 min compaction batch id to delete = 29
[2024-11-12T09:54:16.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO ManifestFileCommitProtocol: Committed batch 129
[2024-11-12T09:54:16.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileFormatWriter: Write Job 30367d3b-fd6d-44dc-8967-571476769902 committed. Elapsed time: 311 ms.
[2024-11-12T09:54:16.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileFormatWriter: Finished processing stats for write job 30367d3b-fd6d-44dc-8967-571476769902.
[2024-11-12T09:54:16.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/129 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.129.994e59b1-1314-4ea6-ad58-ee7a917f9005.tmp
[2024-11-12T09:54:16.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.129.994e59b1-1314-4ea6-ad58-ee7a917f9005.tmp to hdfs://namenode:9000/spark_checkpoint/commits/129
[2024-11-12T09:54:16.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:16.263+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:14.995Z",
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "batchId" : 129,
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7451564828614009,
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7917656373713381,
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:16.264+0000] {spark_submit.py:495} INFO - "addBatch" : 1118,
[2024-11-12T09:54:16.265+0000] {spark_submit.py:495} INFO - "commitOffsets" : 70,
[2024-11-12T09:54:16.265+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:16.265+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:54:16.265+0000] {spark_submit.py:495} INFO - "queryPlanning" : 19,
[2024-11-12T09:54:16.265+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1263,
[2024-11-12T09:54:16.267+0000] {spark_submit.py:495} INFO - "walCommit" : 44
[2024-11-12T09:54:16.267+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.267+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:16.267+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:16.267+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:16.269+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:16.270+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:16.275+0000] {spark_submit.py:495} INFO - "0" : 717
[2024-11-12T09:54:16.276+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.276+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.276+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:16.276+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:16.276+0000] {spark_submit.py:495} INFO - "0" : 718
[2024-11-12T09:54:16.276+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.277+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.277+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:16.277+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:16.278+0000] {spark_submit.py:495} INFO - "0" : 718
[2024-11-12T09:54:16.278+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.278+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.278+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:16.278+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7451564828614009,
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7917656373713381,
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.279+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.284+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/130 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.130.3c5abf14-dd1d-4d00-baaa-338bbad6ae1f.tmp
[2024-11-12T09:54:16.334+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.130.3c5abf14-dd1d-4d00-baaa-338bbad6ae1f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/130
[2024-11-12T09:54:16.335+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MicroBatchExecution: Committed offsets for batch 130. Metadata OffsetSeqMetadata(0,1731405256268,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:16.353+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.354+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.391+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 128, 128, 129
[2024-11-12T09:54:16.400+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:16.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:16.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Got job 130 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:16.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Final stage: ResultStage 130 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:16.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:16.434+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:16.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[524] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:16.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_127_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:16.469+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:16.470+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:16.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:16.480+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:16.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:16.481+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[524] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:16.482+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSchedulerImpl: Adding task set 130.0 with 1 tasks resource profile 0
[2024-11-12T09:54:16.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_126_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:16.484+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSetManager: Starting task 0.0 in stage 130.0 (TID 130) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:16.487+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:16.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_129_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:16.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:16.511+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:16.522+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_125_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:16.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:16.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_128_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:16.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:16.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSetManager: Finished task 0.0 in stage 130.0 (TID 130) in 138 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:16.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool
[2024-11-12T09:54:16.624+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: ResultStage 130 (start at NativeMethodAccessorImpl.java:0) finished in 0.192 s
[2024-11-12T09:54:16.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:16.626+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 130: Stage finished
[2024-11-12T09:54:16.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Job 130 finished: start at NativeMethodAccessorImpl.java:0, took 0.195282 s
[2024-11-12T09:54:16.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileFormatWriter: Start to commit write Job 86887a76-9f83-4266-8ee3-14f697ebd2e0.
[2024-11-12T09:54:16.636+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/130 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.130.12482e48-f1c6-43de-88cf-09f62a1d7a2f.tmp
[2024-11-12T09:54:16.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.130.12482e48-f1c6-43de-88cf-09f62a1d7a2f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/130
[2024-11-12T09:54:16.671+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileStreamSinkLog: Current compact batch id = 130 min compaction batch id to delete = 29
[2024-11-12T09:54:16.674+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO ManifestFileCommitProtocol: Committed batch 130
[2024-11-12T09:54:16.675+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileFormatWriter: Write Job 86887a76-9f83-4266-8ee3-14f697ebd2e0 committed. Elapsed time: 47 ms.
[2024-11-12T09:54:16.675+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileFormatWriter: Finished processing stats for write job 86887a76-9f83-4266-8ee3-14f697ebd2e0.
[2024-11-12T09:54:16.691+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/130 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.130.d5836729-1a5a-48cf-af71-290bba2c8a13.tmp
[2024-11-12T09:54:16.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.130.d5836729-1a5a-48cf-af71-290bba2c8a13.tmp to hdfs://namenode:9000/spark_checkpoint/commits/130
[2024-11-12T09:54:16.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:16.731+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:16.731+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:16.732+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:16.736+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:16.262Z",
[2024-11-12T09:54:16.736+0000] {spark_submit.py:495} INFO - "batchId" : 130,
[2024-11-12T09:54:16.736+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:16.736+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7892659826361484,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.141327623126338,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "addBatch" : 307,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "commitOffsets" : 55,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "queryPlanning" : 30,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "triggerExecution" : 467,
[2024-11-12T09:54:16.737+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:54:16.739+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.739+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:16.739+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:16.739+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:16.739+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - "0" : 718
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - "0" : 719
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.740+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:16.741+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:16.741+0000] {spark_submit.py:495} INFO - "0" : 719
[2024-11-12T09:54:16.741+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.741+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:16.742+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:16.742+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7892659826361484,
[2024-11-12T09:54:16.742+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.141327623126338,
[2024-11-12T09:54:16.744+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:16.744+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:16.744+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:16.745+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:16.745+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.745+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:16.746+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:16.746+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:16.746+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:16.746+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.746+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:16.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/131 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.131.8387eecb-1716-4180-aae0-c8a451c23229.tmp
[2024-11-12T09:54:16.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.131.8387eecb-1716-4180-aae0-c8a451c23229.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/131
[2024-11-12T09:54:16.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MicroBatchExecution: Committed offsets for batch 131. Metadata OffsetSeqMetadata(0,1731405256740,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:16.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:16.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 128, 129, 130, 130
[2024-11-12T09:54:16.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:16.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:16.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Got job 131 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:16.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Final stage: ResultStage 131 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:16.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:16.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:16.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[528] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:16.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:16.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:16.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:16.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:16.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[528] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:16.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks resource profile 0
[2024-11-12T09:54:16.903+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 131) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:16.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:16 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:17.498+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 131) in 595 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:17.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool
[2024-11-12T09:54:17.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO DAGScheduler: ResultStage 131 (start at NativeMethodAccessorImpl.java:0) finished in 0.629 s
[2024-11-12T09:54:17.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:17.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished
[2024-11-12T09:54:17.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO DAGScheduler: Job 131 finished: start at NativeMethodAccessorImpl.java:0, took 0.633081 s
[2024-11-12T09:54:17.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO FileFormatWriter: Start to commit write Job ed1e9783-e131-45da-b5c7-2db95a9ec65e.
[2024-11-12T09:54:17.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/131 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.131.a82be6d9-072b-4d7b-8f7a-2f50adb8834d.tmp
[2024-11-12T09:54:17.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.131.a82be6d9-072b-4d7b-8f7a-2f50adb8834d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/131
[2024-11-12T09:54:17.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO FileStreamSinkLog: Current compact batch id = 131 min compaction batch id to delete = 29
[2024-11-12T09:54:17.558+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO ManifestFileCommitProtocol: Committed batch 131
[2024-11-12T09:54:17.559+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO FileFormatWriter: Write Job ed1e9783-e131-45da-b5c7-2db95a9ec65e committed. Elapsed time: 56 ms.
[2024-11-12T09:54:17.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO FileFormatWriter: Finished processing stats for write job ed1e9783-e131-45da-b5c7-2db95a9ec65e.
[2024-11-12T09:54:17.572+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/131 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.131.30b51a41-53cb-49ae-96d6-f73df1f709a3.tmp
[2024-11-12T09:54:17.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.131.30b51a41-53cb-49ae-96d6-f73df1f709a3.tmp to hdfs://namenode:9000/spark_checkpoint/commits/131
[2024-11-12T09:54:17.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:17.618+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:17.618+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:17.619+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:17.619+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:16.730Z",
[2024-11-12T09:54:17.619+0000] {spark_submit.py:495} INFO - "batchId" : 131,
[2024-11-12T09:54:17.619+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:17.619+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.1367521367521367,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1286681715575622,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "addBatch" : 751,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "commitOffsets" : 57,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "triggerExecution" : 886,
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "walCommit" : 44
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:17.620+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - "0" : 719
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:17.621+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - "0" : 720
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - "0" : 720
[2024-11-12T09:54:17.622+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:17.623+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:17.623+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:17.623+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.1367521367521367,
[2024-11-12T09:54:17.623+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1286681715575622,
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:17.624+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:17.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:17 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/132 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.132.58d47949-f577-4d21-83f3-5f9ecde56b3f.tmp
[2024-11-12T09:54:18.083+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.132.58d47949-f577-4d21-83f3-5f9ecde56b3f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/132
[2024-11-12T09:54:18.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MicroBatchExecution: Committed offsets for batch 132. Metadata OffsetSeqMetadata(0,1731405257631,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:18.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.138+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 130, 131, 131
[2024-11-12T09:54:18.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:18.179+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:18.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Got job 132 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:18.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Final stage: ResultStage 132 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:18.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:18.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:18.190+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[532] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:18.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:18.220+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:18.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:18.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:18.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[532] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:18.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSchedulerImpl: Adding task set 132.0 with 1 tasks resource profile 0
[2024-11-12T09:54:18.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 132) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:18.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:18.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 132) in 283 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:18.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool
[2024-11-12T09:54:18.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: ResultStage 132 (start at NativeMethodAccessorImpl.java:0) finished in 0.324 s
[2024-11-12T09:54:18.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:18.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished
[2024-11-12T09:54:18.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Job 132 finished: start at NativeMethodAccessorImpl.java:0, took 0.334902 s
[2024-11-12T09:54:18.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO FileFormatWriter: Start to commit write Job 1ef403c3-733d-4742-af6d-39163e5b932d.
[2024-11-12T09:54:18.528+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/132 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.132.673244e0-84a6-4a90-9d3e-001f83b30bc5.tmp
[2024-11-12T09:54:18.587+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.132.673244e0-84a6-4a90-9d3e-001f83b30bc5.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/132
[2024-11-12T09:54:18.590+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO FileStreamSinkLog: Current compact batch id = 132 min compaction batch id to delete = 29
[2024-11-12T09:54:18.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO ManifestFileCommitProtocol: Committed batch 132
[2024-11-12T09:54:18.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO FileFormatWriter: Write Job 1ef403c3-733d-4742-af6d-39163e5b932d committed. Elapsed time: 77 ms.
[2024-11-12T09:54:18.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO FileFormatWriter: Finished processing stats for write job 1ef403c3-733d-4742-af6d-39163e5b932d.
[2024-11-12T09:54:18.601+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/132 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.132.e5300716-4a2e-4a75-b4ac-ddbbd7258b72.tmp
[2024-11-12T09:54:18.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.132.e5300716-4a2e-4a75-b4ac-ddbbd7258b72.tmp to hdfs://namenode:9000/spark_checkpoint/commits/132
[2024-11-12T09:54:18.653+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:18.654+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:18.654+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:18.655+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:18.655+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:17.618Z",
[2024-11-12T09:54:18.656+0000] {spark_submit.py:495} INFO - "batchId" : 132,
[2024-11-12T09:54:18.656+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:18.657+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1261261261261262,
[2024-11-12T09:54:18.657+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.970873786407767,
[2024-11-12T09:54:18.657+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:18.657+0000] {spark_submit.py:495} INFO - "addBatch" : 482,
[2024-11-12T09:54:18.657+0000] {spark_submit.py:495} INFO - "commitOffsets" : 55,
[2024-11-12T09:54:18.658+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:18.658+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:54:18.659+0000] {spark_submit.py:495} INFO - "queryPlanning" : 24,
[2024-11-12T09:54:18.659+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1030,
[2024-11-12T09:54:18.659+0000] {spark_submit.py:495} INFO - "walCommit" : 453
[2024-11-12T09:54:18.660+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:18.660+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:18.660+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:18.660+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:18.665+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:18.668+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:18.668+0000] {spark_submit.py:495} INFO - "0" : 720
[2024-11-12T09:54:18.668+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:18.668+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:18.668+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:18.668+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - "0" : 721
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - "0" : 721
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:18.669+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.1261261261261262,
[2024-11-12T09:54:18.673+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.970873786407767,
[2024-11-12T09:54:18.674+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:18.674+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:18.674+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:18.679+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:18.680+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:18.680+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:18.680+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:18.682+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:18.682+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:18.682+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:18.682+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:18.682+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/133 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.133.1154c614-f653-4286-8455-70fdb812ff0f.tmp
[2024-11-12T09:54:18.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.133.1154c614-f653-4286-8455-70fdb812ff0f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/133
[2024-11-12T09:54:18.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MicroBatchExecution: Committed offsets for batch 133. Metadata OffsetSeqMetadata(0,1731405258664,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:18.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.759+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.761+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:18.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 131, 132, 132
[2024-11-12T09:54:18.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:18.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:18.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Got job 133 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:18.803+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Final stage: ResultStage 133 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:18.803+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:18.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:18.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Submitting ResultStage 133 (MapPartitionsRDD[536] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:18.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:18.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:18.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:18.830+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:18.830+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 133 (MapPartitionsRDD[536] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:18.831+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks resource profile 0
[2024-11-12T09:54:18.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 133) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:18.852+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:18 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:19.472+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 133) in 640 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:19.473+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool
[2024-11-12T09:54:19.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: ResultStage 133 (start at NativeMethodAccessorImpl.java:0) finished in 0.671 s
[2024-11-12T09:54:19.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:19.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 133: Stage finished
[2024-11-12T09:54:19.475+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Job 133 finished: start at NativeMethodAccessorImpl.java:0, took 0.673270 s
[2024-11-12T09:54:19.478+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO FileFormatWriter: Start to commit write Job 5fc36f84-3623-4bb8-a92d-97602461900b.
[2024-11-12T09:54:19.490+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/133 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.133.0b04711f-75d1-4a12-9cc1-bd1e029cb0f7.tmp
[2024-11-12T09:54:19.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.133.0b04711f-75d1-4a12-9cc1-bd1e029cb0f7.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/133
[2024-11-12T09:54:19.553+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO FileStreamSinkLog: Current compact batch id = 133 min compaction batch id to delete = 29
[2024-11-12T09:54:19.561+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO ManifestFileCommitProtocol: Committed batch 133
[2024-11-12T09:54:19.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO FileFormatWriter: Write Job 5fc36f84-3623-4bb8-a92d-97602461900b committed. Elapsed time: 81 ms.
[2024-11-12T09:54:19.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO FileFormatWriter: Finished processing stats for write job 5fc36f84-3623-4bb8-a92d-97602461900b.
[2024-11-12T09:54:19.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/133 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.133.194e8008-3c6e-403d-b117-361fa6eafc01.tmp
[2024-11-12T09:54:19.631+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.133.194e8008-3c6e-403d-b117-361fa6eafc01.tmp to hdfs://namenode:9000/spark_checkpoint/commits/133
[2024-11-12T09:54:19.634+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:19.634+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:18.651Z",
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "batchId" : 133,
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9680542110358181,
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0214504596527068,
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:19.636+0000] {spark_submit.py:495} INFO - "addBatch" : 809,
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "commitOffsets" : 72,
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "triggerExecution" : 979,
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "walCommit" : 59
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:19.637+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "0" : 721
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - "0" : 722
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:19.638+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:19.639+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:19.639+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:19.639+0000] {spark_submit.py:495} INFO - "0" : 722
[2024-11-12T09:54:19.639+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9680542110358181,
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0214504596527068,
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:19.640+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:19.641+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:19.641+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:19.641+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:19.641+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:19.642+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:19.642+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:19.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/134 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.134.0e3d758c-67e0-4334-a61d-57ae393842ee.tmp
[2024-11-12T09:54:19.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.134.0e3d758c-67e0-4334-a61d-57ae393842ee.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/134
[2024-11-12T09:54:19.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO MicroBatchExecution: Committed offsets for batch 134. Metadata OffsetSeqMetadata(0,1731405259640,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:19.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:19.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:19.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:19.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:19.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 133
[2024-11-12T09:54:19.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:19.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:19.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Got job 134 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:19.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Final stage: ResultStage 134 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:19.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:19.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:19.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[540] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:19.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:19.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:19.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_132_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:19.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:19.838+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:19.842+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[540] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:19.844+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks resource profile 0
[2024-11-12T09:54:19.847+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:19.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 134) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:19.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_131_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:19.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:19.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_130_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:19.911+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:19.913+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:19.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_133_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:19.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:19 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:20.494+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 134) in 647 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:20.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool
[2024-11-12T09:54:20.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: ResultStage 134 (start at NativeMethodAccessorImpl.java:0) finished in 0.704 s
[2024-11-12T09:54:20.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:20.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished
[2024-11-12T09:54:20.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Job 134 finished: start at NativeMethodAccessorImpl.java:0, took 0.709014 s
[2024-11-12T09:54:20.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO FileFormatWriter: Start to commit write Job 58829486-71e2-4fdb-9b46-1de7710aea1a.
[2024-11-12T09:54:20.516+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/134 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.134.187cc13e-973d-4a75-9855-4e399dff7903.tmp
[2024-11-12T09:54:20.556+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.134.187cc13e-973d-4a75-9855-4e399dff7903.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/134
[2024-11-12T09:54:20.557+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO FileStreamSinkLog: Current compact batch id = 134 min compaction batch id to delete = 29
[2024-11-12T09:54:20.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO ManifestFileCommitProtocol: Committed batch 134
[2024-11-12T09:54:20.561+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO FileFormatWriter: Write Job 58829486-71e2-4fdb-9b46-1de7710aea1a committed. Elapsed time: 60 ms.
[2024-11-12T09:54:20.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO FileFormatWriter: Finished processing stats for write job 58829486-71e2-4fdb-9b46-1de7710aea1a.
[2024-11-12T09:54:20.569+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/134 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.134.2061e8d4-c425-4788-be3b-0f4a13ab3f55.tmp
[2024-11-12T09:54:20.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.134.2061e8d4-c425-4788-be3b-0f4a13ab3f55.tmp to hdfs://namenode:9000/spark_checkpoint/commits/134
[2024-11-12T09:54:20.615+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:20.615+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:20.615+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:20.616+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:20.616+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:19.634Z",
[2024-11-12T09:54:20.616+0000] {spark_submit.py:495} INFO - "batchId" : 134,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.017293997965412,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0204081632653061,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "addBatch" : 835,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "commitOffsets" : 53,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "queryPlanning" : 27,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "triggerExecution" : 980,
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - "walCommit" : 55
[2024-11-12T09:54:20.617+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - "0" : 722
[2024-11-12T09:54:20.618+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - "0" : 723
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - "0" : 723
[2024-11-12T09:54:20.619+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.017293997965412,
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0204081632653061,
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:20.620+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:20.621+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:20.621+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:20.621+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:20.621+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:20.621+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:20.621+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:20.622+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:20.622+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:20.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/135 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.135.2f24ac23-74f6-4cb9-be26-227fddfe7d1a.tmp
[2024-11-12T09:54:20.668+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.135.2f24ac23-74f6-4cb9-be26-227fddfe7d1a.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/135
[2024-11-12T09:54:20.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO MicroBatchExecution: Committed offsets for batch 135. Metadata OffsetSeqMetadata(0,1731405260620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:20.689+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:20.690+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:20.697+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:20.699+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:20.705+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 133, 134, 134
[2024-11-12T09:54:20.709+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:20.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:20.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Got job 135 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:20.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Final stage: ResultStage 135 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:20.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:20.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:20.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:20.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:20.784+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:20.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:20.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:20.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:20.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks resource profile 0
[2024-11-12T09:54:20.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 135) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:20.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:20 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:21.429+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 135) in 634 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:21.429+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool
[2024-11-12T09:54:21.430+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: ResultStage 135 (start at NativeMethodAccessorImpl.java:0) finished in 0.679 s
[2024-11-12T09:54:21.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Job 135 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:21.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished
[2024-11-12T09:54:21.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Job 135 finished: start at NativeMethodAccessorImpl.java:0, took 0.681786 s
[2024-11-12T09:54:21.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO FileFormatWriter: Start to commit write Job ccf54027-a585-446c-a02d-0c09888ca194.
[2024-11-12T09:54:21.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/135 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.135.799effda-a43b-4029-9661-ca0a073713cc.tmp
[2024-11-12T09:54:21.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.135.799effda-a43b-4029-9661-ca0a073713cc.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/135
[2024-11-12T09:54:21.504+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO FileStreamSinkLog: Current compact batch id = 135 min compaction batch id to delete = 29
[2024-11-12T09:54:21.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO ManifestFileCommitProtocol: Committed batch 135
[2024-11-12T09:54:21.508+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO FileFormatWriter: Write Job ccf54027-a585-446c-a02d-0c09888ca194 committed. Elapsed time: 74 ms.
[2024-11-12T09:54:21.509+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO FileFormatWriter: Finished processing stats for write job ccf54027-a585-446c-a02d-0c09888ca194.
[2024-11-12T09:54:21.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/135 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.135.641a904e-e094-4488-8d84-d0444da2129d.tmp
[2024-11-12T09:54:21.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.135.641a904e-e094-4488-8d84-d0444da2129d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/135
[2024-11-12T09:54:21.569+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:21.569+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:21.569+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:20.615Z",
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "batchId" : 135,
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.019367991845056,
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0493179433368311,
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "addBatch" : 816,
[2024-11-12T09:54:21.570+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-12T09:54:21.571+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:21.571+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:54:21.571+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:54:21.571+0000] {spark_submit.py:495} INFO - "triggerExecution" : 953,
[2024-11-12T09:54:21.573+0000] {spark_submit.py:495} INFO - "walCommit" : 48
[2024-11-12T09:54:21.575+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:21.576+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:21.576+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:21.576+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:21.577+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:21.577+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:21.577+0000] {spark_submit.py:495} INFO - "0" : 723
[2024-11-12T09:54:21.578+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:21.579+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:21.580+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:21.580+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:21.580+0000] {spark_submit.py:495} INFO - "0" : 724
[2024-11-12T09:54:21.580+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - "0" : 724
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.019367991845056,
[2024-11-12T09:54:21.581+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0493179433368311,
[2024-11-12T09:54:21.584+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:21.585+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:21.585+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:21.585+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:21.585+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:21.585+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:21.586+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:21.586+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:21.586+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:21.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:21.586+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:21.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/136 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.136.5fa0e61f-417c-4f63-a8c9-3b79a3e7b464.tmp
[2024-11-12T09:54:21.641+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.136.5fa0e61f-417c-4f63-a8c9-3b79a3e7b464.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/136
[2024-11-12T09:54:21.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO MicroBatchExecution: Committed offsets for batch 136. Metadata OffsetSeqMetadata(0,1731405261576,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:21.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:21.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:21.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:21.678+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:21.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 134, 135, 135
[2024-11-12T09:54:21.692+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:21.717+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:21.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Got job 136 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:21.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Final stage: ResultStage 136 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:21.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:21.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:21.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Submitting ResultStage 136 (MapPartitionsRDD[548] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:21.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:21.740+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:21.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:21.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:21.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 136 (MapPartitionsRDD[548] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:21.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO TaskSchedulerImpl: Adding task set 136.0 with 1 tasks resource profile 0
[2024-11-12T09:54:21.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO TaskSetManager: Starting task 0.0 in stage 136.0 (TID 136) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:21.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:21 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:22.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO TaskSetManager: Finished task 0.0 in stage 136.0 (TID 136) in 632 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:22.376+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool
[2024-11-12T09:54:22.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO DAGScheduler: ResultStage 136 (start at NativeMethodAccessorImpl.java:0) finished in 0.657 s
[2024-11-12T09:54:22.378+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:22.378+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 136: Stage finished
[2024-11-12T09:54:22.379+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO DAGScheduler: Job 136 finished: start at NativeMethodAccessorImpl.java:0, took 0.659254 s
[2024-11-12T09:54:22.380+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO FileFormatWriter: Start to commit write Job 811ffc6f-183b-4e36-8765-23240184061f.
[2024-11-12T09:54:22.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/136 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.136.bfc2a0c8-84d3-483a-a762-31a60e5939f6.tmp
[2024-11-12T09:54:22.439+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.136.bfc2a0c8-84d3-483a-a762-31a60e5939f6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/136
[2024-11-12T09:54:22.440+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO FileStreamSinkLog: Current compact batch id = 136 min compaction batch id to delete = 29
[2024-11-12T09:54:22.448+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO ManifestFileCommitProtocol: Committed batch 136
[2024-11-12T09:54:22.450+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO FileFormatWriter: Write Job 811ffc6f-183b-4e36-8765-23240184061f committed. Elapsed time: 70 ms.
[2024-11-12T09:54:22.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO FileFormatWriter: Finished processing stats for write job 811ffc6f-183b-4e36-8765-23240184061f.
[2024-11-12T09:54:22.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/136 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.136.4fc58035-5c1f-441c-b706-9ea99580352b.tmp
[2024-11-12T09:54:22.904+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.136.4fc58035-5c1f-441c-b706-9ea99580352b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/136
[2024-11-12T09:54:22.905+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:22.906+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:22.906+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:22.906+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:22.906+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:21.569Z",
[2024-11-12T09:54:22.907+0000] {spark_submit.py:495} INFO - "batchId" : 136,
[2024-11-12T09:54:22.907+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:22.908+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0482180293501049,
[2024-11-12T09:54:22.908+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7490636704119851,
[2024-11-12T09:54:22.908+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:22.908+0000] {spark_submit.py:495} INFO - "addBatch" : 783,
[2024-11-12T09:54:22.909+0000] {spark_submit.py:495} INFO - "commitOffsets" : 454,
[2024-11-12T09:54:22.909+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:22.909+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:54:22.910+0000] {spark_submit.py:495} INFO - "queryPlanning" : 23,
[2024-11-12T09:54:22.910+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1335,
[2024-11-12T09:54:22.910+0000] {spark_submit.py:495} INFO - "walCommit" : 66
[2024-11-12T09:54:22.910+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:22.911+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:22.911+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:22.911+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:22.912+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:22.912+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:22.912+0000] {spark_submit.py:495} INFO - "0" : 724
[2024-11-12T09:54:22.912+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:22.913+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:22.913+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:22.913+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:22.913+0000] {spark_submit.py:495} INFO - "0" : 725
[2024-11-12T09:54:22.914+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:22.914+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:22.914+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:22.914+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:22.915+0000] {spark_submit.py:495} INFO - "0" : 725
[2024-11-12T09:54:22.915+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:22.917+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:22.918+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:22.918+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0482180293501049,
[2024-11-12T09:54:22.918+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7490636704119851,
[2024-11-12T09:54:22.918+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:22.919+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:22.919+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:22.919+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:22.919+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:22.919+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:22.919+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:22.920+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:22.920+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:22.921+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:22.921+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:22.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/137 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.137.c60abbdf-6f1f-4416-9258-340ae173eadd.tmp
[2024-11-12T09:54:22.957+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.137.c60abbdf-6f1f-4416-9258-340ae173eadd.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/137
[2024-11-12T09:54:22.959+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO MicroBatchExecution: Committed offsets for batch 137. Metadata OffsetSeqMetadata(0,1731405262910,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:22.982+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:22.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:22.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:22.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:23.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 135, 136, 136
[2024-11-12T09:54:23.003+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:23.046+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:23.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Got job 137 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:23.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Final stage: ResultStage 137 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:23.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:23.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:23.048+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[552] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:23.068+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:23.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:23.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:23.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:23.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 137 (MapPartitionsRDD[552] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:23.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO TaskSchedulerImpl: Adding task set 137.0 with 1 tasks resource profile 0
[2024-11-12T09:54:23.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 137) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:23.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:23.517+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 137) in 441 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:23.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool
[2024-11-12T09:54:23.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: ResultStage 137 (start at NativeMethodAccessorImpl.java:0) finished in 0.469 s
[2024-11-12T09:54:23.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:23.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished
[2024-11-12T09:54:23.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO DAGScheduler: Job 137 finished: start at NativeMethodAccessorImpl.java:0, took 0.472487 s
[2024-11-12T09:54:23.521+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO FileFormatWriter: Start to commit write Job bcbfb287-6c05-44c6-ae9f-115479dec92c.
[2024-11-12T09:54:23.528+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/137 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.137.006e5ff1-c09e-41aa-a1ba-2a71b6b5736b.tmp
[2024-11-12T09:54:23.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.137.006e5ff1-c09e-41aa-a1ba-2a71b6b5736b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/137
[2024-11-12T09:54:23.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO FileStreamSinkLog: Current compact batch id = 137 min compaction batch id to delete = 29
[2024-11-12T09:54:23.566+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO ManifestFileCommitProtocol: Committed batch 137
[2024-11-12T09:54:23.567+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO FileFormatWriter: Write Job bcbfb287-6c05-44c6-ae9f-115479dec92c committed. Elapsed time: 45 ms.
[2024-11-12T09:54:23.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO FileFormatWriter: Finished processing stats for write job bcbfb287-6c05-44c6-ae9f-115479dec92c.
[2024-11-12T09:54:23.586+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/137 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.137.79395621-f62d-472a-849e-b5f80a406d6c.tmp
[2024-11-12T09:54:23.623+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.137.79395621-f62d-472a-849e-b5f80a406d6c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/137
[2024-11-12T09:54:23.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:23.626+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:23.626+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:23.626+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:23.626+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:22.905Z",
[2024-11-12T09:54:23.626+0000] {spark_submit.py:495} INFO - "batchId" : 137,
[2024-11-12T09:54:23.627+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:23.627+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.748502994011976,
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.392757660167131,
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "addBatch" : 580,
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "commitOffsets" : 57,
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "latestOffset" : 5,
[2024-11-12T09:54:23.628+0000] {spark_submit.py:495} INFO - "queryPlanning" : 26,
[2024-11-12T09:54:23.630+0000] {spark_submit.py:495} INFO - "triggerExecution" : 718,
[2024-11-12T09:54:23.630+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:54:23.630+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:23.632+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:23.632+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:23.632+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:23.633+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:23.633+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:23.633+0000] {spark_submit.py:495} INFO - "0" : 725
[2024-11-12T09:54:23.633+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:23.634+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:23.634+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:23.634+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:23.635+0000] {spark_submit.py:495} INFO - "0" : 726
[2024-11-12T09:54:23.635+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:23.635+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:23.636+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:23.636+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:23.637+0000] {spark_submit.py:495} INFO - "0" : 726
[2024-11-12T09:54:23.637+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:23.637+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:23.637+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:23.638+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.748502994011976,
[2024-11-12T09:54:23.638+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.392757660167131,
[2024-11-12T09:54:23.638+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:23.638+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:23.638+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:23.643+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:23.643+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:23.644+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:23.644+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:23.644+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:23.644+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:23.644+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:23.645+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:23.651+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/138 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.138.73f23f8a-ec4c-4f7a-9366-d9a411d91e00.tmp
[2024-11-12T09:54:24.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.138.73f23f8a-ec4c-4f7a-9366-d9a411d91e00.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/138
[2024-11-12T09:54:24.100+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO MicroBatchExecution: Committed offsets for batch 138. Metadata OffsetSeqMetadata(0,1731405263634,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:24.117+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:24.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:24.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:24.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:24.152+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 136, 137, 137
[2024-11-12T09:54:24.158+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:24.191+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:24.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Got job 138 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:24.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Final stage: ResultStage 138 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:24.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:24.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:24.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[556] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:24.227+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:24.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:24.233+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:24.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:24.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[556] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:24.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO TaskSchedulerImpl: Adding task set 138.0 with 1 tasks resource profile 0
[2024-11-12T09:54:24.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 138) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:24.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:24.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 138) in 299 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:24.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool
[2024-11-12T09:54:24.539+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: ResultStage 138 (start at NativeMethodAccessorImpl.java:0) finished in 0.344 s
[2024-11-12T09:54:24.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:24.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished
[2024-11-12T09:54:24.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO DAGScheduler: Job 138 finished: start at NativeMethodAccessorImpl.java:0, took 0.347643 s
[2024-11-12T09:54:24.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO FileFormatWriter: Start to commit write Job 086bba2f-6d37-4e24-97df-6e576d9a0116.
[2024-11-12T09:54:24.552+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/138 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.138.33d4a5af-d589-4789-9fb5-d86cb20ba4be.tmp
[2024-11-12T09:54:24.996+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.138.33d4a5af-d589-4789-9fb5-d86cb20ba4be.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/138
[2024-11-12T09:54:24.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:24 INFO FileStreamSinkLog: Current compact batch id = 138 min compaction batch id to delete = 29
[2024-11-12T09:54:25.009+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO ManifestFileCommitProtocol: Committed batch 138
[2024-11-12T09:54:25.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO FileFormatWriter: Write Job 086bba2f-6d37-4e24-97df-6e576d9a0116 committed. Elapsed time: 468 ms.
[2024-11-12T09:54:25.015+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO FileFormatWriter: Finished processing stats for write job 086bba2f-6d37-4e24-97df-6e576d9a0116.
[2024-11-12T09:54:25.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/138 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.138.8b455438-f70c-4bb4-94e7-3b31ea5f768e.tmp
[2024-11-12T09:54:25.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.138.8b455438-f70c-4bb4-94e7-3b31ea5f768e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/138
[2024-11-12T09:54:25.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:23.625Z",
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "batchId" : 138,
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3888888888888888,
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.5330490405117271,
[2024-11-12T09:54:25.504+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "addBatch" : 886,
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "commitOffsets" : 492,
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1876,
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "walCommit" : 465
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:25.505+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "0" : 726
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - "0" : 727
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:25.506+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:25.507+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:25.507+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:25.507+0000] {spark_submit.py:495} INFO - "0" : 727
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3888888888888888,
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.5330490405117271,
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:25.508+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:25.509+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:25.522+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/139 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.139.84b66d6c-f612-4fb7-9d15-b7232330f2dc.tmp
[2024-11-12T09:54:25.556+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.139.84b66d6c-f612-4fb7-9d15-b7232330f2dc.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/139
[2024-11-12T09:54:25.557+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO MicroBatchExecution: Committed offsets for batch 139. Metadata OffsetSeqMetadata(0,1731405265510,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:25.570+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:25.573+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:25.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:25.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:25.602+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 137, 138, 138
[2024-11-12T09:54:25.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:25.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:25.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO DAGScheduler: Got job 139 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:25.632+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO DAGScheduler: Final stage: ResultStage 139 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:25.633+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:25.634+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:25.635+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO DAGScheduler: Submitting ResultStage 139 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:25.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:54:25.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:25.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:25.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:25.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 139 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:25.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO TaskSchedulerImpl: Adding task set 139.0 with 1 tasks resource profile 0
[2024-11-12T09:54:25.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO TaskSetManager: Starting task 0.0 in stage 139.0 (TID 139) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:25.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:25 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:26.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO TaskSetManager: Finished task 0.0 in stage 139.0 (TID 139) in 679 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:26.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool
[2024-11-12T09:54:26.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: ResultStage 139 (start at NativeMethodAccessorImpl.java:0) finished in 0.710 s
[2024-11-12T09:54:26.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:26.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 139: Stage finished
[2024-11-12T09:54:26.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Job 139 finished: start at NativeMethodAccessorImpl.java:0, took 0.715519 s
[2024-11-12T09:54:26.348+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO FileFormatWriter: Start to commit write Job a54b3f5d-1bda-4298-ad7f-32b6c8f8c0c3.
[2024-11-12T09:54:26.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/139.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.139.compact.b17be714-0a27-4ac7-8752-6c7e95632f84.tmp
[2024-11-12T09:54:26.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.139.compact.b17be714-0a27-4ac7-8752-6c7e95632f84.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/139.compact
[2024-11-12T09:54:26.608+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO FileStreamSinkLog: Current compact batch id = 139 min compaction batch id to delete = 39
[2024-11-12T09:54:26.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO ManifestFileCommitProtocol: Committed batch 139
[2024-11-12T09:54:26.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO FileFormatWriter: Write Job a54b3f5d-1bda-4298-ad7f-32b6c8f8c0c3 committed. Elapsed time: 264 ms.
[2024-11-12T09:54:26.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO FileFormatWriter: Finished processing stats for write job a54b3f5d-1bda-4298-ad7f-32b6c8f8c0c3.
[2024-11-12T09:54:26.625+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/139 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.139.14c8f302-a7f6-40c8-8ffb-76ff622a3e93.tmp
[2024-11-12T09:54:26.680+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.139.14c8f302-a7f6-40c8-8ffb-76ff622a3e93.tmp to hdfs://namenode:9000/spark_checkpoint/commits/139
[2024-11-12T09:54:26.682+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:26.685+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:26.687+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:26.688+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:26.689+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:25.502Z",
[2024-11-12T09:54:26.690+0000] {spark_submit.py:495} INFO - "batchId" : 139,
[2024-11-12T09:54:26.691+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:54:26.692+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0655301012253595,
[2024-11-12T09:54:26.692+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.697792869269949,
[2024-11-12T09:54:26.692+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:26.692+0000] {spark_submit.py:495} INFO - "addBatch" : 1032,
[2024-11-12T09:54:26.693+0000] {spark_submit.py:495} INFO - "commitOffsets" : 69,
[2024-11-12T09:54:26.693+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:26.693+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:26.694+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:54:26.694+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1178,
[2024-11-12T09:54:26.695+0000] {spark_submit.py:495} INFO - "walCommit" : 46
[2024-11-12T09:54:26.695+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:26.695+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:26.695+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:26.696+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:26.696+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:26.696+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:26.696+0000] {spark_submit.py:495} INFO - "0" : 727
[2024-11-12T09:54:26.696+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - "0" : 729
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - "0" : 729
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:26.697+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0655301012253595,
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.697792869269949,
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:26.698+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:26.701+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:26.702+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:26.702+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:26.703+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:26.703+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:26.713+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/140 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.140.14bccae7-416f-4cfc-9028-e0a59687a06e.tmp
[2024-11-12T09:54:26.764+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.140.14bccae7-416f-4cfc-9028-e0a59687a06e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/140
[2024-11-12T09:54:26.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO MicroBatchExecution: Committed offsets for batch 140. Metadata OffsetSeqMetadata(0,1731405266698,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:26.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:26.782+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:26.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:26.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:26.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 137, 138, 138, 139
[2024-11-12T09:54:26.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:26.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:26.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Got job 140 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:26.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Final stage: ResultStage 140 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:26.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:26.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:26.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[564] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:26.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:54:26.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:54:26.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:26.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:26.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 140 (MapPartitionsRDD[564] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:26.902+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO TaskSchedulerImpl: Adding task set 140.0 with 1 tasks resource profile 0
[2024-11-12T09:54:26.903+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 140) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:26.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:26 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:27.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 140) in 639 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:27.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool
[2024-11-12T09:54:27.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: ResultStage 140 (start at NativeMethodAccessorImpl.java:0) finished in 0.671 s
[2024-11-12T09:54:27.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Job 140 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:27.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished
[2024-11-12T09:54:27.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Job 140 finished: start at NativeMethodAccessorImpl.java:0, took 0.673938 s
[2024-11-12T09:54:27.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO FileFormatWriter: Start to commit write Job 44303217-923b-44d9-8ac1-55569ed4d1f5.
[2024-11-12T09:54:27.560+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/140 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.140.2b6bcafc-2bda-49c6-ac96-3933b5147edb.tmp
[2024-11-12T09:54:27.593+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.140.2b6bcafc-2bda-49c6-ac96-3933b5147edb.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/140
[2024-11-12T09:54:27.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO FileStreamSinkLog: Current compact batch id = 140 min compaction batch id to delete = 39
[2024-11-12T09:54:27.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO ManifestFileCommitProtocol: Committed batch 140
[2024-11-12T09:54:27.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO FileFormatWriter: Write Job 44303217-923b-44d9-8ac1-55569ed4d1f5 committed. Elapsed time: 52 ms.
[2024-11-12T09:54:27.597+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO FileFormatWriter: Finished processing stats for write job 44303217-923b-44d9-8ac1-55569ed4d1f5.
[2024-11-12T09:54:27.607+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/140 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.140.09d7b728-64df-4739-b250-003372301228.tmp
[2024-11-12T09:54:27.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.140.09d7b728-64df-4739-b250-003372301228.tmp to hdfs://namenode:9000/spark_checkpoint/commits/140
[2024-11-12T09:54:27.645+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:27.645+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:27.645+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:27.645+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:27.645+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:26.682Z",
[2024-11-12T09:54:27.645+0000] {spark_submit.py:495} INFO - "batchId" : 140,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8474576271186441,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.040582726326743,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "addBatch" : 810,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:27.646+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "queryPlanning" : 20,
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "triggerExecution" : 961,
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "walCommit" : 64
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:27.647+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:27.648+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:27.653+0000] {spark_submit.py:495} INFO - "0" : 729
[2024-11-12T09:54:27.653+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:27.653+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:27.654+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:27.654+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:27.654+0000] {spark_submit.py:495} INFO - "0" : 730
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - "0" : 730
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8474576271186441,
[2024-11-12T09:54:27.656+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.040582726326743,
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:27.657+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:27.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/141 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.141.2457b572-ae7a-4e95-ac64-df03a00c19d1.tmp
[2024-11-12T09:54:27.710+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.141.2457b572-ae7a-4e95-ac64-df03a00c19d1.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/141
[2024-11-12T09:54:27.712+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO MicroBatchExecution: Committed offsets for batch 141. Metadata OffsetSeqMetadata(0,1731405267652,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:27.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:27.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:27.741+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:27.742+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:27.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 138, 139, 140, 140
[2024-11-12T09:54:27.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:27.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:27.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Got job 141 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:27.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Final stage: ResultStage 141 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:27.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:27.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:27.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[568] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:27.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 320.7 KiB, free 431.2 MiB)
[2024-11-12T09:54:27.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.1 MiB)
[2024-11-12T09:54:27.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.6 MiB)
[2024-11-12T09:54:27.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:27.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[568] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:27.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO TaskSchedulerImpl: Adding task set 141.0 with 1 tasks resource profile 0
[2024-11-12T09:54:27.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 141) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:27.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:27 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.6 MiB)
[2024-11-12T09:54:28.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 141) in 675 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:28.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool
[2024-11-12T09:54:28.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: ResultStage 141 (start at NativeMethodAccessorImpl.java:0) finished in 0.709 s
[2024-11-12T09:54:28.501+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Job 141 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:28.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished
[2024-11-12T09:54:28.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Job 141 finished: start at NativeMethodAccessorImpl.java:0, took 0.713221 s
[2024-11-12T09:54:28.503+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO FileFormatWriter: Start to commit write Job fab6ed8e-4bda-45d5-bde0-e043967ae1b4.
[2024-11-12T09:54:28.512+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/141 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.141.3e948ea9-7fa7-42ef-9b39-3744db57dc32.tmp
[2024-11-12T09:54:28.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.141.3e948ea9-7fa7-42ef-9b39-3744db57dc32.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/141
[2024-11-12T09:54:28.568+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO FileStreamSinkLog: Current compact batch id = 141 min compaction batch id to delete = 39
[2024-11-12T09:54:28.574+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO ManifestFileCommitProtocol: Committed batch 141
[2024-11-12T09:54:28.577+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO FileFormatWriter: Write Job fab6ed8e-4bda-45d5-bde0-e043967ae1b4 committed. Elapsed time: 69 ms.
[2024-11-12T09:54:28.578+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO FileFormatWriter: Finished processing stats for write job fab6ed8e-4bda-45d5-bde0-e043967ae1b4.
[2024-11-12T09:54:28.588+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/141 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.141.f9fe4945-3753-4d2d-b4a8-bda3465c660c.tmp
[2024-11-12T09:54:28.638+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.141.f9fe4945-3753-4d2d-b4a8-bda3465c660c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/141
[2024-11-12T09:54:28.639+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:28.640+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:28.640+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:28.640+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:28.641+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:27.644Z",
[2024-11-12T09:54:28.641+0000] {spark_submit.py:495} INFO - "batchId" : 141,
[2024-11-12T09:54:28.641+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:28.642+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0395010395010396,
[2024-11-12T09:54:28.642+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0070493454179255,
[2024-11-12T09:54:28.643+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:28.643+0000] {spark_submit.py:495} INFO - "addBatch" : 842,
[2024-11-12T09:54:28.644+0000] {spark_submit.py:495} INFO - "commitOffsets" : 63,
[2024-11-12T09:54:28.644+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:28.644+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:54:28.644+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "triggerExecution" : 993,
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "walCommit" : 59
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:28.645+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "0" : 730
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "0" : 731
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - "0" : 731
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:28.646+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:28.652+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0395010395010396,
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0070493454179255,
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:28.653+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:28.654+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:28.654+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:28.654+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:28.655+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:28.655+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:28.655+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:28.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/142 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.142.fe2739e5-846d-4442-b1df-6d33fa55fc39.tmp
[2024-11-12T09:54:28.694+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.142.fe2739e5-846d-4442-b1df-6d33fa55fc39.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/142
[2024-11-12T09:54:28.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO MicroBatchExecution: Committed offsets for batch 142. Metadata OffsetSeqMetadata(0,1731405268643,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:28.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:28.716+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:28.727+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:28.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:28.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_138_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:28.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 140, 141, 141
[2024-11-12T09:54:28.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:28.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:28.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_134_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:28.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:28.780+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_139_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:28.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_139_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:28.795+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:28.798+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Got job 142 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:28.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Final stage: ResultStage 142 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:28.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:28.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:28.803+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_137_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:28.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Submitting ResultStage 142 (MapPartitionsRDD[572] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:28.805+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:28.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_140_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:28.827+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:28.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:28.847+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_136_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:28.852+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_136_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:28.857+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:28.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:28.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:28.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 142 (MapPartitionsRDD[572] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:28.862+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO TaskSchedulerImpl: Adding task set 142.0 with 1 tasks resource profile 0
[2024-11-12T09:54:28.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 142) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:28.868+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_135_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:28.881+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:28.889+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_141_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:28.891+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:28.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:28 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:29.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 142) in 675 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:29.544+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool
[2024-11-12T09:54:29.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: ResultStage 142 (start at NativeMethodAccessorImpl.java:0) finished in 0.740 s
[2024-11-12T09:54:29.546+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Job 142 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:29.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 142: Stage finished
[2024-11-12T09:54:29.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Job 142 finished: start at NativeMethodAccessorImpl.java:0, took 0.750697 s
[2024-11-12T09:54:29.547+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO FileFormatWriter: Start to commit write Job e6f5d479-65ab-4bba-91e3-97e9cc83e60d.
[2024-11-12T09:54:29.557+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/142 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.142.a0dba565-e0e2-4a48-8fe0-e7622f1643d9.tmp
[2024-11-12T09:54:29.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.142.a0dba565-e0e2-4a48-8fe0-e7622f1643d9.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/142
[2024-11-12T09:54:29.597+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO FileStreamSinkLog: Current compact batch id = 142 min compaction batch id to delete = 39
[2024-11-12T09:54:29.599+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO ManifestFileCommitProtocol: Committed batch 142
[2024-11-12T09:54:29.599+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO FileFormatWriter: Write Job e6f5d479-65ab-4bba-91e3-97e9cc83e60d committed. Elapsed time: 52 ms.
[2024-11-12T09:54:29.599+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO FileFormatWriter: Finished processing stats for write job e6f5d479-65ab-4bba-91e3-97e9cc83e60d.
[2024-11-12T09:54:29.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/142 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.142.da5f780f-f5ea-414d-8fc3-5edc8f994055.tmp
[2024-11-12T09:54:29.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.142.da5f780f-f5ea-414d-8fc3-5edc8f994055.tmp to hdfs://namenode:9000/spark_checkpoint/commits/142
[2024-11-12T09:54:29.665+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:29.666+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:29.666+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:29.667+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:29.667+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:28.639Z",
[2024-11-12T09:54:29.667+0000] {spark_submit.py:495} INFO - "batchId" : 142,
[2024-11-12T09:54:29.668+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:29.668+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0050251256281406,
[2024-11-12T09:54:29.668+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9756097560975611,
[2024-11-12T09:54:29.668+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:29.669+0000] {spark_submit.py:495} INFO - "addBatch" : 880,
[2024-11-12T09:54:29.669+0000] {spark_submit.py:495} INFO - "commitOffsets" : 65,
[2024-11-12T09:54:29.669+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:29.669+0000] {spark_submit.py:495} INFO - "latestOffset" : 4,
[2024-11-12T09:54:29.669+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:54:29.670+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1025,
[2024-11-12T09:54:29.670+0000] {spark_submit.py:495} INFO - "walCommit" : 51
[2024-11-12T09:54:29.670+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:29.671+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:29.671+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:29.672+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:29.672+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:29.673+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:29.673+0000] {spark_submit.py:495} INFO - "0" : 731
[2024-11-12T09:54:29.673+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:29.673+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:29.674+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:29.674+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:29.674+0000] {spark_submit.py:495} INFO - "0" : 732
[2024-11-12T09:54:29.674+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - "0" : 732
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:29.675+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0050251256281406,
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9756097560975611,
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:29.676+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:29.677+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:29.677+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:29.677+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:29.679+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:29.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/143 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.143.a0fab5f5-9c00-49a6-9afc-feba2a0441c0.tmp
[2024-11-12T09:54:29.768+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.143.a0fab5f5-9c00-49a6-9afc-feba2a0441c0.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/143
[2024-11-12T09:54:29.769+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO MicroBatchExecution: Committed offsets for batch 143. Metadata OffsetSeqMetadata(0,1731405269676,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:29.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:29.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:29.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:29.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:29.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 141, 142, 142
[2024-11-12T09:54:29.827+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:29.852+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:29.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Got job 143 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:29.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Final stage: ResultStage 143 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:29.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:29.854+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:29.854+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[576] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:29.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:29.872+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:29.872+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:29.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:29.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[576] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:29.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO TaskSchedulerImpl: Adding task set 143.0 with 1 tasks resource profile 0
[2024-11-12T09:54:29.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 143) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:29.898+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:29 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:30.541+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 143) in 665 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:30.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool
[2024-11-12T09:54:30.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO DAGScheduler: ResultStage 143 (start at NativeMethodAccessorImpl.java:0) finished in 0.688 s
[2024-11-12T09:54:30.542+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO DAGScheduler: Job 143 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:30.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished
[2024-11-12T09:54:30.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO DAGScheduler: Job 143 finished: start at NativeMethodAccessorImpl.java:0, took 0.690120 s
[2024-11-12T09:54:30.543+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO FileFormatWriter: Start to commit write Job 9e0c26be-7a53-44f0-a7f7-bb42c8ccd2ad.
[2024-11-12T09:54:30.556+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/143 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.143.ae8c08e1-f4c5-4360-8e8a-cf641f870d33.tmp
[2024-11-12T09:54:30.602+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.143.ae8c08e1-f4c5-4360-8e8a-cf641f870d33.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/143
[2024-11-12T09:54:30.602+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO FileStreamSinkLog: Current compact batch id = 143 min compaction batch id to delete = 39
[2024-11-12T09:54:30.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO ManifestFileCommitProtocol: Committed batch 143
[2024-11-12T09:54:30.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO FileFormatWriter: Write Job 9e0c26be-7a53-44f0-a7f7-bb42c8ccd2ad committed. Elapsed time: 68 ms.
[2024-11-12T09:54:30.612+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO FileFormatWriter: Finished processing stats for write job 9e0c26be-7a53-44f0-a7f7-bb42c8ccd2ad.
[2024-11-12T09:54:30.618+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/143 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.143.30719ff9-3530-4e3a-a328-afdbd66e060d.tmp
[2024-11-12T09:54:30.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.143.30719ff9-3530-4e3a-a328-afdbd66e060d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/143
[2024-11-12T09:54:30.670+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:30.670+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:30.671+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:30.671+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:30.671+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:29.665Z",
[2024-11-12T09:54:30.672+0000] {spark_submit.py:495} INFO - "batchId" : 143,
[2024-11-12T09:54:30.672+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:30.672+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9746588693957114,
[2024-11-12T09:54:30.672+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0060362173038229,
[2024-11-12T09:54:30.672+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:30.674+0000] {spark_submit.py:495} INFO - "addBatch" : 815,
[2024-11-12T09:54:30.675+0000] {spark_submit.py:495} INFO - "commitOffsets" : 47,
[2024-11-12T09:54:30.675+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:30.675+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:54:30.675+0000] {spark_submit.py:495} INFO - "queryPlanning" : 28,
[2024-11-12T09:54:30.677+0000] {spark_submit.py:495} INFO - "triggerExecution" : 994,
[2024-11-12T09:54:30.678+0000] {spark_submit.py:495} INFO - "walCommit" : 88
[2024-11-12T09:54:30.679+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:30.679+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:30.679+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:30.679+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - "0" : 732
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - "0" : 733
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:30.680+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:30.682+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:30.684+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:30.685+0000] {spark_submit.py:495} INFO - "0" : 733
[2024-11-12T09:54:30.685+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:30.685+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:30.685+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:30.686+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9746588693957114,
[2024-11-12T09:54:30.686+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0060362173038229,
[2024-11-12T09:54:30.686+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:30.686+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:30.687+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:30.687+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:30.687+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:30.687+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:30.688+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:30.688+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:30.689+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:30.689+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:30.690+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:30.692+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:30 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/144 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.144.fbfece5e-b409-427d-bd64-c447f58e0600.tmp
[2024-11-12T09:54:31.148+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.144.fbfece5e-b409-427d-bd64-c447f58e0600.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/144
[2024-11-12T09:54:31.149+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO MicroBatchExecution: Committed offsets for batch 144. Metadata OffsetSeqMetadata(0,1731405270683,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:31.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 142, 143, 143
[2024-11-12T09:54:31.244+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:31.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:31.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Got job 144 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:31.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Final stage: ResultStage 144 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:31.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:31.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:31.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[580] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:31.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:31.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:31.368+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:31.370+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:31.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 144 (MapPartitionsRDD[580] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:31.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO TaskSchedulerImpl: Adding task set 144.0 with 1 tasks resource profile 0
[2024-11-12T09:54:31.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 144) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:31.414+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:31.641+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 144) in 266 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:31.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool
[2024-11-12T09:54:31.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: ResultStage 144 (start at NativeMethodAccessorImpl.java:0) finished in 0.338 s
[2024-11-12T09:54:31.647+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Job 144 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:31.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished
[2024-11-12T09:54:31.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Job 144 finished: start at NativeMethodAccessorImpl.java:0, took 0.348288 s
[2024-11-12T09:54:31.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO FileFormatWriter: Start to commit write Job cc707120-e9a9-4e62-8a87-9fb6fa22eb28.
[2024-11-12T09:54:31.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/144 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.144.78f3b453-10ac-478f-9936-e74f1bf6e855.tmp
[2024-11-12T09:54:31.726+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.144.78f3b453-10ac-478f-9936-e74f1bf6e855.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/144
[2024-11-12T09:54:31.729+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO FileStreamSinkLog: Current compact batch id = 144 min compaction batch id to delete = 39
[2024-11-12T09:54:31.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO ManifestFileCommitProtocol: Committed batch 144
[2024-11-12T09:54:31.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO FileFormatWriter: Write Job cc707120-e9a9-4e62-8a87-9fb6fa22eb28 committed. Elapsed time: 73 ms.
[2024-11-12T09:54:31.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO FileFormatWriter: Finished processing stats for write job cc707120-e9a9-4e62-8a87-9fb6fa22eb28.
[2024-11-12T09:54:31.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/144 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.144.1a264fa1-1a22-4ea1-83e6-74c2880d517b.tmp
[2024-11-12T09:54:31.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.144.1a264fa1-1a22-4ea1-83e6-74c2880d517b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/144
[2024-11-12T09:54:31.790+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:31.791+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:31.791+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:31.791+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:31.791+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:30.669Z",
[2024-11-12T09:54:31.791+0000] {spark_submit.py:495} INFO - "batchId" : 144,
[2024-11-12T09:54:31.794+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:31.794+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9960159362549801,
[2024-11-12T09:54:31.794+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8944543828264757,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "addBatch" : 538,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "commitOffsets" : 55,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "latestOffset" : 14,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "queryPlanning" : 42,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1118,
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - "walCommit" : 465
[2024-11-12T09:54:31.795+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:31.798+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:31.799+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:31.799+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "0" : 733
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "0" : 734
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:31.800+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "0" : 734
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9960159362549801,
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8944543828264757,
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:31.801+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:31.802+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:31.802+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:31.802+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:31.802+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:31.802+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:31.802+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:31.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/145 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.145.47a9752b-b1cd-4f01-b9a0-58b32608f836.tmp
[2024-11-12T09:54:31.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.145.47a9752b-b1cd-4f01-b9a0-58b32608f836.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/145
[2024-11-12T09:54:31.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO MicroBatchExecution: Committed offsets for batch 145. Metadata OffsetSeqMetadata(0,1731405271808,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:31.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.909+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:31.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 144, 144
[2024-11-12T09:54:31.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:31.965+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:31.967+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Got job 145 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:31.968+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Final stage: ResultStage 145 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:31.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:31.969+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:31.970+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:31 INFO DAGScheduler: Submitting ResultStage 145 (MapPartitionsRDD[584] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:32.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:32.005+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:32.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:32.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:32.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[584] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:32.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks resource profile 0
[2024-11-12T09:54:32.010+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 145) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:32.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:32.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 145) in 650 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:32.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool
[2024-11-12T09:54:32.662+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO DAGScheduler: ResultStage 145 (start at NativeMethodAccessorImpl.java:0) finished in 0.690 s
[2024-11-12T09:54:32.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO DAGScheduler: Job 145 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:32.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 145: Stage finished
[2024-11-12T09:54:32.666+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO DAGScheduler: Job 145 finished: start at NativeMethodAccessorImpl.java:0, took 0.695591 s
[2024-11-12T09:54:32.666+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO FileFormatWriter: Start to commit write Job cd237875-dd40-4ef0-9cbc-0d873b122bbc.
[2024-11-12T09:54:32.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/145 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.145.2ea8621d-81ad-446a-a66b-b184e50d7191.tmp
[2024-11-12T09:54:32.755+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.145.2ea8621d-81ad-446a-a66b-b184e50d7191.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/145
[2024-11-12T09:54:32.756+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO FileStreamSinkLog: Current compact batch id = 145 min compaction batch id to delete = 39
[2024-11-12T09:54:32.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO ManifestFileCommitProtocol: Committed batch 145
[2024-11-12T09:54:32.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO FileFormatWriter: Write Job cd237875-dd40-4ef0-9cbc-0d873b122bbc committed. Elapsed time: 103 ms.
[2024-11-12T09:54:32.770+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO FileFormatWriter: Finished processing stats for write job cd237875-dd40-4ef0-9cbc-0d873b122bbc.
[2024-11-12T09:54:32.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/145 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.145.e2071425-dd0d-4b9b-a207-dea54bdb3dcb.tmp
[2024-11-12T09:54:32.884+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.145.e2071425-dd0d-4b9b-a207-dea54bdb3dcb.tmp to hdfs://namenode:9000/spark_checkpoint/commits/145
[2024-11-12T09:54:32.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:32 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:32.889+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:31.790Z",
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "batchId" : 145,
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8920606601248885,
[2024-11-12T09:54:32.890+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9140767824497257,
[2024-11-12T09:54:32.892+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:32.893+0000] {spark_submit.py:495} INFO - "addBatch" : 877,
[2024-11-12T09:54:32.901+0000] {spark_submit.py:495} INFO - "commitOffsets" : 116,
[2024-11-12T09:54:32.902+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:32.902+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:54:32.903+0000] {spark_submit.py:495} INFO - "queryPlanning" : 23,
[2024-11-12T09:54:32.903+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1094,
[2024-11-12T09:54:32.903+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:54:32.903+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:32.903+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:32.903+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:32.904+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:32.904+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:32.904+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:32.905+0000] {spark_submit.py:495} INFO - "0" : 734
[2024-11-12T09:54:32.905+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:32.905+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:32.906+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:32.906+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:32.906+0000] {spark_submit.py:495} INFO - "0" : 735
[2024-11-12T09:54:32.906+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:32.907+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:32.907+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:32.907+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:32.908+0000] {spark_submit.py:495} INFO - "0" : 735
[2024-11-12T09:54:32.908+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:32.909+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:32.910+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8920606601248885,
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9140767824497257,
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:32.911+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:32.912+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:32.912+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:32.912+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:32.912+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:32.913+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:32.913+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:33.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/146 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.146.cbeff121-2299-4aec-b4ea-cd208715fa95.tmp
[2024-11-12T09:54:33.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.146.cbeff121-2299-4aec-b4ea-cd208715fa95.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/146
[2024-11-12T09:54:33.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO MicroBatchExecution: Committed offsets for batch 146. Metadata OffsetSeqMetadata(0,1731405272902,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:33.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:33.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:33.141+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:33.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:33.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 144, 145, 145
[2024-11-12T09:54:33.164+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:33.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:33.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Got job 146 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:33.236+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Final stage: ResultStage 146 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:33.241+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:33.242+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:33.242+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[588] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:33.275+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:33.285+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:33.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:33.288+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO SparkContext: Created broadcast 146 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:33.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 146 (MapPartitionsRDD[588] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:33.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO TaskSchedulerImpl: Adding task set 146.0 with 1 tasks resource profile 0
[2024-11-12T09:54:33.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 146) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:33.349+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:33.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 146) in 340 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:33.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool
[2024-11-12T09:54:33.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: ResultStage 146 (start at NativeMethodAccessorImpl.java:0) finished in 0.401 s
[2024-11-12T09:54:33.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Job 146 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:33.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished
[2024-11-12T09:54:33.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO DAGScheduler: Job 146 finished: start at NativeMethodAccessorImpl.java:0, took 0.409382 s
[2024-11-12T09:54:33.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO FileFormatWriter: Start to commit write Job 62ebe092-c056-4485-9c9f-d747dfb67374.
[2024-11-12T09:54:33.661+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:33 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/146 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.146.d6475ff9-4242-4721-876a-8b7f905c9a48.tmp
[2024-11-12T09:54:34.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.146.d6475ff9-4242-4721-876a-8b7f905c9a48.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/146
[2024-11-12T09:54:34.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileStreamSinkLog: Current compact batch id = 146 min compaction batch id to delete = 39
[2024-11-12T09:54:34.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO ManifestFileCommitProtocol: Committed batch 146
[2024-11-12T09:54:34.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileFormatWriter: Write Job 62ebe092-c056-4485-9c9f-d747dfb67374 committed. Elapsed time: 476 ms.
[2024-11-12T09:54:34.122+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileFormatWriter: Finished processing stats for write job 62ebe092-c056-4485-9c9f-d747dfb67374.
[2024-11-12T09:54:34.140+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/146 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.146.d0951ad8-0884-4fb9-b065-27940b143469.tmp
[2024-11-12T09:54:34.201+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.146.d0951ad8-0884-4fb9-b065-27940b143469.tmp to hdfs://namenode:9000/spark_checkpoint/commits/146
[2024-11-12T09:54:34.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:34.204+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:34.205+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:34.205+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:34.205+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:32.885Z",
[2024-11-12T09:54:34.205+0000] {spark_submit.py:495} INFO - "batchId" : 146,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9132420091324202,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7604562737642586,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "addBatch" : 1004,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "commitOffsets" : 79,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "queryPlanning" : 45,
[2024-11-12T09:54:34.206+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1315,
[2024-11-12T09:54:34.207+0000] {spark_submit.py:495} INFO - "walCommit" : 161
[2024-11-12T09:54:34.207+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:34.207+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "0" : 735
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - "0" : 736
[2024-11-12T09:54:34.208+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:34.209+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:34.209+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:34.209+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:34.209+0000] {spark_submit.py:495} INFO - "0" : 736
[2024-11-12T09:54:34.209+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:34.209+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:34.210+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:34.210+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9132420091324202,
[2024-11-12T09:54:34.210+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7604562737642586,
[2024-11-12T09:54:34.210+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:34.210+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:34.211+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:34.211+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:34.212+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:34.212+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:34.212+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:34.213+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:34.213+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:34.213+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:34.214+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:34.232+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/147 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.147.7fea7e19-7bbc-4e18-934a-73296b01da6d.tmp
[2024-11-12T09:54:34.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.147.7fea7e19-7bbc-4e18-934a-73296b01da6d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/147
[2024-11-12T09:54:34.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO MicroBatchExecution: Committed offsets for batch 147. Metadata OffsetSeqMetadata(0,1731405274215,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:34.353+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:34.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:34.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:34.432+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:34.449+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 145, 146, 146
[2024-11-12T09:54:34.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:34.600+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:34.605+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Got job 147 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:34.606+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Final stage: ResultStage 147 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:34.607+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:34.607+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:34.607+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[592] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:34.640+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:54:34.649+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:34.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:34.655+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:34.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[592] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:34.656+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks resource profile 0
[2024-11-12T09:54:34.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 147) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:34.695+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:34.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 147) in 167 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:34.827+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool
[2024-11-12T09:54:34.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: ResultStage 147 (start at NativeMethodAccessorImpl.java:0) finished in 0.220 s
[2024-11-12T09:54:34.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Job 147 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:34.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished
[2024-11-12T09:54:34.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO DAGScheduler: Job 147 finished: start at NativeMethodAccessorImpl.java:0, took 0.229099 s
[2024-11-12T09:54:34.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileFormatWriter: Start to commit write Job 080e9477-966d-4a64-8e48-80ea4a4df17f.
[2024-11-12T09:54:34.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/147 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.147.345d2217-51d5-44ca-857d-f8ceedce2503.tmp
[2024-11-12T09:54:34.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.147.345d2217-51d5-44ca-857d-f8ceedce2503.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/147
[2024-11-12T09:54:34.920+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileStreamSinkLog: Current compact batch id = 147 min compaction batch id to delete = 39
[2024-11-12T09:54:34.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO ManifestFileCommitProtocol: Committed batch 147
[2024-11-12T09:54:34.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileFormatWriter: Write Job 080e9477-966d-4a64-8e48-80ea4a4df17f committed. Elapsed time: 94 ms.
[2024-11-12T09:54:34.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO FileFormatWriter: Finished processing stats for write job 080e9477-966d-4a64-8e48-80ea4a4df17f.
[2024-11-12T09:54:34.936+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:34 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/147 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.147.fe2ce756-adbb-43e4-810b-1d52b60f56f4.tmp
[2024-11-12T09:54:35.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.147.fe2ce756-adbb-43e4-810b-1d52b60f56f4.tmp to hdfs://namenode:9000/spark_checkpoint/commits/147
[2024-11-12T09:54:35.014+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:35.015+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:35.017+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:35.019+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:35.019+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:34.204Z",
[2024-11-12T09:54:35.026+0000] {spark_submit.py:495} INFO - "batchId" : 147,
[2024-11-12T09:54:35.026+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:35.027+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7581501137225171,
[2024-11-12T09:54:35.027+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2391573729863692,
[2024-11-12T09:54:35.027+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:35.028+0000] {spark_submit.py:495} INFO - "addBatch" : 552,
[2024-11-12T09:54:35.030+0000] {spark_submit.py:495} INFO - "commitOffsets" : 86,
[2024-11-12T09:54:35.030+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:35.031+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:54:35.033+0000] {spark_submit.py:495} INFO - "queryPlanning" : 46,
[2024-11-12T09:54:35.033+0000] {spark_submit.py:495} INFO - "triggerExecution" : 807,
[2024-11-12T09:54:35.033+0000] {spark_submit.py:495} INFO - "walCommit" : 95
[2024-11-12T09:54:35.034+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.034+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:35.034+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:35.034+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:35.034+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:35.035+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:35.035+0000] {spark_submit.py:495} INFO - "0" : 736
[2024-11-12T09:54:35.035+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.035+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.035+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:35.035+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:35.037+0000] {spark_submit.py:495} INFO - "0" : 737
[2024-11-12T09:54:35.042+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.044+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.054+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:35.055+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:35.057+0000] {spark_submit.py:495} INFO - "0" : 737
[2024-11-12T09:54:35.059+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.060+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.060+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:35.060+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7581501137225171,
[2024-11-12T09:54:35.061+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2391573729863692,
[2024-11-12T09:54:35.061+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:35.062+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:35.062+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:35.062+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:35.063+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.063+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:35.063+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:35.068+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:35.069+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:35.070+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.072+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/148 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.148.0de9d2f3-862f-4bac-b924-b1a926fe11f3.tmp
[2024-11-12T09:54:35.123+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.148.0de9d2f3-862f-4bac-b924-b1a926fe11f3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/148
[2024-11-12T09:54:35.124+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO MicroBatchExecution: Committed offsets for batch 148. Metadata OffsetSeqMetadata(0,1731405275023,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:35.159+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:35.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:35.182+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:35.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:35.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 146, 147, 147
[2024-11-12T09:54:35.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:35.242+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:35.249+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Got job 148 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:35.249+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Final stage: ResultStage 148 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:35.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:35.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:35.255+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[596] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:35.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:54:35.309+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:54:35.310+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:35.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:35.311+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 148 (MapPartitionsRDD[596] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:35.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO TaskSchedulerImpl: Adding task set 148.0 with 1 tasks resource profile 0
[2024-11-12T09:54:35.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 148) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:35.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:35.717+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 148) in 400 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:35.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool
[2024-11-12T09:54:35.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: ResultStage 148 (start at NativeMethodAccessorImpl.java:0) finished in 0.469 s
[2024-11-12T09:54:35.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Job 148 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:35.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished
[2024-11-12T09:54:35.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO DAGScheduler: Job 148 finished: start at NativeMethodAccessorImpl.java:0, took 0.478877 s
[2024-11-12T09:54:35.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO FileFormatWriter: Start to commit write Job 2c03970b-b144-49c5-ba84-b9e2dfbc41c9.
[2024-11-12T09:54:35.739+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/148 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.148.a8ffcf3c-7b5b-46e5-8ce0-d62cdab0700c.tmp
[2024-11-12T09:54:35.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.148.a8ffcf3c-7b5b-46e5-8ce0-d62cdab0700c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/148
[2024-11-12T09:54:35.815+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO FileStreamSinkLog: Current compact batch id = 148 min compaction batch id to delete = 39
[2024-11-12T09:54:35.819+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO ManifestFileCommitProtocol: Committed batch 148
[2024-11-12T09:54:35.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO FileFormatWriter: Write Job 2c03970b-b144-49c5-ba84-b9e2dfbc41c9 committed. Elapsed time: 97 ms.
[2024-11-12T09:54:35.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO FileFormatWriter: Finished processing stats for write job 2c03970b-b144-49c5-ba84-b9e2dfbc41c9.
[2024-11-12T09:54:35.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/148 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.148.7dd3bafa-c40b-49e3-997a-a15902d8d5ff.tmp
[2024-11-12T09:54:35.922+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.148.7dd3bafa-c40b-49e3-997a-a15902d8d5ff.tmp to hdfs://namenode:9000/spark_checkpoint/commits/148
[2024-11-12T09:54:35.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:35.930+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:35.931+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:35.931+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:35.931+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:35.012Z",
[2024-11-12T09:54:35.932+0000] {spark_submit.py:495} INFO - "batchId" : 148,
[2024-11-12T09:54:35.933+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:35.933+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2376237623762376,
[2024-11-12T09:54:35.933+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0989010989010988,
[2024-11-12T09:54:35.934+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:35.934+0000] {spark_submit.py:495} INFO - "addBatch" : 653,
[2024-11-12T09:54:35.934+0000] {spark_submit.py:495} INFO - "commitOffsets" : 102,
[2024-11-12T09:54:35.934+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:35.935+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:54:35.935+0000] {spark_submit.py:495} INFO - "queryPlanning" : 39,
[2024-11-12T09:54:35.935+0000] {spark_submit.py:495} INFO - "triggerExecution" : 910,
[2024-11-12T09:54:35.935+0000] {spark_submit.py:495} INFO - "walCommit" : 99
[2024-11-12T09:54:35.936+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - "0" : 737
[2024-11-12T09:54:35.937+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.938+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.938+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:35.938+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:35.938+0000] {spark_submit.py:495} INFO - "0" : 738
[2024-11-12T09:54:35.938+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - "0" : 738
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:35.939+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:35.940+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.2376237623762376,
[2024-11-12T09:54:35.940+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0989010989010988,
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:35.941+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:35.942+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:35.942+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:35.942+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.942+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:35.946+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:35 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/149 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.149.978aaa5b-3817-4c8e-96d1-f1643c411c44.tmp
[2024-11-12T09:54:36.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.149.978aaa5b-3817-4c8e-96d1-f1643c411c44.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/149
[2024-11-12T09:54:36.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO MicroBatchExecution: Committed offsets for batch 149. Metadata OffsetSeqMetadata(0,1731405275937,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:36.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:36.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:36.107+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:36.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:36.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 147, 148, 148
[2024-11-12T09:54:36.131+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:36.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:36.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Got job 149 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:36.188+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Final stage: ResultStage 149 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:36.192+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:36.193+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:36.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[600] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:36.261+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_143_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:36.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:36.286+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:54:36.307+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_147_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:36.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:36.313+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_147_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:36.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:36.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:36.319+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 149 (MapPartitionsRDD[600] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:36.321+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO TaskSchedulerImpl: Adding task set 149.0 with 1 tasks resource profile 0
[2024-11-12T09:54:36.328+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 149) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:36.352+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_148_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:36.369+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_148_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:36.373+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:36.394+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_144_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:36.407+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_144_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:36.431+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_146_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:36.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_146_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:36.464+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_142_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:36.471+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_142_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:36.487+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_145_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:36.502+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO BlockManagerInfo: Removed broadcast_145_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:36.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 149) in 424 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:36.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool
[2024-11-12T09:54:36.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: ResultStage 149 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
[2024-11-12T09:54:36.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:36.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished
[2024-11-12T09:54:36.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO DAGScheduler: Job 149 finished: start at NativeMethodAccessorImpl.java:0, took 0.577114 s
[2024-11-12T09:54:36.758+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO FileFormatWriter: Start to commit write Job 86c9ea4a-76f0-47e5-aca4-710a0d055823.
[2024-11-12T09:54:36.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:36 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/149.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.149.compact.aac2c6af-1fb2-48b1-a3df-4d7379ad4775.tmp
[2024-11-12T09:54:37.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.149.compact.aac2c6af-1fb2-48b1-a3df-4d7379ad4775.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/149.compact
[2024-11-12T09:54:37.177+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO FileStreamSinkLog: Current compact batch id = 149 min compaction batch id to delete = 49
[2024-11-12T09:54:37.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO ManifestFileCommitProtocol: Committed batch 149
[2024-11-12T09:54:37.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO FileFormatWriter: Write Job 86c9ea4a-76f0-47e5-aca4-710a0d055823 committed. Elapsed time: 424 ms.
[2024-11-12T09:54:37.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO FileFormatWriter: Finished processing stats for write job 86c9ea4a-76f0-47e5-aca4-710a0d055823.
[2024-11-12T09:54:37.224+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/149 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.149.b0804e67-a2ee-47ad-9f6b-ef4445268ff5.tmp
[2024-11-12T09:54:37.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.149.b0804e67-a2ee-47ad-9f6b-ef4445268ff5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/149
[2024-11-12T09:54:37.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:35.925Z",
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "batchId" : 149,
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.095290251916758,
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7267441860465117,
[2024-11-12T09:54:37.307+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:37.308+0000] {spark_submit.py:495} INFO - "addBatch" : 1098,
[2024-11-12T09:54:37.308+0000] {spark_submit.py:495} INFO - "commitOffsets" : 117,
[2024-11-12T09:54:37.308+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:37.309+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:54:37.312+0000] {spark_submit.py:495} INFO - "queryPlanning" : 43,
[2024-11-12T09:54:37.313+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1376,
[2024-11-12T09:54:37.314+0000] {spark_submit.py:495} INFO - "walCommit" : 95
[2024-11-12T09:54:37.314+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:37.315+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:37.316+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:37.316+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:37.316+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:37.316+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:37.316+0000] {spark_submit.py:495} INFO - "0" : 738
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - "0" : 739
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:37.317+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:37.320+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:37.325+0000] {spark_submit.py:495} INFO - "0" : 739
[2024-11-12T09:54:37.326+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:37.327+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:37.328+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:37.328+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.095290251916758,
[2024-11-12T09:54:37.329+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7267441860465117,
[2024-11-12T09:54:37.329+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:37.330+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:37.330+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:37.330+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:37.330+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:37.330+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:37.330+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:37.331+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:37.331+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:37.332+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:37.332+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:37.371+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/150 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.150.20249b74-4fe2-4f99-acc4-2535af75f60c.tmp
[2024-11-12T09:54:37.424+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.150.20249b74-4fe2-4f99-acc4-2535af75f60c.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/150
[2024-11-12T09:54:37.425+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO MicroBatchExecution: Committed offsets for batch 150. Metadata OffsetSeqMetadata(0,1731405277338,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:37.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:37.492+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:37.526+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:37.528+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:37.540+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 147, 148, 148, 149
[2024-11-12T09:54:37.551+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:37.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:37.608+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO DAGScheduler: Got job 150 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:37.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO DAGScheduler: Final stage: ResultStage 150 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:37.609+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:37.610+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:37.610+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[604] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:37.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:37.821+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:37.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:37.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:37.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 150 (MapPartitionsRDD[604] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:37.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO TaskSchedulerImpl: Adding task set 150.0 with 1 tasks resource profile 0
[2024-11-12T09:54:37.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 150) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:37.953+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:37 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:38.228+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 150) in 372 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:38.229+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool
[2024-11-12T09:54:38.233+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: ResultStage 150 (start at NativeMethodAccessorImpl.java:0) finished in 0.591 s
[2024-11-12T09:54:38.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Job 150 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:38.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished
[2024-11-12T09:54:38.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Job 150 finished: start at NativeMethodAccessorImpl.java:0, took 0.628254 s
[2024-11-12T09:54:38.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO FileFormatWriter: Start to commit write Job 5ad91666-d481-45e7-93ba-c27877886186.
[2024-11-12T09:54:38.250+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/150 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.150.e03e7eb2-f200-43e0-9519-97d21efb7dd6.tmp
[2024-11-12T09:54:38.317+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.150.e03e7eb2-f200-43e0-9519-97d21efb7dd6.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/150
[2024-11-12T09:54:38.320+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO FileStreamSinkLog: Current compact batch id = 150 min compaction batch id to delete = 49
[2024-11-12T09:54:38.325+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO ManifestFileCommitProtocol: Committed batch 150
[2024-11-12T09:54:38.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO FileFormatWriter: Write Job 5ad91666-d481-45e7-93ba-c27877886186 committed. Elapsed time: 92 ms.
[2024-11-12T09:54:38.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO FileFormatWriter: Finished processing stats for write job 5ad91666-d481-45e7-93ba-c27877886186.
[2024-11-12T09:54:38.347+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/150 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.150.c7afd9b7-77b1-4c8a-a6dd-bd6621e163b2.tmp
[2024-11-12T09:54:38.425+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.150.c7afd9b7-77b1-4c8a-a6dd-bd6621e163b2.tmp to hdfs://namenode:9000/spark_checkpoint/commits/150
[2024-11-12T09:54:38.427+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:38.428+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:38.428+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:38.428+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:38.429+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:37.305Z",
[2024-11-12T09:54:38.429+0000] {spark_submit.py:495} INFO - "batchId" : 150,
[2024-11-12T09:54:38.429+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:38.430+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7246376811594204,
[2024-11-12T09:54:38.430+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8936550491510277,
[2024-11-12T09:54:38.430+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:38.430+0000] {spark_submit.py:495} INFO - "addBatch" : 825,
[2024-11-12T09:54:38.430+0000] {spark_submit.py:495} INFO - "commitOffsets" : 99,
[2024-11-12T09:54:38.431+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:38.431+0000] {spark_submit.py:495} INFO - "latestOffset" : 33,
[2024-11-12T09:54:38.431+0000] {spark_submit.py:495} INFO - "queryPlanning" : 70,
[2024-11-12T09:54:38.431+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1119,
[2024-11-12T09:54:38.432+0000] {spark_submit.py:495} INFO - "walCommit" : 87
[2024-11-12T09:54:38.432+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:38.435+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:38.436+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:38.436+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:38.436+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:38.437+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:38.440+0000] {spark_submit.py:495} INFO - "0" : 739
[2024-11-12T09:54:38.441+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:38.442+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:38.443+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:38.443+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:38.444+0000] {spark_submit.py:495} INFO - "0" : 740
[2024-11-12T09:54:38.444+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - "0" : 740
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:38.445+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:38.446+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7246376811594204,
[2024-11-12T09:54:38.446+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8936550491510277,
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:38.447+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:38.448+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:38.448+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:38.455+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/151 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.151.34dddd32-2064-41a8-812d-fce7a00896ea.tmp
[2024-11-12T09:54:38.513+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.151.34dddd32-2064-41a8-812d-fce7a00896ea.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/151
[2024-11-12T09:54:38.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO MicroBatchExecution: Committed offsets for batch 151. Metadata OffsetSeqMetadata(0,1731405278440,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:38.554+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:38.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:38.585+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:38.587+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:38.596+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 148, 149, 150, 150
[2024-11-12T09:54:38.607+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:38.648+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:38.658+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Got job 151 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:38.659+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Final stage: ResultStage 151 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:38.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:38.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:38.660+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Submitting ResultStage 151 (MapPartitionsRDD[608] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:38.688+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:38.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:38.697+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:38.697+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:38.698+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 151 (MapPartitionsRDD[608] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:38.699+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSchedulerImpl: Adding task set 151.0 with 1 tasks resource profile 0
[2024-11-12T09:54:38.703+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSetManager: Starting task 0.0 in stage 151.0 (TID 151) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:38.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:38.924+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSetManager: Finished task 0.0 in stage 151.0 (TID 151) in 224 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:38.925+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool
[2024-11-12T09:54:38.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: ResultStage 151 (start at NativeMethodAccessorImpl.java:0) finished in 0.266 s
[2024-11-12T09:54:38.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Job 151 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:38.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 151: Stage finished
[2024-11-12T09:54:38.936+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO DAGScheduler: Job 151 finished: start at NativeMethodAccessorImpl.java:0, took 0.282231 s
[2024-11-12T09:54:38.936+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO FileFormatWriter: Start to commit write Job a64ec1dd-989c-48c8-8cd4-09540f0c2cb5.
[2024-11-12T09:54:38.944+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:38 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/151 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.151.b310b080-6576-4418-9b4d-f4a15d6b11e9.tmp
[2024-11-12T09:54:39.020+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.151.b310b080-6576-4418-9b4d-f4a15d6b11e9.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/151
[2024-11-12T09:54:39.021+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileStreamSinkLog: Current compact batch id = 151 min compaction batch id to delete = 49
[2024-11-12T09:54:39.032+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO ManifestFileCommitProtocol: Committed batch 151
[2024-11-12T09:54:39.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileFormatWriter: Write Job a64ec1dd-989c-48c8-8cd4-09540f0c2cb5 committed. Elapsed time: 101 ms.
[2024-11-12T09:54:39.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileFormatWriter: Finished processing stats for write job a64ec1dd-989c-48c8-8cd4-09540f0c2cb5.
[2024-11-12T09:54:39.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/151 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.151.8250ca3e-b3c6-4384-87ff-e133369b562c.tmp
[2024-11-12T09:54:39.126+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.151.8250ca3e-b3c6-4384-87ff-e133369b562c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/151
[2024-11-12T09:54:39.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:39.130+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:39.131+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:39.131+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:39.131+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:38.426Z",
[2024-11-12T09:54:39.133+0000] {spark_submit.py:495} INFO - "batchId" : 151,
[2024-11-12T09:54:39.133+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:39.135+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8920606601248885,
[2024-11-12T09:54:39.135+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4285714285714286,
[2024-11-12T09:54:39.136+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:39.136+0000] {spark_submit.py:495} INFO - "addBatch" : 460,
[2024-11-12T09:54:39.136+0000] {spark_submit.py:495} INFO - "commitOffsets" : 93,
[2024-11-12T09:54:39.136+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:39.137+0000] {spark_submit.py:495} INFO - "latestOffset" : 14,
[2024-11-12T09:54:39.137+0000] {spark_submit.py:495} INFO - "queryPlanning" : 53,
[2024-11-12T09:54:39.140+0000] {spark_submit.py:495} INFO - "triggerExecution" : 700,
[2024-11-12T09:54:39.144+0000] {spark_submit.py:495} INFO - "walCommit" : 73
[2024-11-12T09:54:39.145+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.152+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:39.153+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:39.154+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:39.154+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:39.156+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:39.156+0000] {spark_submit.py:495} INFO - "0" : 740
[2024-11-12T09:54:39.156+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.157+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.157+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:39.157+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:39.157+0000] {spark_submit.py:495} INFO - "0" : 741
[2024-11-12T09:54:39.158+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.158+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.159+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:39.159+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:39.159+0000] {spark_submit.py:495} INFO - "0" : 741
[2024-11-12T09:54:39.160+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.160+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.160+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:39.160+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8920606601248885,
[2024-11-12T09:54:39.161+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4285714285714286,
[2024-11-12T09:54:39.161+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:39.161+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:39.161+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:39.162+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:39.162+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.163+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:39.163+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:39.163+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:39.164+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:39.165+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.165+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/152 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.152.9081859a-3516-42ac-84bf-9c27c91c73b5.tmp
[2024-11-12T09:54:39.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.152.9081859a-3516-42ac-84bf-9c27c91c73b5.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/152
[2024-11-12T09:54:39.226+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO MicroBatchExecution: Committed offsets for batch 152. Metadata OffsetSeqMetadata(0,1731405279141,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:39.257+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:39.268+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:39.289+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:39.291+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:39.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 150, 151, 151
[2024-11-12T09:54:39.312+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:39.353+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:39.355+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Got job 152 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:39.356+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Final stage: ResultStage 152 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:39.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:39.365+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:39.366+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[612] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:39.381+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO MemoryStore: Block broadcast_152 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:39.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:39.388+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:39.397+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:39.399+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 152 (MapPartitionsRDD[612] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:39.401+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO TaskSchedulerImpl: Adding task set 152.0 with 1 tasks resource profile 0
[2024-11-12T09:54:39.402+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 152) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:39.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:39.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 152) in 316 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:39.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool
[2024-11-12T09:54:39.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: ResultStage 152 (start at NativeMethodAccessorImpl.java:0) finished in 0.360 s
[2024-11-12T09:54:39.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Job 152 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:39.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished
[2024-11-12T09:54:39.720+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO DAGScheduler: Job 152 finished: start at NativeMethodAccessorImpl.java:0, took 0.366990 s
[2024-11-12T09:54:39.722+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileFormatWriter: Start to commit write Job 43ed0591-240b-4e02-9d2c-4e89d6247d0f.
[2024-11-12T09:54:39.736+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/152 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.152.df7a0eef-ffa0-43f6-8978-d0de339a9552.tmp
[2024-11-12T09:54:39.798+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.152.df7a0eef-ffa0-43f6-8978-d0de339a9552.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/152
[2024-11-12T09:54:39.800+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileStreamSinkLog: Current compact batch id = 152 min compaction batch id to delete = 49
[2024-11-12T09:54:39.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO ManifestFileCommitProtocol: Committed batch 152
[2024-11-12T09:54:39.811+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileFormatWriter: Write Job 43ed0591-240b-4e02-9d2c-4e89d6247d0f committed. Elapsed time: 84 ms.
[2024-11-12T09:54:39.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO FileFormatWriter: Finished processing stats for write job 43ed0591-240b-4e02-9d2c-4e89d6247d0f.
[2024-11-12T09:54:39.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/152 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.152.8cf94aa0-3d02-49c6-bd13-80fe013a1267.tmp
[2024-11-12T09:54:39.885+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.152.8cf94aa0-3d02-49c6-bd13-80fe013a1267.tmp to hdfs://namenode:9000/spark_checkpoint/commits/152
[2024-11-12T09:54:39.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:39.889+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:39.891+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:39.892+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:39.892+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:39.129Z",
[2024-11-12T09:54:39.894+0000] {spark_submit.py:495} INFO - "batchId" : 152,
[2024-11-12T09:54:39.894+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:39.895+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.4224751066856332,
[2024-11-12T09:54:39.895+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:54:39.895+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:39.896+0000] {spark_submit.py:495} INFO - "addBatch" : 537,
[2024-11-12T09:54:39.896+0000] {spark_submit.py:495} INFO - "commitOffsets" : 79,
[2024-11-12T09:54:39.896+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:39.896+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-12T09:54:39.897+0000] {spark_submit.py:495} INFO - "queryPlanning" : 36,
[2024-11-12T09:54:39.897+0000] {spark_submit.py:495} INFO - "triggerExecution" : 755,
[2024-11-12T09:54:39.897+0000] {spark_submit.py:495} INFO - "walCommit" : 85
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "0" : 741
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.898+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - "0" : 742
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - "0" : 742
[2024-11-12T09:54:39.899+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.4224751066856332,
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3227513227513228,
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:39.900+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:39.901+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:39.901+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.901+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:39.901+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:39.902+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:39.902+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:39.903+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.903+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:39.904+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/153 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.153.5fbea592-8435-4bb7-b93d-a75209c16dd1.tmp
[2024-11-12T09:54:39.973+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.153.5fbea592-8435-4bb7-b93d-a75209c16dd1.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/153
[2024-11-12T09:54:39.976+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:39 INFO MicroBatchExecution: Committed offsets for batch 153. Metadata OffsetSeqMetadata(0,1731405279894,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:40.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:40.009+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:40.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:40.043+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:40.050+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 151, 152, 152
[2024-11-12T09:54:40.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:40.101+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:40.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Got job 153 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:40.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Final stage: ResultStage 153 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:40.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:40.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:40.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[616] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:40.155+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:40.161+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:40.164+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:40.167+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:40.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[616] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:40.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO TaskSchedulerImpl: Adding task set 153.0 with 1 tasks resource profile 0
[2024-11-12T09:54:40.170+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 153) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:40.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:40.783+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 153) in 613 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:40.783+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool
[2024-11-12T09:54:40.789+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: ResultStage 153 (start at NativeMethodAccessorImpl.java:0) finished in 0.681 s
[2024-11-12T09:54:40.795+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Job 153 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:40.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished
[2024-11-12T09:54:40.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO DAGScheduler: Job 153 finished: start at NativeMethodAccessorImpl.java:0, took 0.694181 s
[2024-11-12T09:54:40.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO FileFormatWriter: Start to commit write Job 0b622082-adda-421a-9f5b-363ebcdb521f.
[2024-11-12T09:54:40.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/153 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.153.4efccabc-23e9-40fe-8ea6-be19abe516aa.tmp
[2024-11-12T09:54:40.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.153.4efccabc-23e9-40fe-8ea6-be19abe516aa.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/153
[2024-11-12T09:54:40.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO FileStreamSinkLog: Current compact batch id = 153 min compaction batch id to delete = 49
[2024-11-12T09:54:40.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO ManifestFileCommitProtocol: Committed batch 153
[2024-11-12T09:54:40.875+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO FileFormatWriter: Write Job 0b622082-adda-421a-9f5b-363ebcdb521f committed. Elapsed time: 75 ms.
[2024-11-12T09:54:40.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO FileFormatWriter: Finished processing stats for write job 0b622082-adda-421a-9f5b-363ebcdb521f.
[2024-11-12T09:54:40.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/153 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.153.e32b77cf-8d9d-46c3-b1dd-1d3c143af20b.tmp
[2024-11-12T09:54:40.966+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.153.e32b77cf-8d9d-46c3-b1dd-1d3c143af20b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/153
[2024-11-12T09:54:40.968+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:40.971+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:40.976+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:40.977+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:40.977+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:39.887Z",
[2024-11-12T09:54:40.977+0000] {spark_submit.py:495} INFO - "batchId" : 153,
[2024-11-12T09:54:40.978+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:40.978+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3192612137203166,
[2024-11-12T09:54:40.978+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9267840593141798,
[2024-11-12T09:54:40.978+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:40.978+0000] {spark_submit.py:495} INFO - "addBatch" : 859,
[2024-11-12T09:54:40.978+0000] {spark_submit.py:495} INFO - "commitOffsets" : 90,
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "queryPlanning" : 37,
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1079,
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "walCommit" : 80
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - "0" : 742
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:40.979+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:40.982+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:40.983+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:40.985+0000] {spark_submit.py:495} INFO - "0" : 743
[2024-11-12T09:54:40.985+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:40.986+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:40.996+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:40.997+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:40.997+0000] {spark_submit.py:495} INFO - "0" : 743
[2024-11-12T09:54:40.999+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.000+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:41.001+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:41.002+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.3192612137203166,
[2024-11-12T09:54:41.002+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9267840593141798,
[2024-11-12T09:54:41.002+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:41.006+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:41.007+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:41.008+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:41.011+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.015+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:41.017+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:41.017+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:41.017+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:41.021+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.021+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/154 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.154.19951c14-31ed-4e5d-bf4d-8d85e9946e0f.tmp
[2024-11-12T09:54:41.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.154.19951c14-31ed-4e5d-bf4d-8d85e9946e0f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/154
[2024-11-12T09:54:41.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO MicroBatchExecution: Committed offsets for batch 154. Metadata OffsetSeqMetadata(0,1731405280982,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:41.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:41.136+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:41.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:41.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:41.184+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 152, 153, 153
[2024-11-12T09:54:41.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:41.235+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:41.237+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Got job 154 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:41.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Final stage: ResultStage 154 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:41.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:41.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:41.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Submitting ResultStage 154 (MapPartitionsRDD[620] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:41.283+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:54:41.292+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:41.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:41.296+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:41.297+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 154 (MapPartitionsRDD[620] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:41.298+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO TaskSchedulerImpl: Adding task set 154.0 with 1 tasks resource profile 0
[2024-11-12T09:54:41.299+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO TaskSetManager: Starting task 0.0 in stage 154.0 (TID 154) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:41.332+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:41.737+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 154) in 438 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:41.739+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool
[2024-11-12T09:54:41.743+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: ResultStage 154 (start at NativeMethodAccessorImpl.java:0) finished in 0.499 s
[2024-11-12T09:54:41.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Job 154 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:41.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 154: Stage finished
[2024-11-12T09:54:41.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO DAGScheduler: Job 154 finished: start at NativeMethodAccessorImpl.java:0, took 0.504300 s
[2024-11-12T09:54:41.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO FileFormatWriter: Start to commit write Job 7458495b-910d-4df7-8fd9-df4d064393d9.
[2024-11-12T09:54:41.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/154 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.154.50e63189-3e96-4484-9aab-93a91d2a3614.tmp
[2024-11-12T09:54:41.817+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.154.50e63189-3e96-4484-9aab-93a91d2a3614.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/154
[2024-11-12T09:54:41.820+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO FileStreamSinkLog: Current compact batch id = 154 min compaction batch id to delete = 49
[2024-11-12T09:54:41.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO ManifestFileCommitProtocol: Committed batch 154
[2024-11-12T09:54:41.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO FileFormatWriter: Write Job 7458495b-910d-4df7-8fd9-df4d064393d9 committed. Elapsed time: 83 ms.
[2024-11-12T09:54:41.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO FileFormatWriter: Finished processing stats for write job 7458495b-910d-4df7-8fd9-df4d064393d9.
[2024-11-12T09:54:41.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/154 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.154.2617320c-c569-4423-a8b9-748c0cce5ae5.tmp
[2024-11-12T09:54:41.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.154.2617320c-c569-4423-a8b9-748c0cce5ae5.tmp to hdfs://namenode:9000/spark_checkpoint/commits/154
[2024-11-12T09:54:41.916+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:41.917+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:41.918+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:41.918+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:41.919+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:40.967Z",
[2024-11-12T09:54:41.924+0000] {spark_submit.py:495} INFO - "batchId" : 154,
[2024-11-12T09:54:41.925+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:41.925+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9259259259259258,
[2024-11-12T09:54:41.925+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0548523206751055,
[2024-11-12T09:54:41.925+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:41.926+0000] {spark_submit.py:495} INFO - "addBatch" : 675,
[2024-11-12T09:54:41.930+0000] {spark_submit.py:495} INFO - "commitOffsets" : 90,
[2024-11-12T09:54:41.934+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:41.936+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:54:41.937+0000] {spark_submit.py:495} INFO - "queryPlanning" : 31,
[2024-11-12T09:54:41.937+0000] {spark_submit.py:495} INFO - "triggerExecution" : 948,
[2024-11-12T09:54:41.939+0000] {spark_submit.py:495} INFO - "walCommit" : 124
[2024-11-12T09:54:41.940+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:41.942+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:41.942+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:41.942+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:41.942+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:41.943+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:41.943+0000] {spark_submit.py:495} INFO - "0" : 743
[2024-11-12T09:54:41.945+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.946+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:41.946+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:41.947+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:41.948+0000] {spark_submit.py:495} INFO - "0" : 744
[2024-11-12T09:54:41.948+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.948+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:41.948+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:41.948+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:41.949+0000] {spark_submit.py:495} INFO - "0" : 744
[2024-11-12T09:54:41.949+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.949+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:41.949+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9259259259259258,
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0548523206751055,
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.950+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:41.952+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:41.953+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:41.954+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:41.954+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.954+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:41.954+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:41 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/155 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.155.995095d6-d7d5-48b2-9f04-ed0b6e4357de.tmp
[2024-11-12T09:54:42.021+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.155.995095d6-d7d5-48b2-9f04-ed0b6e4357de.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/155
[2024-11-12T09:54:42.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO MicroBatchExecution: Committed offsets for batch 155. Metadata OffsetSeqMetadata(0,1731405281936,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:42.052+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:42.057+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:42.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:42.093+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:42.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 153, 154, 154
[2024-11-12T09:54:42.116+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:42.164+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:42.173+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Got job 155 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:42.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Final stage: ResultStage 155 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:42.175+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:42.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:42.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[624] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:42.212+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:54:42.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.5 MiB)
[2024-11-12T09:54:42.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.7 MiB)
[2024-11-12T09:54:42.265+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO SparkContext: Created broadcast 155 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:42.271+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[624] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:42.278+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks resource profile 0
[2024-11-12T09:54:42.280+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_149_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:42.284+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 155) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:42.285+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_149_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:42.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_150_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:42.318+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_150_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:42.331+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:42.337+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_153_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:42.343+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_153_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:42.359+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_152_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:42.361+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_152_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:42.379+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_151_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:42.405+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_151_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:42.422+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_154_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:42.440+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO BlockManagerInfo: Removed broadcast_154_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:42.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 155) in 474 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:42.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool
[2024-11-12T09:54:42.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: ResultStage 155 (start at NativeMethodAccessorImpl.java:0) finished in 0.568 s
[2024-11-12T09:54:42.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Job 155 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:42.746+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished
[2024-11-12T09:54:42.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO DAGScheduler: Job 155 finished: start at NativeMethodAccessorImpl.java:0, took 0.580783 s
[2024-11-12T09:54:42.747+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO FileFormatWriter: Start to commit write Job 1e0d97b6-4237-4f3d-870a-54a5773bd524.
[2024-11-12T09:54:42.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/155 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.155.7fc36820-cbc7-4c64-ab7a-03337876931d.tmp
[2024-11-12T09:54:42.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.155.7fc36820-cbc7-4c64-ab7a-03337876931d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/155
[2024-11-12T09:54:42.825+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO FileStreamSinkLog: Current compact batch id = 155 min compaction batch id to delete = 49
[2024-11-12T09:54:42.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO ManifestFileCommitProtocol: Committed batch 155
[2024-11-12T09:54:42.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO FileFormatWriter: Write Job 1e0d97b6-4237-4f3d-870a-54a5773bd524 committed. Elapsed time: 87 ms.
[2024-11-12T09:54:42.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO FileFormatWriter: Finished processing stats for write job 1e0d97b6-4237-4f3d-870a-54a5773bd524.
[2024-11-12T09:54:42.847+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/155 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.155.c5c16e09-dfec-4435-8901-5b3bd475082c.tmp
[2024-11-12T09:54:42.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.155.c5c16e09-dfec-4435-8901-5b3bd475082c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/155
[2024-11-12T09:54:42.929+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:42.930+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:41.916Z",
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "batchId" : 155,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.053740779768177,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9900990099009901,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "addBatch" : 770,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "commitOffsets" : 90,
[2024-11-12T09:54:42.931+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "latestOffset" : 20,
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "queryPlanning" : 39,
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1010,
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "walCommit" : 86
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:42.932+0000] {spark_submit.py:495} INFO - "0" : 744
[2024-11-12T09:54:42.944+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:42.944+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:42.944+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:42.945+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:42.946+0000] {spark_submit.py:495} INFO - "0" : 745
[2024-11-12T09:54:42.946+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:42.946+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:42.947+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:42.947+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:42.947+0000] {spark_submit.py:495} INFO - "0" : 745
[2024-11-12T09:54:42.948+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:42.948+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:42.948+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:42.949+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.053740779768177,
[2024-11-12T09:54:42.949+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9900990099009901,
[2024-11-12T09:54:42.950+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:42.950+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:42.950+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:42.950+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:42.951+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:42.951+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:42.951+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:42.951+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:42.951+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:42.952+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:42.952+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:42.957+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:42 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/156 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.156.92c31d47-f27e-480c-9457-1c91145b2e80.tmp
[2024-11-12T09:54:43.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.156.92c31d47-f27e-480c-9457-1c91145b2e80.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/156
[2024-11-12T09:54:43.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO MicroBatchExecution: Committed offsets for batch 156. Metadata OffsetSeqMetadata(0,1731405282949,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:43.446+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:43.448+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:43.497+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:43.499+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:43.518+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 154, 155, 155
[2024-11-12T09:54:43.521+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:43.573+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:43.574+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Got job 156 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:43.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Final stage: ResultStage 156 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:43.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:43.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:43.575+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[628] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:43.604+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:43.611+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:43.613+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:43.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:43.615+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[628] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:43.616+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO TaskSchedulerImpl: Adding task set 156.0 with 1 tasks resource profile 0
[2024-11-12T09:54:43.619+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 156) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:43.651+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:43.808+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 156) in 192 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:43.811+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool
[2024-11-12T09:54:43.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: ResultStage 156 (start at NativeMethodAccessorImpl.java:0) finished in 0.235 s
[2024-11-12T09:54:43.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Job 156 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:43.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished
[2024-11-12T09:54:43.815+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO DAGScheduler: Job 156 finished: start at NativeMethodAccessorImpl.java:0, took 0.238481 s
[2024-11-12T09:54:43.816+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO FileFormatWriter: Start to commit write Job fcc0a938-cac9-499b-88d2-d1cee533dbb5.
[2024-11-12T09:54:43.827+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:43 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/156 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.156.6a93144f-47fb-446a-a361-0c230a16dfc8.tmp
[2024-11-12T09:54:44.301+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.156.6a93144f-47fb-446a-a361-0c230a16dfc8.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/156
[2024-11-12T09:54:44.302+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileStreamSinkLog: Current compact batch id = 156 min compaction batch id to delete = 49
[2024-11-12T09:54:44.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO ManifestFileCommitProtocol: Committed batch 156
[2024-11-12T09:54:44.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileFormatWriter: Write Job fcc0a938-cac9-499b-88d2-d1cee533dbb5 committed. Elapsed time: 491 ms.
[2024-11-12T09:54:44.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileFormatWriter: Finished processing stats for write job fcc0a938-cac9-499b-88d2-d1cee533dbb5.
[2024-11-12T09:54:44.316+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/156 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.156.1628061d-34da-4443-964f-ad168f09167d.tmp
[2024-11-12T09:54:44.377+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.156.1628061d-34da-4443-964f-ad168f09167d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/156
[2024-11-12T09:54:44.387+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:44.391+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:44.391+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:44.391+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:44.391+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:42.928Z",
[2024-11-12T09:54:44.391+0000] {spark_submit.py:495} INFO - "batchId" : 156,
[2024-11-12T09:54:44.392+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:44.392+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9881422924901185,
[2024-11-12T09:54:44.392+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6901311249137336,
[2024-11-12T09:54:44.392+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:44.393+0000] {spark_submit.py:495} INFO - "addBatch" : 851,
[2024-11-12T09:54:44.393+0000] {spark_submit.py:495} INFO - "commitOffsets" : 72,
[2024-11-12T09:54:44.393+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:44.393+0000] {spark_submit.py:495} INFO - "latestOffset" : 21,
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - "queryPlanning" : 30,
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1449,
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - "walCommit" : 471
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:44.394+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - "0" : 745
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:44.395+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:44.396+0000] {spark_submit.py:495} INFO - "0" : 746
[2024-11-12T09:54:44.396+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.396+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.396+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:44.397+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:44.397+0000] {spark_submit.py:495} INFO - "0" : 746
[2024-11-12T09:54:44.398+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.398+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.398+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:44.399+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9881422924901185,
[2024-11-12T09:54:44.400+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6901311249137336,
[2024-11-12T09:54:44.409+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:44.409+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:44.410+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:44.410+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:44.410+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.410+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:44.410+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:44.411+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:44.411+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:44.411+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.411+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.411+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/157 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.157.796cc530-842b-4569-86e1-5b5c757c7d90.tmp
[2024-11-12T09:54:44.441+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.157.796cc530-842b-4569-86e1-5b5c757c7d90.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/157
[2024-11-12T09:54:44.443+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO MicroBatchExecution: Committed offsets for batch 157. Metadata OffsetSeqMetadata(0,1731405284390,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:44.456+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.476+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.477+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.485+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 155, 156, 156
[2024-11-12T09:54:44.486+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:44.528+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:44.530+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Got job 157 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:44.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Final stage: ResultStage 157 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:44.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:44.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:44.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:44.562+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:44.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:44.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:44.584+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:44.584+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:44.585+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks resource profile 0
[2024-11-12T09:54:44.585+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 157) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:44.614+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:44.725+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 157) in 139 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:44.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool
[2024-11-12T09:54:44.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: ResultStage 157 (start at NativeMethodAccessorImpl.java:0) finished in 0.194 s
[2024-11-12T09:54:44.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Job 157 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:44.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished
[2024-11-12T09:54:44.734+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO DAGScheduler: Job 157 finished: start at NativeMethodAccessorImpl.java:0, took 0.205511 s
[2024-11-12T09:54:44.735+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileFormatWriter: Start to commit write Job 42ef335b-6b2b-4287-9e3b-95e5c1015dd0.
[2024-11-12T09:54:44.740+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/157 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.157.8d6c8cdf-cce6-48c7-8d01-c806b784d8df.tmp
[2024-11-12T09:54:44.775+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.157.8d6c8cdf-cce6-48c7-8d01-c806b784d8df.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/157
[2024-11-12T09:54:44.776+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileStreamSinkLog: Current compact batch id = 157 min compaction batch id to delete = 49
[2024-11-12T09:54:44.778+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO ManifestFileCommitProtocol: Committed batch 157
[2024-11-12T09:54:44.778+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileFormatWriter: Write Job 42ef335b-6b2b-4287-9e3b-95e5c1015dd0 committed. Elapsed time: 42 ms.
[2024-11-12T09:54:44.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileFormatWriter: Finished processing stats for write job 42ef335b-6b2b-4287-9e3b-95e5c1015dd0.
[2024-11-12T09:54:44.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/157 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.157.5e8534a3-cf5f-4b8a-b070-55b2bbc84a2c.tmp
[2024-11-12T09:54:44.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.157.5e8534a3-cf5f-4b8a-b070-55b2bbc84a2c.tmp to hdfs://namenode:9000/spark_checkpoint/commits/157
[2024-11-12T09:54:44.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:44.837+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:44.838+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:44.838+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:44.838+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:44.382Z",
[2024-11-12T09:54:44.838+0000] {spark_submit.py:495} INFO - "batchId" : 157,
[2024-11-12T09:54:44.839+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:44.839+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.687757909215956,
[2024-11-12T09:54:44.839+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.2123893805309733,
[2024-11-12T09:54:44.839+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:44.839+0000] {spark_submit.py:495} INFO - "addBatch" : 311,
[2024-11-12T09:54:44.844+0000] {spark_submit.py:495} INFO - "commitOffsets" : 56,
[2024-11-12T09:54:44.844+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "latestOffset" : 8,
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "queryPlanning" : 17,
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "triggerExecution" : 452,
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "walCommit" : 51
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:44.845+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - "0" : 746
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:44.846+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:44.852+0000] {spark_submit.py:495} INFO - "0" : 747
[2024-11-12T09:54:44.853+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.854+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.854+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:44.854+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:44.855+0000] {spark_submit.py:495} INFO - "0" : 747
[2024-11-12T09:54:44.855+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.856+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:44.856+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:44.857+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.687757909215956,
[2024-11-12T09:54:44.857+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.2123893805309733,
[2024-11-12T09:54:44.858+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:44.858+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:44.859+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:44.859+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:44.859+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.860+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:44.861+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:44.861+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:44.861+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:44.861+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.861+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:44.863+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/158 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.158.70decf82-158a-497a-bf0c-5a42ef7a2158.tmp
[2024-11-12T09:54:44.918+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.158.70decf82-158a-497a-bf0c-5a42ef7a2158.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/158
[2024-11-12T09:54:44.918+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO MicroBatchExecution: Committed offsets for batch 158. Metadata OffsetSeqMetadata(0,1731405284849,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:44.929+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.967+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:44.976+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 156, 157, 157
[2024-11-12T09:54:44.981+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:44 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:45.022+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:45.023+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Got job 158 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:45.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Final stage: ResultStage 158 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:45.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:45.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:45.025+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[636] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:45.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:45.067+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:45.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:45.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO SparkContext: Created broadcast 158 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:45.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 158 (MapPartitionsRDD[636] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:45.074+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO TaskSchedulerImpl: Adding task set 158.0 with 1 tasks resource profile 0
[2024-11-12T09:54:45.075+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 158) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:45.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:45.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 158) in 674 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:45.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool
[2024-11-12T09:54:45.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: ResultStage 158 (start at NativeMethodAccessorImpl.java:0) finished in 0.725 s
[2024-11-12T09:54:45.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Job 158 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:45.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished
[2024-11-12T09:54:45.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO DAGScheduler: Job 158 finished: start at NativeMethodAccessorImpl.java:0, took 0.728242 s
[2024-11-12T09:54:45.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO FileFormatWriter: Start to commit write Job 064d16ac-7ead-4cda-960f-5f5f8b853fb8.
[2024-11-12T09:54:45.763+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/158 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.158.4986a900-e96f-4100-95b5-5d59528bba8e.tmp
[2024-11-12T09:54:45.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.158.4986a900-e96f-4100-95b5-5d59528bba8e.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/158
[2024-11-12T09:54:45.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO FileStreamSinkLog: Current compact batch id = 158 min compaction batch id to delete = 49
[2024-11-12T09:54:45.808+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO ManifestFileCommitProtocol: Committed batch 158
[2024-11-12T09:54:45.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO FileFormatWriter: Write Job 064d16ac-7ead-4cda-960f-5f5f8b853fb8 committed. Elapsed time: 55 ms.
[2024-11-12T09:54:45.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO FileFormatWriter: Finished processing stats for write job 064d16ac-7ead-4cda-960f-5f5f8b853fb8.
[2024-11-12T09:54:45.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/158 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.158.a19ac62e-567f-44d8-aa0c-9755ad0a55af.tmp
[2024-11-12T09:54:45.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.158.a19ac62e-567f-44d8-aa0c-9755ad0a55af.tmp to hdfs://namenode:9000/spark_checkpoint/commits/158
[2024-11-12T09:54:45.871+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:45.874+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:45.875+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:45.875+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:45.876+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:44.836Z",
[2024-11-12T09:54:45.876+0000] {spark_submit.py:495} INFO - "batchId" : 158,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.202643171806167,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9718172983479106,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "addBatch" : 864,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "commitOffsets" : 58,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1029,
[2024-11-12T09:54:45.877+0000] {spark_submit.py:495} INFO - "walCommit" : 69
[2024-11-12T09:54:45.878+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:45.879+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:45.879+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "0" : 747
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - "0" : 748
[2024-11-12T09:54:45.880+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - "0" : 748
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.202643171806167,
[2024-11-12T09:54:45.881+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9718172983479106,
[2024-11-12T09:54:45.882+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:45.882+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:45.883+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:45.883+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:45.883+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:45.884+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:45.884+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:45.885+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:45.885+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:45.885+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:45.889+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:45.890+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/159 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.159.3e744f15-976e-4f82-ac35-3c00ada67b1e.tmp
[2024-11-12T09:54:45.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.159.3e744f15-976e-4f82-ac35-3c00ada67b1e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/159
[2024-11-12T09:54:45.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO MicroBatchExecution: Committed offsets for batch 159. Metadata OffsetSeqMetadata(0,1731405285882,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:45.949+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:45.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:45.975+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:45.977+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:45.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 158
[2024-11-12T09:54:45.992+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:45 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:46.031+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:46.033+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Got job 159 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:46.034+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Final stage: ResultStage 159 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:46.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:46.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:46.035+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[640] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:46.082+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:46.085+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:46.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:46.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:46.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[640] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:46.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks resource profile 0
[2024-11-12T09:54:46.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 159) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:46.137+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:46.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 159) in 746 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:46.839+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool
[2024-11-12T09:54:46.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: ResultStage 159 (start at NativeMethodAccessorImpl.java:0) finished in 0.803 s
[2024-11-12T09:54:46.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Job 159 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:46.842+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished
[2024-11-12T09:54:46.842+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO DAGScheduler: Job 159 finished: start at NativeMethodAccessorImpl.java:0, took 0.808202 s
[2024-11-12T09:54:46.842+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO FileFormatWriter: Start to commit write Job 2636a8ee-ec1d-4fe1-8566-cbc37bf4c0dc.
[2024-11-12T09:54:46.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:46 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/159.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.159.compact.90b3d808-00a4-407d-886c-3467c5cad224.tmp
[2024-11-12T09:54:47.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.159.compact.90b3d808-00a4-407d-886c-3467c5cad224.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/159.compact
[2024-11-12T09:54:47.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO FileStreamSinkLog: Current compact batch id = 159 min compaction batch id to delete = 59
[2024-11-12T09:54:47.129+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO ManifestFileCommitProtocol: Committed batch 159
[2024-11-12T09:54:47.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO FileFormatWriter: Write Job 2636a8ee-ec1d-4fe1-8566-cbc37bf4c0dc committed. Elapsed time: 289 ms.
[2024-11-12T09:54:47.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO FileFormatWriter: Finished processing stats for write job 2636a8ee-ec1d-4fe1-8566-cbc37bf4c0dc.
[2024-11-12T09:54:47.138+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/159 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.159.8a659cb9-a9a9-4e5d-9d8d-f48185aaa6a3.tmp
[2024-11-12T09:54:47.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.159.8a659cb9-a9a9-4e5d-9d8d-f48185aaa6a3.tmp to hdfs://namenode:9000/spark_checkpoint/commits/159
[2024-11-12T09:54:47.216+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:47.216+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:47.216+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:47.217+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:47.217+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:45.871Z",
[2024-11-12T09:54:47.217+0000] {spark_submit.py:495} INFO - "batchId" : 159,
[2024-11-12T09:54:47.218+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:47.219+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9661835748792271,
[2024-11-12T09:54:47.219+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7446016381236039,
[2024-11-12T09:54:47.219+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:47.219+0000] {spark_submit.py:495} INFO - "addBatch" : 1171,
[2024-11-12T09:54:47.219+0000] {spark_submit.py:495} INFO - "commitOffsets" : 85,
[2024-11-12T09:54:47.219+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:47.220+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:54:47.220+0000] {spark_submit.py:495} INFO - "queryPlanning" : 19,
[2024-11-12T09:54:47.220+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1343,
[2024-11-12T09:54:47.220+0000] {spark_submit.py:495} INFO - "walCommit" : 55
[2024-11-12T09:54:47.222+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:47.226+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:47.227+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:47.227+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:47.227+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:47.228+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:47.229+0000] {spark_submit.py:495} INFO - "0" : 748
[2024-11-12T09:54:47.230+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:47.230+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:47.230+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:47.231+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:47.231+0000] {spark_submit.py:495} INFO - "0" : 749
[2024-11-12T09:54:47.231+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:47.231+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:47.232+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:47.232+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:47.232+0000] {spark_submit.py:495} INFO - "0" : 749
[2024-11-12T09:54:47.232+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:47.233+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:47.233+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:47.233+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9661835748792271,
[2024-11-12T09:54:47.234+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7446016381236039,
[2024-11-12T09:54:47.235+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:47.235+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:47.235+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:47.235+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:47.236+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:47.236+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:47.238+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:47.251+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:47.252+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:47.253+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:47.253+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:47.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/160 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.160.12399955-574a-4ae2-853a-4bca6524cf3c.tmp
[2024-11-12T09:54:47.713+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.160.12399955-574a-4ae2-853a-4bca6524cf3c.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/160
[2024-11-12T09:54:47.714+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO MicroBatchExecution: Committed offsets for batch 160. Metadata OffsetSeqMetadata(0,1731405287225,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:47.731+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:47.733+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:47.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:47.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:47.756+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 158, 159
[2024-11-12T09:54:47.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:47.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:47.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Got job 160 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:47.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Final stage: ResultStage 160 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:47.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:47.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:47.802+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Submitting ResultStage 160 (MapPartitionsRDD[644] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:47.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:54:47.830+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:47.831+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:47.831+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:47.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 160 (MapPartitionsRDD[644] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:47.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO TaskSchedulerImpl: Adding task set 160.0 with 1 tasks resource profile 0
[2024-11-12T09:54:47.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 160) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:47.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:47.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 160) in 149 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:47.985+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool
[2024-11-12T09:54:47.985+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: ResultStage 160 (start at NativeMethodAccessorImpl.java:0) finished in 0.182 s
[2024-11-12T09:54:47.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Job 160 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:47.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 160: Stage finished
[2024-11-12T09:54:47.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO DAGScheduler: Job 160 finished: start at NativeMethodAccessorImpl.java:0, took 0.186994 s
[2024-11-12T09:54:47.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO FileFormatWriter: Start to commit write Job 887bb8bc-65a2-4eed-8a55-ae26fc896ac5.
[2024-11-12T09:54:47.997+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:47 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/160 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.160.6e662bc0-62a0-4adc-bf5b-83f675830717.tmp
[2024-11-12T09:54:48.063+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.160.6e662bc0-62a0-4adc-bf5b-83f675830717.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/160
[2024-11-12T09:54:48.064+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO FileStreamSinkLog: Current compact batch id = 160 min compaction batch id to delete = 59
[2024-11-12T09:54:48.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO ManifestFileCommitProtocol: Committed batch 160
[2024-11-12T09:54:48.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO FileFormatWriter: Write Job 887bb8bc-65a2-4eed-8a55-ae26fc896ac5 committed. Elapsed time: 86 ms.
[2024-11-12T09:54:48.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO FileFormatWriter: Finished processing stats for write job 887bb8bc-65a2-4eed-8a55-ae26fc896ac5.
[2024-11-12T09:54:48.086+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/160 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.160.a7e31898-f4b1-446d-ac2f-42e29b1eb3f8.tmp
[2024-11-12T09:54:48.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.160.a7e31898-f4b1-446d-ac2f-42e29b1eb3f8.tmp to hdfs://namenode:9000/spark_checkpoint/commits/160
[2024-11-12T09:54:48.133+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:48.136+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:48.136+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:48.136+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:48.136+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:47.215Z",
[2024-11-12T09:54:48.136+0000] {spark_submit.py:495} INFO - "batchId" : 160,
[2024-11-12T09:54:48.145+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:48.146+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.744047619047619,
[2024-11-12T09:54:48.147+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0928961748633879,
[2024-11-12T09:54:48.147+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:48.147+0000] {spark_submit.py:495} INFO - "addBatch" : 336,
[2024-11-12T09:54:48.148+0000] {spark_submit.py:495} INFO - "commitOffsets" : 59,
[2024-11-12T09:54:48.148+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:48.148+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-12T09:54:48.148+0000] {spark_submit.py:495} INFO - "queryPlanning" : 20,
[2024-11-12T09:54:48.149+0000] {spark_submit.py:495} INFO - "triggerExecution" : 915,
[2024-11-12T09:54:48.149+0000] {spark_submit.py:495} INFO - "walCommit" : 489
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - "0" : 749
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:48.150+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - "0" : 750
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - "0" : 750
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:48.151+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:48.152+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:48.152+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.744047619047619,
[2024-11-12T09:54:48.152+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0928961748633879,
[2024-11-12T09:54:48.153+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:48.153+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:48.153+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:48.153+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:48.154+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:48.158+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/161 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.161.a454ab56-a8ea-4970-8fb3-250c6ce6242e.tmp
[2024-11-12T09:54:48.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.161.a454ab56-a8ea-4970-8fb3-250c6ce6242e.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/161
[2024-11-12T09:54:48.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO MicroBatchExecution: Committed offsets for batch 161. Metadata OffsetSeqMetadata(0,1731405288150,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:48.238+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:48.239+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:48.253+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:48.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:48.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 158, 159, 160, 160
[2024-11-12T09:54:48.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:48.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:48.304+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO DAGScheduler: Got job 161 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:48.305+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO DAGScheduler: Final stage: ResultStage 161 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:48.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:48.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:48.306+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[648] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:48.326+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:54:48.341+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.8 MiB)
[2024-11-12T09:54:48.343+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_159_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:48.343+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:48.343+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:48.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 161 (MapPartitionsRDD[648] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:48.345+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks resource profile 0
[2024-11-12T09:54:48.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_159_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:48.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 161) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:48.358+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_158_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:48.362+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_158_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:48.375+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_155_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:48.383+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_155_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:48.385+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:48.386+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_156_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:48.392+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_156_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:48.409+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_160_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:48.410+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_160_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:48.425+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_157_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:48.435+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:48 INFO BlockManagerInfo: Removed broadcast_157_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:49.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 161) in 857 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:49.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool
[2024-11-12T09:54:49.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: ResultStage 161 (start at NativeMethodAccessorImpl.java:0) finished in 0.897 s
[2024-11-12T09:54:49.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Job 161 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:49.206+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished
[2024-11-12T09:54:49.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Job 161 finished: start at NativeMethodAccessorImpl.java:0, took 0.900620 s
[2024-11-12T09:54:49.208+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO FileFormatWriter: Start to commit write Job aa6433c9-c28e-42de-a28e-668cee5412aa.
[2024-11-12T09:54:49.222+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/161 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.161.364b5ad5-f84c-471e-a155-75a42ed1d100.tmp
[2024-11-12T09:54:49.285+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.161.364b5ad5-f84c-471e-a155-75a42ed1d100.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/161
[2024-11-12T09:54:49.287+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO FileStreamSinkLog: Current compact batch id = 161 min compaction batch id to delete = 59
[2024-11-12T09:54:49.293+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO ManifestFileCommitProtocol: Committed batch 161
[2024-11-12T09:54:49.294+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO FileFormatWriter: Write Job aa6433c9-c28e-42de-a28e-668cee5412aa committed. Elapsed time: 87 ms.
[2024-11-12T09:54:49.295+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO FileFormatWriter: Finished processing stats for write job aa6433c9-c28e-42de-a28e-668cee5412aa.
[2024-11-12T09:54:49.303+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/161 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.161.eb998106-e472-499b-aaa1-134f4236234d.tmp
[2024-11-12T09:54:49.360+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.161.eb998106-e472-499b-aaa1-134f4236234d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/161
[2024-11-12T09:54:49.364+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:49.364+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:49.364+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:49.365+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:49.365+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:48.131Z",
[2024-11-12T09:54:49.365+0000] {spark_submit.py:495} INFO - "batchId" : 161,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0917030567685588,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8136696501220504,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "addBatch" : 1051,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "commitOffsets" : 67,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "latestOffset" : 19,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "queryPlanning" : 36,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1229,
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "walCommit" : 54
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:49.366+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:49.367+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:49.367+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:49.367+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:49.367+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:49.371+0000] {spark_submit.py:495} INFO - "0" : 750
[2024-11-12T09:54:49.374+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:49.376+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:49.376+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:49.377+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:49.377+0000] {spark_submit.py:495} INFO - "0" : 751
[2024-11-12T09:54:49.377+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:49.378+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:49.378+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:49.378+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:49.378+0000] {spark_submit.py:495} INFO - "0" : 751
[2024-11-12T09:54:49.379+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:49.379+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:49.379+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:49.379+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0917030567685588,
[2024-11-12T09:54:49.380+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8136696501220504,
[2024-11-12T09:54:49.380+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:49.381+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:49.382+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/162 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.162.3a85b0bb-ad41-4d98-b85c-a5577bce55c2.tmp
[2024-11-12T09:54:49.419+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.162.3a85b0bb-ad41-4d98-b85c-a5577bce55c2.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/162
[2024-11-12T09:54:49.420+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO MicroBatchExecution: Committed offsets for batch 162. Metadata OffsetSeqMetadata(0,1731405289372,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:49.438+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:49.439+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:49.454+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:49.456+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:49.461+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 160, 161, 161
[2024-11-12T09:54:49.462+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:49.490+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Got job 162 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Final stage: ResultStage 162 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:49.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[652] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:49.511+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:49.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:49.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:49.514+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:49.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 162 (MapPartitionsRDD[652] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:49.515+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSchedulerImpl: Adding task set 162.0 with 1 tasks resource profile 0
[2024-11-12T09:54:49.520+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 162) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:49.538+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:49.783+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 162) in 264 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:49.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool
[2024-11-12T09:54:49.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: ResultStage 162 (start at NativeMethodAccessorImpl.java:0) finished in 0.293 s
[2024-11-12T09:54:49.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Job 162 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:49.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished
[2024-11-12T09:54:49.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO DAGScheduler: Job 162 finished: start at NativeMethodAccessorImpl.java:0, took 0.294782 s
[2024-11-12T09:54:49.786+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO FileFormatWriter: Start to commit write Job 42a40b66-e346-4dfc-ba25-91a7a4c939cb.
[2024-11-12T09:54:49.804+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:49 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/162 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.162.74b22ef3-31c0-40e7-aa84-99d1cce87509.tmp
[2024-11-12T09:54:50.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.162.74b22ef3-31c0-40e7-aa84-99d1cce87509.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/162
[2024-11-12T09:54:50.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileStreamSinkLog: Current compact batch id = 162 min compaction batch id to delete = 59
[2024-11-12T09:54:50.261+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO ManifestFileCommitProtocol: Committed batch 162
[2024-11-12T09:54:50.262+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileFormatWriter: Write Job 42a40b66-e346-4dfc-ba25-91a7a4c939cb committed. Elapsed time: 476 ms.
[2024-11-12T09:54:50.263+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileFormatWriter: Finished processing stats for write job 42a40b66-e346-4dfc-ba25-91a7a4c939cb.
[2024-11-12T09:54:50.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/162 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.162.359d81e0-d79f-459d-a68d-f7552851a87a.tmp
[2024-11-12T09:54:50.322+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.162.359d81e0-d79f-459d-a68d-f7552851a87a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/162
[2024-11-12T09:54:50.330+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:50.331+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:50.331+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:50.332+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:50.332+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:49.361Z",
[2024-11-12T09:54:50.332+0000] {spark_submit.py:495} INFO - "batchId" : 162,
[2024-11-12T09:54:50.333+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:50.334+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8130081300813008,
[2024-11-12T09:54:50.334+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.040582726326743,
[2024-11-12T09:54:50.335+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:50.336+0000] {spark_submit.py:495} INFO - "addBatch" : 820,
[2024-11-12T09:54:50.336+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-12T09:54:50.336+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:50.336+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "queryPlanning" : 21,
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "triggerExecution" : 961,
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "walCommit" : 47
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - "0" : 751
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.337+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - "0" : 752
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - "0" : 752
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.338+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8130081300813008,
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.040582726326743,
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.339+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:50.340+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:50.340+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:50.340+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:50.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.340+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.346+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/163 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.163.3feb4113-2e6e-43bf-a090-b6797695c3c4.tmp
[2024-11-12T09:54:50.404+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.163.3feb4113-2e6e-43bf-a090-b6797695c3c4.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/163
[2024-11-12T09:54:50.406+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO MicroBatchExecution: Committed offsets for batch 163. Metadata OffsetSeqMetadata(0,1731405290339,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:50.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:50.428+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:50.451+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:50.452+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:50.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 161, 162, 162
[2024-11-12T09:54:50.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:50.491+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:50.498+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Got job 163 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:50.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Final stage: ResultStage 163 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:50.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:50.500+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:50.506+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Submitting ResultStage 163 (MapPartitionsRDD[656] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:50.524+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:50.529+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:50.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:50.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO SparkContext: Created broadcast 163 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:50.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 163 (MapPartitionsRDD[656] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:50.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks resource profile 0
[2024-11-12T09:54:50.537+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 163) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:50.563+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:50.774+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 163) in 238 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:50.775+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool
[2024-11-12T09:54:50.776+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: ResultStage 163 (start at NativeMethodAccessorImpl.java:0) finished in 0.281 s
[2024-11-12T09:54:50.776+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Job 163 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:50.776+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 163: Stage finished
[2024-11-12T09:54:50.779+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO DAGScheduler: Job 163 finished: start at NativeMethodAccessorImpl.java:0, took 0.284097 s
[2024-11-12T09:54:50.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileFormatWriter: Start to commit write Job 423ca22d-b2d2-47a8-a658-6327b31ac992.
[2024-11-12T09:54:50.792+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/163 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.163.b1529969-4c4e-48d1-a6c1-26acb7710d7c.tmp
[2024-11-12T09:54:50.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.163.b1529969-4c4e-48d1-a6c1-26acb7710d7c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/163
[2024-11-12T09:54:50.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileStreamSinkLog: Current compact batch id = 163 min compaction batch id to delete = 59
[2024-11-12T09:54:50.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO ManifestFileCommitProtocol: Committed batch 163
[2024-11-12T09:54:50.845+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileFormatWriter: Write Job 423ca22d-b2d2-47a8-a658-6327b31ac992 committed. Elapsed time: 67 ms.
[2024-11-12T09:54:50.846+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO FileFormatWriter: Finished processing stats for write job 423ca22d-b2d2-47a8-a658-6327b31ac992.
[2024-11-12T09:54:50.855+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/163 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.163.6f62cc4c-213d-4965-9b99-55b219191225.tmp
[2024-11-12T09:54:50.932+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.163.6f62cc4c-213d-4965-9b99-55b219191225.tmp to hdfs://namenode:9000/spark_checkpoint/commits/163
[2024-11-12T09:54:50.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:50.939+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:50.939+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:50.939+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:50.940+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:50.324Z",
[2024-11-12T09:54:50.940+0000] {spark_submit.py:495} INFO - "batchId" : 163,
[2024-11-12T09:54:50.940+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:50.940+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0384215991692627,
[2024-11-12T09:54:50.945+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.6447368421052633,
[2024-11-12T09:54:50.945+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:50.945+0000] {spark_submit.py:495} INFO - "addBatch" : 413,
[2024-11-12T09:54:50.945+0000] {spark_submit.py:495} INFO - "commitOffsets" : 86,
[2024-11-12T09:54:50.945+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:50.945+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "queryPlanning" : 25,
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "triggerExecution" : 608,
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "walCommit" : 65
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:50.946+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:50.947+0000] {spark_submit.py:495} INFO - "0" : 752
[2024-11-12T09:54:50.947+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.947+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.948+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:50.948+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:50.948+0000] {spark_submit.py:495} INFO - "0" : 753
[2024-11-12T09:54:50.950+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.951+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.951+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:50.951+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:50.952+0000] {spark_submit.py:495} INFO - "0" : 753
[2024-11-12T09:54:50.952+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.953+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:50.953+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:50.954+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0384215991692627,
[2024-11-12T09:54:50.954+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.6447368421052633,
[2024-11-12T09:54:50.954+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:50.954+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:50.955+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:50.955+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:50.955+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.955+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:50.956+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:50.956+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:50.956+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:50.956+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.957+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:50.958+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:50 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/164 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.164.16419c65-5697-43c7-bf69-c8ea50edce6a.tmp
[2024-11-12T09:54:51.421+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.164.16419c65-5697-43c7-bf69-c8ea50edce6a.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/164
[2024-11-12T09:54:51.423+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO MicroBatchExecution: Committed offsets for batch 164. Metadata OffsetSeqMetadata(0,1731405290943,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:51.446+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:51.449+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:51.458+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:51.460+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:51.469+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 162, 163, 163
[2024-11-12T09:54:51.474+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:51.504+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:51.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Got job 164 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:51.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Final stage: ResultStage 164 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:51.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:51.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:51.505+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Submitting ResultStage 164 (MapPartitionsRDD[660] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:51.528+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:51.531+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:51.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:51.533+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:51.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 164 (MapPartitionsRDD[660] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:51.534+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO TaskSchedulerImpl: Adding task set 164.0 with 1 tasks resource profile 0
[2024-11-12T09:54:51.535+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 164) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:51.588+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:51.856+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 164) in 321 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:51.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool
[2024-11-12T09:54:51.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: ResultStage 164 (start at NativeMethodAccessorImpl.java:0) finished in 0.351 s
[2024-11-12T09:54:51.858+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Job 164 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:51.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 164: Stage finished
[2024-11-12T09:54:51.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO DAGScheduler: Job 164 finished: start at NativeMethodAccessorImpl.java:0, took 0.353655 s
[2024-11-12T09:54:51.859+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO FileFormatWriter: Start to commit write Job 33c4b2ce-8f4b-4aaa-b8e3-115655bf698f.
[2024-11-12T09:54:51.874+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/164 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.164.483dab5d-852d-48ce-89ed-a95499d29e5b.tmp
[2024-11-12T09:54:51.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.164.483dab5d-852d-48ce-89ed-a95499d29e5b.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/164
[2024-11-12T09:54:51.933+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO FileStreamSinkLog: Current compact batch id = 164 min compaction batch id to delete = 59
[2024-11-12T09:54:51.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO ManifestFileCommitProtocol: Committed batch 164
[2024-11-12T09:54:51.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO FileFormatWriter: Write Job 33c4b2ce-8f4b-4aaa-b8e3-115655bf698f committed. Elapsed time: 78 ms.
[2024-11-12T09:54:51.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO FileFormatWriter: Finished processing stats for write job 33c4b2ce-8f4b-4aaa-b8e3-115655bf698f.
[2024-11-12T09:54:51.945+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:51 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/164 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.164.2199cf61-6538-4878-af92-b98a52bf6a84.tmp
[2024-11-12T09:54:52.006+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.164.2199cf61-6538-4878-af92-b98a52bf6a84.tmp to hdfs://namenode:9000/spark_checkpoint/commits/164
[2024-11-12T09:54:52.007+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:50.936Z",
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "batchId" : 164,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.6339869281045751,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9345794392523364,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "addBatch" : 487,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "commitOffsets" : 67,
[2024-11-12T09:54:52.008+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "queryPlanning" : 28,
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1070,
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "walCommit" : 478
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "0" : 753
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:52.009+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:52.010+0000] {spark_submit.py:495} INFO - "0" : 754
[2024-11-12T09:54:52.012+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:52.013+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:52.013+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:52.014+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:52.014+0000] {spark_submit.py:495} INFO - "0" : 754
[2024-11-12T09:54:52.015+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:52.015+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:52.015+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:52.015+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.6339869281045751,
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9345794392523364,
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:52.016+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:52.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/165 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.165.b0abd113-4b7a-4940-b241-1b1dbee5a4f8.tmp
[2024-11-12T09:54:52.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.165.b0abd113-4b7a-4940-b241-1b1dbee5a4f8.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/165
[2024-11-12T09:54:52.073+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO MicroBatchExecution: Committed offsets for batch 165. Metadata OffsetSeqMetadata(0,1731405292020,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:52.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:52.096+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:52.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:52.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:52.128+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 163, 164, 164
[2024-11-12T09:54:52.130+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:52.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:52.168+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Got job 165 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:52.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Final stage: ResultStage 165 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:52.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:52.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:52.169+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Submitting ResultStage 165 (MapPartitionsRDD[664] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:52.194+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:52.199+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:52.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:52.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:52.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[664] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:52.203+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks resource profile 0
[2024-11-12T09:54:52.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 165) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:52.228+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:52.832+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 165) in 626 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:52.833+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool
[2024-11-12T09:54:52.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: ResultStage 165 (start at NativeMethodAccessorImpl.java:0) finished in 0.663 s
[2024-11-12T09:54:52.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Job 165 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:52.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 165: Stage finished
[2024-11-12T09:54:52.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO DAGScheduler: Job 165 finished: start at NativeMethodAccessorImpl.java:0, took 0.667828 s
[2024-11-12T09:54:52.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO FileFormatWriter: Start to commit write Job 95ea4975-4a54-4346-9d3c-b6a55e947346.
[2024-11-12T09:54:52.853+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/165 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.165.c8733a74-a448-4912-b0ed-565431a2deda.tmp
[2024-11-12T09:54:52.927+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.165.c8733a74-a448-4912-b0ed-565431a2deda.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/165
[2024-11-12T09:54:52.928+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO FileStreamSinkLog: Current compact batch id = 165 min compaction batch id to delete = 59
[2024-11-12T09:54:52.930+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO ManifestFileCommitProtocol: Committed batch 165
[2024-11-12T09:54:52.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO FileFormatWriter: Write Job 95ea4975-4a54-4346-9d3c-b6a55e947346 committed. Elapsed time: 96 ms.
[2024-11-12T09:54:52.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO FileFormatWriter: Finished processing stats for write job 95ea4975-4a54-4346-9d3c-b6a55e947346.
[2024-11-12T09:54:52.952+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/165 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.165.22d20101-efe0-4e71-bd9d-9ceaa9b6a101.tmp
[2024-11-12T09:54:53.000+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:52 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.165.22d20101-efe0-4e71-bd9d-9ceaa9b6a101.tmp to hdfs://namenode:9000/spark_checkpoint/commits/165
[2024-11-12T09:54:53.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:53.002+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:53.002+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:53.003+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:53.003+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:52.007Z",
[2024-11-12T09:54:53.004+0000] {spark_submit.py:495} INFO - "batchId" : 165,
[2024-11-12T09:54:53.009+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:53.010+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9337068160597572,
[2024-11-12T09:54:53.010+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0080645161290323,
[2024-11-12T09:54:53.011+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:53.011+0000] {spark_submit.py:495} INFO - "addBatch" : 832,
[2024-11-12T09:54:53.012+0000] {spark_submit.py:495} INFO - "commitOffsets" : 68,
[2024-11-12T09:54:53.012+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:53.012+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "queryPlanning" : 25,
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "triggerExecution" : 992,
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "walCommit" : 52
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:53.013+0000] {spark_submit.py:495} INFO - "0" : 754
[2024-11-12T09:54:53.014+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.014+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.014+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:53.014+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - "0" : 755
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - "0" : 755
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.015+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:53.016+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9337068160597572,
[2024-11-12T09:54:53.021+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0080645161290323,
[2024-11-12T09:54:53.022+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:53.022+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:53.023+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:53.024+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:53.024+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.026+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:53.027+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:53.027+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:53.028+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:53.028+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.028+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.054+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/166 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.166.3ec35d1f-b7b7-422b-bfd7-5614757f924f.tmp
[2024-11-12T09:54:53.516+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.166.3ec35d1f-b7b7-422b-bfd7-5614757f924f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/166
[2024-11-12T09:54:53.519+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO MicroBatchExecution: Committed offsets for batch 166. Metadata OffsetSeqMetadata(0,1731405293030,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:53.556+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:53.559+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:53.583+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:53.588+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:53.594+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 164, 165, 165
[2024-11-12T09:54:53.595+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:53.642+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:53.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Got job 166 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:53.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Final stage: ResultStage 166 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:53.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:53.643+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:53.644+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Submitting ResultStage 166 (MapPartitionsRDD[668] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:53.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:54:53.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:53.672+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:53.677+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:53.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 166 (MapPartitionsRDD[668] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:53.679+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks resource profile 0
[2024-11-12T09:54:53.681+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 166) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:53.717+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:53.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 166) in 133 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:53.813+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool
[2024-11-12T09:54:53.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: ResultStage 166 (start at NativeMethodAccessorImpl.java:0) finished in 0.169 s
[2024-11-12T09:54:53.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Job 166 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:53.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 166: Stage finished
[2024-11-12T09:54:53.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO DAGScheduler: Job 166 finished: start at NativeMethodAccessorImpl.java:0, took 0.171829 s
[2024-11-12T09:54:53.814+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO FileFormatWriter: Start to commit write Job c60648fb-b000-4e1f-8f27-05934d1c27c4.
[2024-11-12T09:54:53.826+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/166 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.166.5ae02325-fcf5-4a8e-943d-fc66fcc5c93e.tmp
[2024-11-12T09:54:53.869+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.166.5ae02325-fcf5-4a8e-943d-fc66fcc5c93e.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/166
[2024-11-12T09:54:53.870+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO FileStreamSinkLog: Current compact batch id = 166 min compaction batch id to delete = 59
[2024-11-12T09:54:53.871+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO ManifestFileCommitProtocol: Committed batch 166
[2024-11-12T09:54:53.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO FileFormatWriter: Write Job c60648fb-b000-4e1f-8f27-05934d1c27c4 committed. Elapsed time: 57 ms.
[2024-11-12T09:54:53.873+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO FileFormatWriter: Finished processing stats for write job c60648fb-b000-4e1f-8f27-05934d1c27c4.
[2024-11-12T09:54:53.888+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/166 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.166.44deaec5-66ff-42ca-ab68-99fa77cbae4b.tmp
[2024-11-12T09:54:53.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.166.44deaec5-66ff-42ca-ab68-99fa77cbae4b.tmp to hdfs://namenode:9000/spark_checkpoint/commits/166
[2024-11-12T09:54:53.933+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:53.934+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:53.934+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:53.935+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:53.935+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:53.001Z",
[2024-11-12T09:54:53.935+0000] {spark_submit.py:495} INFO - "batchId" : 166,
[2024-11-12T09:54:53.936+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:53.937+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0060362173038229,
[2024-11-12T09:54:53.937+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.075268817204301,
[2024-11-12T09:54:53.938+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:53.939+0000] {spark_submit.py:495} INFO - "addBatch" : 308,
[2024-11-12T09:54:53.939+0000] {spark_submit.py:495} INFO - "commitOffsets" : 58,
[2024-11-12T09:54:53.939+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:53.939+0000] {spark_submit.py:495} INFO - "latestOffset" : 29,
[2024-11-12T09:54:53.939+0000] {spark_submit.py:495} INFO - "queryPlanning" : 46,
[2024-11-12T09:54:53.939+0000] {spark_submit.py:495} INFO - "triggerExecution" : 930,
[2024-11-12T09:54:53.940+0000] {spark_submit.py:495} INFO - "walCommit" : 487
[2024-11-12T09:54:53.940+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.940+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:53.940+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:53.940+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:53.941+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:53.941+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:53.943+0000] {spark_submit.py:495} INFO - "0" : 755
[2024-11-12T09:54:53.944+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.944+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - "0" : 756
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - "0" : 756
[2024-11-12T09:54:53.945+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.946+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:53.946+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:53.946+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0060362173038229,
[2024-11-12T09:54:53.946+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.075268817204301,
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.947+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:53.950+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/167 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.167.98dee5d0-10df-4c08-88f4-556e2c632a22.tmp
[2024-11-12T09:54:53.990+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.167.98dee5d0-10df-4c08-88f4-556e2c632a22.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/167
[2024-11-12T09:54:53.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:53 INFO MicroBatchExecution: Committed offsets for batch 167. Metadata OffsetSeqMetadata(0,1731405293938,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:54.011+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:54.016+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:54.042+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:54.045+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:54.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 165, 166, 166
[2024-11-12T09:54:54.060+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:54.095+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:54.099+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Got job 167 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:54.102+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Final stage: ResultStage 167 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:54.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:54.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:54.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[672] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:54.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:54:54.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.8 MiB)
[2024-11-12T09:54:54.166+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_164_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:54.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:54.171+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO SparkContext: Created broadcast 167 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:54.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 167 (MapPartitionsRDD[672] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:54.172+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks resource profile 0
[2024-11-12T09:54:54.174+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_164_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:54.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 167) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:54.182+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_166_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:54.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_166_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:54.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_161_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:54.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_161_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:54.205+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:54.223+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_162_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:54.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_162_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:54.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_165_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:54.232+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_165_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:54.256+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_163_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:54.259+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO BlockManagerInfo: Removed broadcast_163_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:54:54.881+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 167) in 693 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:54.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool
[2024-11-12T09:54:54.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: ResultStage 167 (start at NativeMethodAccessorImpl.java:0) finished in 0.775 s
[2024-11-12T09:54:54.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Job 167 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:54.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished
[2024-11-12T09:54:54.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO DAGScheduler: Job 167 finished: start at NativeMethodAccessorImpl.java:0, took 0.786680 s
[2024-11-12T09:54:54.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO FileFormatWriter: Start to commit write Job 3d4bdae9-200b-41a1-9623-d032e4421f57.
[2024-11-12T09:54:54.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/167 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.167.095ce6d6-2ff8-470e-ac19-3956fb113b26.tmp
[2024-11-12T09:54:54.980+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.167.095ce6d6-2ff8-470e-ac19-3956fb113b26.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/167
[2024-11-12T09:54:54.980+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO FileStreamSinkLog: Current compact batch id = 167 min compaction batch id to delete = 59
[2024-11-12T09:54:54.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO ManifestFileCommitProtocol: Committed batch 167
[2024-11-12T09:54:54.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO FileFormatWriter: Write Job 3d4bdae9-200b-41a1-9623-d032e4421f57 committed. Elapsed time: 100 ms.
[2024-11-12T09:54:54.984+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO FileFormatWriter: Finished processing stats for write job 3d4bdae9-200b-41a1-9623-d032e4421f57.
[2024-11-12T09:54:54.991+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:54 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/167 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.167.7502314f-6a6a-4117-b9ff-e82f507b7241.tmp
[2024-11-12T09:54:55.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.167.7502314f-6a6a-4117-b9ff-e82f507b7241.tmp to hdfs://namenode:9000/spark_checkpoint/commits/167
[2024-11-12T09:54:55.044+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:55.044+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:55.045+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:55.045+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:55.045+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:53.932Z",
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "batchId" : 167,
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0741138560687433,
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9025270758122743,
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "addBatch" : 963,
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "commitOffsets" : 57,
[2024-11-12T09:54:55.046+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:55.049+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:55.049+0000] {spark_submit.py:495} INFO - "queryPlanning" : 28,
[2024-11-12T09:54:55.050+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1108,
[2024-11-12T09:54:55.050+0000] {spark_submit.py:495} INFO - "walCommit" : 52
[2024-11-12T09:54:55.051+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:55.051+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:55.051+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:55.051+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:55.051+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:55.052+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:55.053+0000] {spark_submit.py:495} INFO - "0" : 756
[2024-11-12T09:54:55.053+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:55.054+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:55.054+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - "0" : 757
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - "0" : 757
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:55.055+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:55.056+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0741138560687433,
[2024-11-12T09:54:55.056+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9025270758122743,
[2024-11-12T09:54:55.056+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:55.057+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:55.057+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:55.057+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:55.057+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:55.057+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:55.058+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:55.059+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:55.059+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:55.059+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:55.059+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:55.069+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/168 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.168.07fc790e-5f75-41ca-bdb9-cfa4fc347efc.tmp
[2024-11-12T09:54:55.110+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.168.07fc790e-5f75-41ca-bdb9-cfa4fc347efc.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/168
[2024-11-12T09:54:55.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO MicroBatchExecution: Committed offsets for batch 168. Metadata OffsetSeqMetadata(0,1731405295054,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:55.139+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:55.142+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:55.160+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:55.162+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:55.176+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 166, 167, 167
[2024-11-12T09:54:55.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:55.225+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:55.228+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Got job 168 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:55.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Final stage: ResultStage 168 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:55.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:55.230+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:55.232+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[676] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:55.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:54:55.264+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:54:55.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:55.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:55.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[676] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:55.267+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks resource profile 0
[2024-11-12T09:54:55.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 168) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:55.308+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:54:55.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 168) in 623 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:55.894+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool
[2024-11-12T09:54:55.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: ResultStage 168 (start at NativeMethodAccessorImpl.java:0) finished in 0.662 s
[2024-11-12T09:54:55.895+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Job 168 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:55.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished
[2024-11-12T09:54:55.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO DAGScheduler: Job 168 finished: start at NativeMethodAccessorImpl.java:0, took 0.669366 s
[2024-11-12T09:54:55.896+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO FileFormatWriter: Start to commit write Job dc8875bc-dfef-4633-9b71-34603b0d5fa2.
[2024-11-12T09:54:55.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/168 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.168.56ac5d40-baff-498e-ba77-db8c439b4072.tmp
[2024-11-12T09:54:55.979+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.168.56ac5d40-baff-498e-ba77-db8c439b4072.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/168
[2024-11-12T09:54:55.980+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO FileStreamSinkLog: Current compact batch id = 168 min compaction batch id to delete = 59
[2024-11-12T09:54:55.983+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO ManifestFileCommitProtocol: Committed batch 168
[2024-11-12T09:54:55.985+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO FileFormatWriter: Write Job dc8875bc-dfef-4633-9b71-34603b0d5fa2 committed. Elapsed time: 88 ms.
[2024-11-12T09:54:55.986+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:55 INFO FileFormatWriter: Finished processing stats for write job dc8875bc-dfef-4633-9b71-34603b0d5fa2.
[2024-11-12T09:54:56.001+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/168 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.168.f2f87197-6783-4c8f-a702-458dd570ddad.tmp
[2024-11-12T09:54:56.119+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.168.f2f87197-6783-4c8f-a702-458dd570ddad.tmp to hdfs://namenode:9000/spark_checkpoint/commits/168
[2024-11-12T09:54:56.121+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:56.122+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:56.122+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:56.123+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:56.123+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:55.043Z",
[2024-11-12T09:54:56.123+0000] {spark_submit.py:495} INFO - "batchId" : 168,
[2024-11-12T09:54:56.124+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:56.124+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9000900090009001,
[2024-11-12T09:54:56.124+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9310986964618249,
[2024-11-12T09:54:56.124+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:56.125+0000] {spark_submit.py:495} INFO - "addBatch" : 842,
[2024-11-12T09:54:56.125+0000] {spark_submit.py:495} INFO - "commitOffsets" : 131,
[2024-11-12T09:54:56.125+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:56.126+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-12T09:54:56.126+0000] {spark_submit.py:495} INFO - "queryPlanning" : 32,
[2024-11-12T09:54:56.126+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1074,
[2024-11-12T09:54:56.126+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:54:56.126+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:56.126+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:56.127+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:56.127+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:56.127+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:56.137+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:56.138+0000] {spark_submit.py:495} INFO - "0" : 757
[2024-11-12T09:54:56.139+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:56.139+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:56.139+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:56.139+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:56.139+0000] {spark_submit.py:495} INFO - "0" : 758
[2024-11-12T09:54:56.140+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:56.140+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:56.140+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:56.140+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:56.146+0000] {spark_submit.py:495} INFO - "0" : 758
[2024-11-12T09:54:56.147+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:56.147+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:56.148+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:56.148+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9000900090009001,
[2024-11-12T09:54:56.148+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9310986964618249,
[2024-11-12T09:54:56.153+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:56.154+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:56.154+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:56.154+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:56.155+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:56.155+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:56.156+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:56.156+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:56.156+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:56.157+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:56.157+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:56.213+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/169 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.169.a1c6bf23-6097-4449-bfdb-a9584f9a9b55.tmp
[2024-11-12T09:54:56.734+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.169.a1c6bf23-6097-4449-bfdb-a9584f9a9b55.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/169
[2024-11-12T09:54:56.735+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO MicroBatchExecution: Committed offsets for batch 169. Metadata OffsetSeqMetadata(0,1731405296126,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:56.765+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:56.767+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:56.785+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:56.787+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:56.798+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 167, 168, 168
[2024-11-12T09:54:56.801+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:56.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:56.840+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO DAGScheduler: Got job 169 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:56.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO DAGScheduler: Final stage: ResultStage 169 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:56.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:56.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:56.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO DAGScheduler: Submitting ResultStage 169 (MapPartitionsRDD[680] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:56.921+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-12T09:54:56.932+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.1 MiB)
[2024-11-12T09:54:56.933+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:56.937+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:56.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 169 (MapPartitionsRDD[680] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:56.938+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO TaskSchedulerImpl: Adding task set 169.0 with 1 tasks resource profile 0
[2024-11-12T09:54:56.945+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:56 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 169) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:57.020+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:54:57.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 169) in 305 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:57.246+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool
[2024-11-12T09:54:57.247+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: ResultStage 169 (start at NativeMethodAccessorImpl.java:0) finished in 0.405 s
[2024-11-12T09:54:57.256+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Job 169 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:57.266+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 169: Stage finished
[2024-11-12T09:54:57.269+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Job 169 finished: start at NativeMethodAccessorImpl.java:0, took 0.430702 s
[2024-11-12T09:54:57.270+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO FileFormatWriter: Start to commit write Job 7f62aeef-067f-4953-981a-04601cc3139e.
[2024-11-12T09:54:57.300+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/169.compact using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.169.compact.abf81496-3de6-4bce-9e41-30765163f556.tmp
[2024-11-12T09:54:57.567+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.169.compact.abf81496-3de6-4bce-9e41-30765163f556.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/169.compact
[2024-11-12T09:54:57.571+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO FileStreamSinkLog: Current compact batch id = 169 min compaction batch id to delete = 69
[2024-11-12T09:54:57.588+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO ManifestFileCommitProtocol: Committed batch 169
[2024-11-12T09:54:57.592+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO FileFormatWriter: Write Job 7f62aeef-067f-4953-981a-04601cc3139e committed. Elapsed time: 318 ms.
[2024-11-12T09:54:57.593+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO FileFormatWriter: Finished processing stats for write job 7f62aeef-067f-4953-981a-04601cc3139e.
[2024-11-12T09:54:57.605+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/169 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.169.46bda887-8719-4224-88a2-a9de5fcc01bd.tmp
[2024-11-12T09:54:57.651+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.169.46bda887-8719-4224-88a2-a9de5fcc01bd.tmp to hdfs://namenode:9000/spark_checkpoint/commits/169
[2024-11-12T09:54:57.654+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:57.655+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:57.657+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:57.658+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:57.659+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:56.120Z",
[2024-11-12T09:54:57.660+0000] {spark_submit.py:495} INFO - "batchId" : 169,
[2024-11-12T09:54:57.660+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:57.660+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9285051067780873,
[2024-11-12T09:54:57.660+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6535947712418301,
[2024-11-12T09:54:57.660+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "addBatch" : 815,
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "queryPlanning" : 35,
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1530,
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - "walCommit" : 608
[2024-11-12T09:54:57.661+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:57.662+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:57.663+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:57.665+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:57.666+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:57.666+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:57.667+0000] {spark_submit.py:495} INFO - "0" : 758
[2024-11-12T09:54:57.667+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:57.667+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:57.668+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:57.669+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:57.679+0000] {spark_submit.py:495} INFO - "0" : 759
[2024-11-12T09:54:57.687+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:57.687+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:57.687+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:57.687+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:57.690+0000] {spark_submit.py:495} INFO - "0" : 759
[2024-11-12T09:54:57.691+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:57.692+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:57.692+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:57.692+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9285051067780873,
[2024-11-12T09:54:57.692+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6535947712418301,
[2024-11-12T09:54:57.692+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:57.692+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:57.693+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:57.693+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:57.694+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:57.694+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:57.695+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:57.695+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:57.695+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:57.696+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:57.696+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:57.696+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/170 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.170.3693b08c-0d84-4c5b-80b4-ee5357d7f6c9.tmp
[2024-11-12T09:54:57.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.170.3693b08c-0d84-4c5b-80b4-ee5357d7f6c9.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/170
[2024-11-12T09:54:57.754+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO MicroBatchExecution: Committed offsets for batch 170. Metadata OffsetSeqMetadata(0,1731405297667,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:57.771+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:57.774+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:57.788+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:57.791+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:57.809+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 167, 168, 168, 169
[2024-11-12T09:54:57.817+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:57.855+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:57.860+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Got job 170 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:57.861+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Final stage: ResultStage 170 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:57.863+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:57.865+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:57.866+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[684] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:57.897+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 320.7 KiB, free 432.8 MiB)
[2024-11-12T09:54:57.906+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.7 MiB)
[2024-11-12T09:54:57.912+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:57.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:57.914+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 170 (MapPartitionsRDD[684] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:57.915+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO TaskSchedulerImpl: Adding task set 170.0 with 1 tasks resource profile 0
[2024-11-12T09:54:57.917+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 170) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:57.952+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:57 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:54:58.109+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 170) in 190 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:58.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool
[2024-11-12T09:54:58.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: ResultStage 170 (start at NativeMethodAccessorImpl.java:0) finished in 0.246 s
[2024-11-12T09:54:58.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:58.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished
[2024-11-12T09:54:58.113+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Job 170 finished: start at NativeMethodAccessorImpl.java:0, took 0.253062 s
[2024-11-12T09:54:58.114+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO FileFormatWriter: Start to commit write Job 44716da2-bf5e-4a2b-9cdc-b06678f9aad4.
[2024-11-12T09:54:58.127+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/170 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.170.07ec35d9-bcf2-48ae-8a83-5aec790c97c5.tmp
[2024-11-12T09:54:58.178+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.170.07ec35d9-bcf2-48ae-8a83-5aec790c97c5.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/170
[2024-11-12T09:54:58.180+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO FileStreamSinkLog: Current compact batch id = 170 min compaction batch id to delete = 69
[2024-11-12T09:54:58.182+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO ManifestFileCommitProtocol: Committed batch 170
[2024-11-12T09:54:58.182+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO FileFormatWriter: Write Job 44716da2-bf5e-4a2b-9cdc-b06678f9aad4 committed. Elapsed time: 73 ms.
[2024-11-12T09:54:58.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO FileFormatWriter: Finished processing stats for write job 44716da2-bf5e-4a2b-9cdc-b06678f9aad4.
[2024-11-12T09:54:58.189+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/170 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.170.1b7d6cf1-d11a-4f1e-af96-ab6147672674.tmp
[2024-11-12T09:54:58.231+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.170.1b7d6cf1-d11a-4f1e-af96-ab6147672674.tmp to hdfs://namenode:9000/spark_checkpoint/commits/170
[2024-11-12T09:54:58.234+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:57.654Z",
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "batchId" : 170,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.651890482398957,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.733102253032929,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "addBatch" : 405,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "commitOffsets" : 49,
[2024-11-12T09:54:58.235+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "triggerExecution" : 577,
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "walCommit" : 85
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - "0" : 759
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:58.236+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "0" : 760
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "0" : 760
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.651890482398957,
[2024-11-12T09:54:58.237+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.733102253032929,
[2024-11-12T09:54:58.238+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:58.239+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:58.240+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:58.258+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/171 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.171.23d021a6-c0ac-45dc-a7ec-48873227cc9f.tmp
[2024-11-12T09:54:58.712+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.171.23d021a6-c0ac-45dc-a7ec-48873227cc9f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/171
[2024-11-12T09:54:58.721+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO MicroBatchExecution: Committed offsets for batch 171. Metadata OffsetSeqMetadata(0,1731405298241,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:58.730+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:58.732+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:58.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:58.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:58.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 168, 169, 170, 170
[2024-11-12T09:54:58.756+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:58.793+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:58.795+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Got job 171 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:58.797+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Final stage: ResultStage 171 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:58.798+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:58.798+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:58.799+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Submitting ResultStage 171 (MapPartitionsRDD[688] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:58.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 320.7 KiB, free 432.4 MiB)
[2024-11-12T09:54:58.834+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 432.3 MiB)
[2024-11-12T09:54:58.835+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:58.836+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO SparkContext: Created broadcast 171 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:58.838+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[688] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:58.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO TaskSchedulerImpl: Adding task set 171.0 with 1 tasks resource profile 0
[2024-11-12T09:54:58.843+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 171) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:58.877+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:58 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:54:59.047+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 171) in 204 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:59.055+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool
[2024-11-12T09:54:59.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: ResultStage 171 (start at NativeMethodAccessorImpl.java:0) finished in 0.248 s
[2024-11-12T09:54:59.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Job 171 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:59.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 171: Stage finished
[2024-11-12T09:54:59.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Job 171 finished: start at NativeMethodAccessorImpl.java:0, took 0.260220 s
[2024-11-12T09:54:59.056+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileFormatWriter: Start to commit write Job 0b259577-48df-4e33-8a8f-5585a74cdfed.
[2024-11-12T09:54:59.061+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/171 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.171.3d2a67f6-d218-46a2-8710-6110ea0c9c6e.tmp
[2024-11-12T09:54:59.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.171.3d2a67f6-d218-46a2-8710-6110ea0c9c6e.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/171
[2024-11-12T09:54:59.104+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileStreamSinkLog: Current compact batch id = 171 min compaction batch id to delete = 69
[2024-11-12T09:54:59.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO ManifestFileCommitProtocol: Committed batch 171
[2024-11-12T09:54:59.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileFormatWriter: Write Job 0b259577-48df-4e33-8a8f-5585a74cdfed committed. Elapsed time: 50 ms.
[2024-11-12T09:54:59.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileFormatWriter: Finished processing stats for write job 0b259577-48df-4e33-8a8f-5585a74cdfed.
[2024-11-12T09:54:59.115+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/171 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.171.00fe162a-45c5-4965-be39-48d7129dfa87.tmp
[2024-11-12T09:54:59.555+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.171.00fe162a-45c5-4965-be39-48d7129dfa87.tmp to hdfs://namenode:9000/spark_checkpoint/commits/171
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:58.234Z",
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "batchId" : 171,
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7241379310344829,
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.757002271006813,
[2024-11-12T09:54:59.557+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:54:59.558+0000] {spark_submit.py:495} INFO - "addBatch" : 369,
[2024-11-12T09:54:59.558+0000] {spark_submit.py:495} INFO - "commitOffsets" : 450,
[2024-11-12T09:54:59.559+0000] {spark_submit.py:495} INFO - "getBatch" : 2,
[2024-11-12T09:54:59.566+0000] {spark_submit.py:495} INFO - "latestOffset" : 7,
[2024-11-12T09:54:59.566+0000] {spark_submit.py:495} INFO - "queryPlanning" : 15,
[2024-11-12T09:54:59.567+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1321,
[2024-11-12T09:54:59.573+0000] {spark_submit.py:495} INFO - "walCommit" : 475
[2024-11-12T09:54:59.573+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:59.573+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:54:59.573+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:54:59.573+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:54:59.573+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:54:59.574+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:59.574+0000] {spark_submit.py:495} INFO - "0" : 760
[2024-11-12T09:54:59.574+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:59.574+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:59.574+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:54:59.574+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:59.575+0000] {spark_submit.py:495} INFO - "0" : 761
[2024-11-12T09:54:59.575+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:59.575+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:59.575+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:54:59.576+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:54:59.576+0000] {spark_submit.py:495} INFO - "0" : 761
[2024-11-12T09:54:59.576+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:59.578+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:54:59.578+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:54:59.578+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7241379310344829,
[2024-11-12T09:54:59.578+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.757002271006813,
[2024-11-12T09:54:59.579+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:54:59.579+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:54:59.579+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:54:59.579+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:59.580+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:54:59.582+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/172 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.172.6b7d621c-c3d5-45fa-91a1-311e8e8a39b3.tmp
[2024-11-12T09:54:59.629+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.172.6b7d621c-c3d5-45fa-91a1-311e8e8a39b3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/172
[2024-11-12T09:54:59.630+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO MicroBatchExecution: Committed offsets for batch 172. Metadata OffsetSeqMetadata(0,1731405299573,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:54:59.646+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:59.648+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:59.664+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:59.669+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:54:59.686+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 170, 171, 171
[2024-11-12T09:54:59.689+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:54:59.715+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:54:59.717+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Got job 172 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:54:59.717+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Final stage: ResultStage 172 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:54:59.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:54:59.718+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:54:59.719+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Submitting ResultStage 172 (MapPartitionsRDD[692] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:54:59.745+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-12T09:54:59.748+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:54:59.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:59.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:54:59.751+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 172 (MapPartitionsRDD[692] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:54:59.752+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSchedulerImpl: Adding task set 172.0 with 1 tasks resource profile 0
[2024-11-12T09:54:59.758+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 172) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:54:59.782+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:54:59.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 172) in 136 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:54:59.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool
[2024-11-12T09:54:59.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: ResultStage 172 (start at NativeMethodAccessorImpl.java:0) finished in 0.174 s
[2024-11-12T09:54:59.893+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Job 172 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:54:59.900+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 172: Stage finished
[2024-11-12T09:54:59.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO DAGScheduler: Job 172 finished: start at NativeMethodAccessorImpl.java:0, took 0.177535 s
[2024-11-12T09:54:59.901+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileFormatWriter: Start to commit write Job 833bf086-a60d-45df-9e96-81612235851b.
[2024-11-12T09:54:59.906+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/172 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.172.2d045c0f-6796-4af5-8cd4-2794424c3d5f.tmp
[2024-11-12T09:54:59.960+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.172.2d045c0f-6796-4af5-8cd4-2794424c3d5f.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/172
[2024-11-12T09:54:59.961+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileStreamSinkLog: Current compact batch id = 172 min compaction batch id to delete = 69
[2024-11-12T09:54:59.970+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO ManifestFileCommitProtocol: Committed batch 172
[2024-11-12T09:54:59.971+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileFormatWriter: Write Job 833bf086-a60d-45df-9e96-81612235851b committed. Elapsed time: 71 ms.
[2024-11-12T09:54:59.972+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO FileFormatWriter: Finished processing stats for write job 833bf086-a60d-45df-9e96-81612235851b.
[2024-11-12T09:54:59.987+0000] {spark_submit.py:495} INFO - 24/11/12 09:54:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/172 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.172.71b5bf53-a4a2-45e9-98a7-b86e5acb1c2e.tmp
[2024-11-12T09:55:00.024+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.172.71b5bf53-a4a2-45e9-98a7-b86e5acb1c2e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/172
[2024-11-12T09:55:00.026+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:55:00.026+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:55:00.027+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:55:00.027+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:55:00.027+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:54:59.556Z",
[2024-11-12T09:55:00.027+0000] {spark_submit.py:495} INFO - "batchId" : 172,
[2024-11-12T09:55:00.028+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:55:00.028+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.75642965204236,
[2024-11-12T09:55:00.028+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.1367521367521367,
[2024-11-12T09:55:00.028+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:55:00.029+0000] {spark_submit.py:495} INFO - "addBatch" : 319,
[2024-11-12T09:55:00.029+0000] {spark_submit.py:495} INFO - "commitOffsets" : 53,
[2024-11-12T09:55:00.029+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:55:00.029+0000] {spark_submit.py:495} INFO - "latestOffset" : 17,
[2024-11-12T09:55:00.029+0000] {spark_submit.py:495} INFO - "queryPlanning" : 18,
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "triggerExecution" : 468,
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "walCommit" : 57
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:55:00.030+0000] {spark_submit.py:495} INFO - "0" : 761
[2024-11-12T09:55:00.031+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.031+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.031+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:55:00.031+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "0" : 762
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "0" : 762
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.75642965204236,
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.1367521367521367,
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:55:00.032+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:55:00.033+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:55:00.034+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.034+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.040+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/173 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.173.70d3d932-d4c5-4eed-bf41-70ed9301e732.tmp
[2024-11-12T09:55:00.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.173.70d3d932-d4c5-4eed-bf41-70ed9301e732.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/173
[2024-11-12T09:55:00.072+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO MicroBatchExecution: Committed offsets for batch 173. Metadata OffsetSeqMetadata(0,1731405300031,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:55:00.087+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:00.089+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:00.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:00.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:00.108+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 171, 172, 172
[2024-11-12T09:55:00.112+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:55:00.143+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:55:00.144+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Got job 173 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:55:00.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Final stage: ResultStage 173 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:55:00.145+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:55:00.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:55:00.146+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[696] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:55:00.165+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-12T09:55:00.183+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_171_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:55:00.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 431.9 MiB)
[2024-11-12T09:55:00.185+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 433.8 MiB)
[2024-11-12T09:55:00.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:55:00.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_171_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:55:00.186+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 173 (MapPartitionsRDD[696] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:55:00.187+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO TaskSchedulerImpl: Adding task set 173.0 with 1 tasks resource profile 0
[2024-11-12T09:55:00.195+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 173) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:55:00.202+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_168_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 433.9 MiB)
[2024-11-12T09:55:00.204+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_168_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:55:00.211+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_170_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:55:00.214+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_170_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:55:00.215+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.0 MiB)
[2024-11-12T09:55:00.218+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_169_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:55:00.232+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_169_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.1 MiB)
[2024-11-12T09:55:00.242+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_172_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:55:00.245+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_172_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:55:00.249+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_167_piece0 on b51985a4b8f1:33097 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:55:00.254+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO BlockManagerInfo: Removed broadcast_167_piece0 on 172.19.0.7:42109 in memory (size: 106.2 KiB, free: 434.3 MiB)
[2024-11-12T09:55:00.827+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 173) in 634 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:55:00.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool
[2024-11-12T09:55:00.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: ResultStage 173 (start at NativeMethodAccessorImpl.java:0) finished in 0.682 s
[2024-11-12T09:55:00.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Job 173 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:55:00.828+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished
[2024-11-12T09:55:00.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO DAGScheduler: Job 173 finished: start at NativeMethodAccessorImpl.java:0, took 0.687097 s
[2024-11-12T09:55:00.829+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO FileFormatWriter: Start to commit write Job d0b65e43-3075-4b77-ade2-b4a576bd9a70.
[2024-11-12T09:55:00.841+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/173 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.173.6e5d3721-f6e1-4bee-a015-efb9d235779c.tmp
[2024-11-12T09:55:00.876+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.173.6e5d3721-f6e1-4bee-a015-efb9d235779c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/173
[2024-11-12T09:55:00.878+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO FileStreamSinkLog: Current compact batch id = 173 min compaction batch id to delete = 69
[2024-11-12T09:55:00.882+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO ManifestFileCommitProtocol: Committed batch 173
[2024-11-12T09:55:00.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO FileFormatWriter: Write Job d0b65e43-3075-4b77-ade2-b4a576bd9a70 committed. Elapsed time: 52 ms.
[2024-11-12T09:55:00.883+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO FileFormatWriter: Finished processing stats for write job d0b65e43-3075-4b77-ade2-b4a576bd9a70.
[2024-11-12T09:55:00.892+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/173 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.173.0204ef70-f163-49ab-a1e7-a15a03cb252a.tmp
[2024-11-12T09:55:00.926+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.173.0204ef70-f163-49ab-a1e7-a15a03cb252a.tmp to hdfs://namenode:9000/spark_checkpoint/commits/173
[2024-11-12T09:55:00.931+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-12T09:55:00.932+0000] {spark_submit.py:495} INFO - "id" : "3481c982-a298-4e8b-aacf-54ed3a142e79",
[2024-11-12T09:55:00.933+0000] {spark_submit.py:495} INFO - "runId" : "96b096b9-58d5-4caf-8bcf-39e570060840",
[2024-11-12T09:55:00.933+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-12T09:55:00.933+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-12T09:55:00.025Z",
[2024-11-12T09:55:00.933+0000] {spark_submit.py:495} INFO - "batchId" : 173,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.1321961620469083,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1098779134295227,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "addBatch" : 790,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "commitOffsets" : 44,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "latestOffset" : 6,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "queryPlanning" : 19,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "triggerExecution" : 901,
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "walCommit" : 40
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-12T09:55:00.934+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - "0" : 762
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:55:00.935+0000] {spark_submit.py:495} INFO - "0" : 763
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "0" : 763
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - },
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.1321961620469083,
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1098779134295227,
[2024-11-12T09:55:00.936+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-12T09:55:00.937+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-12T09:55:00.937+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-12T09:55:00.937+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-12T09:55:00.937+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.937+0000] {spark_submit.py:495} INFO - } ],
[2024-11-12T09:55:00.937+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-12T09:55:00.938+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-12T09:55:00.938+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-12T09:55:00.938+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.938+0000] {spark_submit.py:495} INFO - }
[2024-11-12T09:55:00.955+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/174 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.174.f791c73c-1526-4c86-872c-c436fe6fe0a6.tmp
[2024-11-12T09:55:00.993+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.174.f791c73c-1526-4c86-872c-c436fe6fe0a6.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/174
[2024-11-12T09:55:00.994+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:00 INFO MicroBatchExecution: Committed offsets for batch 174. Metadata OffsetSeqMetadata(0,1731405300937,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-12T09:55:01.009+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:01.014+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:01.028+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:01.029+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-12T09:55:01.036+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO FileStreamSinkLog: BatchIds found from listing: 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 172, 173, 173
[2024-11-12T09:55:01.037+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-12T09:55:01.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-12T09:55:01.070+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Got job 174 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-12T09:55:01.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Final stage: ResultStage 174 (start at NativeMethodAccessorImpl.java:0)
[2024-11-12T09:55:01.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Parents of final stage: List()
[2024-11-12T09:55:01.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Missing parents: List()
[2024-11-12T09:55:01.071+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Submitting ResultStage 174 (MapPartitionsRDD[700] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-12T09:55:01.094+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-12T09:55:01.097+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 106.2 KiB, free 433.6 MiB)
[2024-11-12T09:55:01.103+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on b51985a4b8f1:33097 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:55:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO SparkContext: Created broadcast 174 from broadcast at DAGScheduler.scala:1535
[2024-11-12T09:55:01.105+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 174 (MapPartitionsRDD[700] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-12T09:55:01.106+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO TaskSchedulerImpl: Adding task set 174.0 with 1 tasks resource profile 0
[2024-11-12T09:55:01.106+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 174) (172.19.0.7, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-12T09:55:01.134+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on 172.19.0.7:42109 (size: 106.2 KiB, free: 434.2 MiB)
[2024-11-12T09:55:01.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 174) in 637 ms on 172.19.0.7 (executor 0) (1/1)
[2024-11-12T09:55:01.744+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool
[2024-11-12T09:55:01.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: ResultStage 174 (start at NativeMethodAccessorImpl.java:0) finished in 0.673 s
[2024-11-12T09:55:01.749+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Job 174 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-12T09:55:01.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 174: Stage finished
[2024-11-12T09:55:01.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO DAGScheduler: Job 174 finished: start at NativeMethodAccessorImpl.java:0, took 0.675112 s
[2024-11-12T09:55:01.750+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO FileFormatWriter: Start to commit write Job 170f8ac2-43db-4b96-aab7-6ddc5c5a2558.
[2024-11-12T09:55:01.753+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/174 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.174.c3f1f340-fca7-4dd5-974f-0a273babf3c1.tmp
[2024-11-12T09:55:01.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.174.c3f1f340-fca7-4dd5-974f-0a273babf3c1.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/174
[2024-11-12T09:55:01.812+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO FileStreamSinkLog: Current compact batch id = 174 min compaction batch id to delete = 69
[2024-11-12T09:55:01.822+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO ManifestFileCommitProtocol: Committed batch 174
[2024-11-12T09:55:01.823+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO FileFormatWriter: Write Job 170f8ac2-43db-4b96-aab7-6ddc5c5a2558 committed. Elapsed time: 76 ms.
[2024-11-12T09:55:01.824+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO FileFormatWriter: Finished processing stats for write job 170f8ac2-43db-4b96-aab7-6ddc5c5a2558.
[2024-11-12T09:55:01.837+0000] {spark_submit.py:495} INFO - 24/11/12 09:55:01 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/174 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.174.cc97a502-c385-4ec4-adc8-f4fbc5c31354.tmp
[2024-11-12T09:55:01.839+0000] {local_task_job_runner.py:313} WARNING - State of this instance has been externally set to success. Terminating instance.
[2024-11-12T09:55:01.840+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-11-12T09:55:01.848+0000] {process_utils.py:132} INFO - Sending 15 to group 684. PIDs of all processes in the group: [685, 749, 684]
[2024-11-12T09:55:01.849+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 684
[2024-11-12T09:55:01.853+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-11-12T09:55:01.857+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2024-11-12T09:55:01.860+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-11-12T09:55:01.952+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=684, status='terminated', exitcode=0, started='09:41:55') (684) terminated with exit code 0
[2024-11-12T09:55:01.985+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=749, status='terminated', started='09:42:03') (749) terminated with exit code None
[2024-11-12T09:55:01.987+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=685, status='terminated', started='09:41:55') (685) terminated with exit code None
