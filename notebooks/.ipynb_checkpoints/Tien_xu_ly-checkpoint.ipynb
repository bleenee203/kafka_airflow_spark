{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd236c83-409d-4808-9c00-d217457bb0f0",
   "metadata": {
    "id": "cd236c83-409d-4808-9c00-d217457bb0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python'\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, sum\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify which python is being used\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35cff397-32e2-4775-ad58-375b3aaab6dd",
   "metadata": {
    "id": "35cff397-32e2-4775-ad58-375b3aaab6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+--------------------+---------+--------+---------+----------+--------------------+\n",
      "|         crash_date|crash_time|      on_street_name|     off_street_name|   cross_street_name|number_of_persons_injured|number_of_persons_killed|number_of_pedestrians_injured|number_of_pedestrians_killed|number_of_cyclist_injured|number_of_cyclist_killed|number_of_motorist_injured|number_of_motorist_killed|contributing_factor_vehicle_1|contributing_factor_vehicle_2|contributing_factor_vehicle_3|contributing_factor_vehicle_4|contributing_factor_vehicle_5|collision_id|  vehicle_type_code1|  vehicle_type_code2|  borough|zip_code| latitude| longitude|            location|\n",
      "+-------------------+----------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+--------------------+---------+--------+---------+----------+--------------------+\n",
      "|2021-09-11 00:00:00|      2:39|WHITESTONE EXPRES...|           20 AVENUE|                null|                        2|                       0|                            0|                           0|                        0|                       0|                         2|                        0|         Aggressive Drivin...|                  Unspecified|                         null|                         null|                         null|     4455765|               Sedan|               Sedan|     null|    null|     null|      null|                null|\n",
      "|2022-03-26 00:00:00|     11:45|QUEENSBORO BRIDGE...|                null|                null|                        1|                       0|                            0|                           0|                        0|                       0|                         1|                        0|            Pavement Slippery|                         null|                         null|                         null|                         null|     4513547|               Sedan|                null|     null|    null|     null|      null|                null|\n",
      "|2022-06-29 00:00:00|      6:55|  THROGS NECK BRIDGE|                null|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Following Too Clo...|                  Unspecified|                         null|                         null|                         null|     4541903|               Sedan|       Pick-up Truck|     null|    null|     null|      null|                null|\n",
      "|2021-09-11 00:00:00|      9:35|                null|                null|1211      LORING ...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                         null|                         null|                         null|                         null|     4456314|               Sedan|                null| BROOKLYN|   11208|40.667202|  -73.8665|{40.667202, -73.8...|\n",
      "|2021-12-14 00:00:00|      8:13|     SARATOGA AVENUE|      DECATUR STREET|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                         null|                         null|                         null|                         null|                         null|     4486609|                null|                null| BROOKLYN|   11233|40.683304|-73.917274|{40.683304, -73.9...|\n",
      "|2021-04-14 00:00:00|     12:47|MAJOR DEEGAN EXPR...|                null|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                         null|                         null|                         null|     4407458|                Dump|               Sedan|     null|    null|     null|      null|                null|\n",
      "|2021-12-14 00:00:00|     17:05|BROOKLYN QUEENS E...|                null|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         null|                         null|                         null|     4486555|               Sedan|Tractor Truck Diesel|     null|    null|40.709183|-73.956825|{40.709183, -73.9...|\n",
      "|2021-12-14 00:00:00|      8:17|                null|                null|344       BAYCHES...|                        2|                       0|                            0|                           0|                        0|                       0|                         2|                        0|                  Unspecified|                  Unspecified|                         null|                         null|                         null|     4486660|               Sedan|               Sedan|    BRONX|   10475| 40.86816| -73.83148|{40.86816, -73.83...|\n",
      "|2021-12-14 00:00:00|     21:10|                null|                null|2047      PITKIN ...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Driver Inexperience|                  Unspecified|                         null|                         null|                         null|     4487074|               Sedan|                null| BROOKLYN|   11207| 40.67172|  -73.8971|{40.67172, -73.89...|\n",
      "|2021-12-14 00:00:00|     14:58|            3 AVENUE|      EAST 43 STREET|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         null|                         null|                         null|     4486519|               Sedan|Station Wagon/Spo...|MANHATTAN|   10017| 40.75144| -73.97397|{40.75144, -73.97...|\n",
      "|2021-12-13 00:00:00|      0:34|       MYRTLE AVENUE|                null|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Passing or Lane U...|                  Unspecified|                         null|                         null|                         null|     4486934|Station Wagon/Spo...|                null|     null|    null|40.701275| -73.88887|{40.701275, -73.8...|\n",
      "|2021-12-14 00:00:00|     16:50|SPRINGFIELD BOULE...|     EAST GATE PLAZA|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|           Turning Improperly|                  Unspecified|                         null|                         null|                         null|     4487127|               Sedan|Station Wagon/Spo...|   QUEENS|   11413|40.675884| -73.75577|{40.675884, -73.7...|\n",
      "|2021-12-14 00:00:00|      8:30|            broadway|west 80 street -w...|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Unsafe Lane Changing|                  Unspecified|                         null|                         null|                         null|     4486634|Station Wagon/Spo...|               Sedan|     null|    null|     null|      null|                null|\n",
      "|2021-12-14 00:00:00|      0:59|        BELT PARKWAY|                null|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                 Unsafe Speed|                         null|                         null|                         null|                         null|     4486564|               Sedan|                null|     null|    null| 40.59662| -74.00231|{40.59662, -74.00...|\n",
      "|2021-12-14 00:00:00|     23:10|NORTH CONDUIT AVENUE|          150 STREET|                null|                        2|                       0|                            0|                           0|                        0|                       0|                         2|                        0|         Reaction to Uninv...|                  Unspecified|                         null|                         null|                         null|     4486635|               Sedan|               Sedan|   QUEENS|   11434| 40.66684| -73.78941|{40.66684, -73.78...|\n",
      "|2021-12-14 00:00:00|     17:58|                null|                null|480       DEAN ST...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         null|                         null|                         null|     4486604|              Tanker|Station Wagon/Spo...| BROOKLYN|   11217| 40.68158| -73.97463|{40.68158, -73.97...|\n",
      "|2021-12-14 00:00:00|     20:03|                null|                null|878       FLATBUS...|                        4|                       0|                            0|                           0|                        0|                       0|                         4|                        0|             Steering Failure|                         null|                         null|                         null|                         null|     4486991|               Sedan|                null| BROOKLYN|   11226| 40.65068| -73.95881|{40.65068, -73.95...|\n",
      "|2021-12-14 00:00:00|      1:28|       MEEKER AVENUE|      LORIMER STREET|                null|                        3|                       0|                            0|                           0|                        0|                       0|                         3|                        0|         Traffic Control D...|                  Unspecified|                         null|                         null|                         null|     4486284|Station Wagon/Spo...|Station Wagon/Spo...|     null|    null|     null|      null|                null|\n",
      "|2021-12-11 00:00:00|     19:43|WEST KINGSBRIDGE ...|        HEATH AVENUE|                null|                        1|                       0|                            0|                           0|                        0|                       0|                         1|                        0|                  Unspecified|                  Unspecified|                         null|                         null|                         null|     4487040|Station Wagon/Spo...|               Sedan|    BRONX|   10463| 40.87262|-73.904686|{40.87262, -73.90...|\n",
      "|2021-12-14 00:00:00|     14:30|WHITESTONE EXPRES...|                null|                null|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Following Too Clo...|                  Unspecified|                  Unspecified|                         null|                         null|     4486537|Station Wagon/Spo...|               Sedan|     null|    null|40.783268| -73.82485|{40.783268, -73.8...|\n",
      "+-------------------+----------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+--------------------+---------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType,TimestampType,DoubleType\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"crash_date\", TimestampType(), True),\n",
    "    StructField(\"crash_time\", StringType(), True), \n",
    "    StructField(\"on_street_name\", StringType(), True), \n",
    "    StructField(\"off_street_name\", StringType(), True), \n",
    "    StructField(\"cross_street_name\", StringType(), True), \n",
    "    StructField(\"number_of_persons_injured\", StringType(), True), \n",
    "    StructField(\"number_of_persons_killed\", StringType(), True), \n",
    "    StructField(\"number_of_pedestrians_injured\", StringType(), True), \n",
    "    StructField(\"number_of_pedestrians_killed\", StringType(), True), \n",
    "    StructField(\"number_of_cyclist_injured\", StringType(), True), \n",
    "    StructField(\"number_of_cyclist_killed\", StringType(), True), \n",
    "    StructField(\"number_of_motorist_injured\", StringType(), True), \n",
    "    StructField(\"number_of_motorist_killed\", StringType(), True), \n",
    "    StructField(\"contributing_factor_vehicle_1\", StringType(), True), \n",
    "    StructField(\"contributing_factor_vehicle_2\", StringType(), True),\n",
    "    StructField(\"contributing_factor_vehicle_3\", StringType(), True), \n",
    "    StructField(\"contributing_factor_vehicle_4\", StringType(), True), \n",
    "    StructField(\"contributing_factor_vehicle_5\", StringType(), True), \n",
    "    StructField(\"collision_id\", StringType(), True), \n",
    "    StructField(\"vehicle_type_code1\", StringType(), True), \n",
    "    StructField(\"vehicle_type_code2\", StringType(), True), \n",
    "    StructField(\"borough\", StringType(), True), \n",
    "    StructField(\"zip_code\", StringType(), True), \n",
    "    StructField(\"latitude\", StringType(), True), \n",
    "    StructField(\"longitude\", StringType(), True), \n",
    "    StructField(\"location\", StructType([ \n",
    "        StructField(\"latitude\", StringType(), True),\n",
    "        StructField(\"longitude\", StringType(), True), \n",
    "        StructField(\"human_address\", StringType(), True) ]), True) ])\n",
    "\n",
    "# Load the JSON file with the schema\n",
    "df = spark.read.schema(schema).option(\"multiline\", \"true\").json(\"hdfs://namenode:9000/raw_data/h9gi-nx95.json\")\n",
    "\n",
    "# Show the first few rows\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4417733-6bb5-43c1-8f62-4474b4fd4576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_PYTHON:  /home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"PYSPARK_PYTHON: \", os.environ.get('PYSPARK_PYTHON'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8641285a-9339-45bb-992b-8669bbcb081a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of persons injured: 454\n",
      "+-----------------+-----------------------------+-----------------------------+-----------------------------+------------+----------+-------------------------------------+-------------------------------------+--------------------------+--------------------------+---------------+\n",
      "|cross_street_name|contributing_factor_vehicle_3|contributing_factor_vehicle_4|contributing_factor_vehicle_5|collision_id|is_injured|contributing_factor_vehicle_1_encoded|contributing_factor_vehicle_2_encoded|vehicle_type_code1_encoded|vehicle_type_code2_encoded|borough_encoded|\n",
      "+-----------------+-----------------------------+-----------------------------+-----------------------------+------------+----------+-------------------------------------+-------------------------------------+--------------------------+--------------------------+---------------+\n",
      "|              742|                          909|                          973|                          994|           0|         0|                                    0|                                    0|                         0|                         0|              0|\n",
      "+-----------------+-----------------------------+-----------------------------+-----------------------------+------------+----------+-------------------------------------+-------------------------------------+--------------------------+--------------------------+---------------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|            features|is_injured|\n",
      "+--------------------+----------+\n",
      "|(5,[0,3],[16.0,1.0])|         1|\n",
      "|      (5,[0],[17.0])|         1|\n",
      "| (5,[0,3],[2.0,5.0])|         0|\n",
      "|       (5,[4],[1.0])|         0|\n",
      "|       (5,[4],[1.0])|         0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, Imputer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Convert relevant columns to appropriate data types\n",
    "df = df.withColumn(\"number_of_persons_injured\", df[\"number_of_persons_injured\"].cast(\"int\"))\n",
    "\n",
    "# Calculate total number of injured people\n",
    "total_injured = df.agg({\"number_of_persons_injured\": \"sum\"}).collect()[0][0]\n",
    "print(\"Total number of persons injured:\", total_injured)\n",
    "\n",
    "# Step 2: Add the `is_injured` column (1 if injury or fatality, otherwise 0)\n",
    "df = df.withColumn('is_injured',\n",
    "                   F.when((F.col('number_of_persons_injured') + F.col('number_of_persons_killed')) > 0, 1)\n",
    "                    .otherwise(0))\n",
    "\n",
    "# Step 3: Drop irrelevant columns\n",
    "cols_to_drop = [\n",
    "    'latitude', 'longitude', 'on_street_name', 'off_street_name', 'number_of_persons_injured',\n",
    "    'number_of_pedestrians_killed', 'number_of_cyclist_injured', 'number_of_motorist_injured',\n",
    "    'number_of_motorist_killed', 'vehicle_type_code3', 'vehicle_type_code4', 'vehicle_type_code5',\n",
    "    'crash_date', 'crash_time', 'number_of_persons_killed', 'number_of_pedestrians_injured',\n",
    "    'number_of_cyclist_killed', 'location', 'zip_code'\n",
    "]\n",
    "df_dropped_cols = df.drop(*cols_to_drop)\n",
    "\n",
    "# Step 4: Handle missing values for categorical columns using mode\n",
    "categorical_cols = ['contributing_factor_vehicle_1', 'contributing_factor_vehicle_2',\n",
    "                    'vehicle_type_code1', 'vehicle_type_code2', 'borough']\n",
    "\n",
    "# Fill missing values with the mode of each categorical column\n",
    "for col_name in categorical_cols:\n",
    "    mode_row = df_dropped_cols.groupBy(col_name).count().orderBy('count', ascending=False).first()\n",
    "    \n",
    "    # Handle missing mode value (if the mode is None, use 0)\n",
    "    if mode_row is not None and mode_row[0] is not None:\n",
    "        mode_value = mode_row[0]\n",
    "    else:\n",
    "        mode_value = 0\n",
    "\n",
    "    df_dropped_cols = df_dropped_cols.fillna({col_name: mode_value})\n",
    "\n",
    "# Step 5: Encode categorical columns using StringIndexer\n",
    "def encode_categorical_cols(df, categorical_cols):\n",
    "    for col_name in categorical_cols:\n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_encoded\", handleInvalid='skip')\n",
    "        df = indexer.fit(df).transform(df)\n",
    "    return df\n",
    "\n",
    "df_encoded = encode_categorical_cols(df_dropped_cols, categorical_cols)\n",
    "\n",
    "# Step 6: Drop the original categorical columns\n",
    "df_final = df_encoded.drop(*categorical_cols)\n",
    "\n",
    "# Step 7: Check for any remaining missing values\n",
    "missing_counts = df_final.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df_final.columns])\n",
    "missing_counts.show()\n",
    "\n",
    "# Step 8: Handle missing values for numerical columns (e.g., zip_code) using Imputer\n",
    "numerical_cols = ['contributing_factor_vehicle_1_encoded', 'contributing_factor_vehicle_2_encoded', \n",
    "                  'vehicle_type_code1_encoded', 'vehicle_type_code2_encoded', 'borough_encoded']\n",
    "\n",
    "imputer = Imputer(inputCols=numerical_cols, outputCols=[col + \"_imputed\" for col in numerical_cols])\n",
    "df_imputed = imputer.fit(df_final).transform(df_final)\n",
    "\n",
    "# Step 9: Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[col + \"_imputed\" for col in numerical_cols], outputCol=\"features\")\n",
    "df_final_assembled = assembler.transform(df_imputed)\n",
    "\n",
    "# Show the final dataset with 'features' and 'is_injured' column\n",
    "df_final_assembled.select(\"features\", \"is_injured\").show(5)\n",
    "\n",
    "# Now you can safely access the 'features' column as shown below in RDD\n",
    "test_rdd = df_final_assembled.select(\"features\", \"is_injured\").rdd\n",
    "test_rdd = test_rdd.map(lambda row: (row[\"features\"], row[\"is_injured\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90944ecb-6b83-4ed8-93c4-02ef37f092f4",
   "metadata": {
    "id": "90944ecb-6b83-4ed8-93c4-02ef37f092f4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import math\n",
    "\n",
    "# Feature and label columns\n",
    "x = df_final_assembled.drop(\"is_injured\")\n",
    "y = df_final_assembled.select(\"is_injured\")\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = df_final_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Shard the training data into three parts\n",
    "train_shards = train.randomSplit([1/3, 1/3, 1/3], seed=42)\n",
    "\n",
    "# Convert test DataFrame to RDD with features and labels\n",
    "test_rdd = test.select(\"features\", \"is_injured\").rdd\n",
    "test_rdd = test_rdd.map(lambda row: (row[0], row[1]))  # Row[0] -> 'features', Row[1] -> 'is_injured'\n",
    "\n",
    "# Convert train shards to RDDs with index\n",
    "train_shards_rdd = [\n",
    "    shard.select(\"features\", \"is_injured\").rdd.zipWithIndex().map(lambda row: (row[1], row[0])) \n",
    "    for shard in train_shards\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14c9248-e719-4b52-b119-bc7d12923b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19506e87-dce4-487e-85c0-bd81ca77af02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f832468-57e7-4097-9638-76ef2b33411a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o582.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 4 times, most recent failure: Lost task 0.3 in stage 44.0 (TID 34) (172.19.0.8 executor 0): java.io.IOException: Cannot run program \"/home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:222)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Cannot run program \"/home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:222)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Add a new column \"distance\" using the UDF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df_with_distance \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mcrossJoin(df_train) \\\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, distance_udf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data.features\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data.features\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m \u001b[43mdf_with_distance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o582.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 4 times, most recent failure: Lost task 0.3 in stage 44.0 (TID 34) (172.19.0.8 executor 0): java.io.IOException: Cannot run program \"/home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:222)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Cannot run program \"/home/thanhtk/kafka_airflow_spark/airflow/myenv/bin/python\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:222)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "import numpy  \n",
    "\n",
    "# Step 1: Define the Euclidean distance UDF\n",
    "def calculate_distance_udf(test_features, train_features):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two feature vectors.\n",
    "    \"\"\"\n",
    "    test_features = test_features.toArray()  # Convert sparse vector to dense array\n",
    "    train_features = train_features.toArray()  # Convert sparse vector to dense array\n",
    "    \n",
    "    squared_differences = [(t - r) ** 2 for t, r in zip(test_features, train_features)]\n",
    "    distance = math.sqrt(sum(squared_differences))\n",
    "    return distance\n",
    "\n",
    "# Register the UDF with PySpark\n",
    "distance_udf = udf(calculate_distance_udf, DoubleType())\n",
    "\n",
    "# Step 2: Apply the UDF to calculate distances between test and train rows\n",
    "# Assuming 'test_rdd' and 'train_rdd' are already loaded into DataFrames with 'features' columns\n",
    "\n",
    "# Join test data with train data to calculate distances\n",
    "df_test = test.select(\"features\", \"is_injured\").alias(\"test_data\")\n",
    "df_train = train.select(\"features\", \"is_injured\").alias(\"train_data\")\n",
    "\n",
    "# Add a new column \"distance\" using the UDF\n",
    "df_with_distance = df_test.crossJoin(df_train) \\\n",
    "    .withColumn(\"distance\", distance_udf(\"test_data.features\", \"train_data.features\"))\n",
    "\n",
    "df_with_distance.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008de9a-e19a-4ff2-aa26-2e4c97f92b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "# Euclidean distance function\n",
    "def calculate_distance(test_row, train_row):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between a test row and a train row.\n",
    "    \"\"\"\n",
    "    test_features = test_row.features.toArray()  # Convert sparse vector to dense array\n",
    "    train_features = train_row.features.toArray()  # Convert sparse vector to dense array\n",
    "\n",
    "    squared_differences = [(t - r) ** 2 for t, r in zip(test_features, train_features)]\n",
    "    distance = math.sqrt(sum(squared_differences))\n",
    "    return distance\n",
    "\n",
    "# Mapper function for each test partition\n",
    "def mapper(test_data_partition, train_rdd_broadcast, k):\n",
    "    \"\"\"\n",
    "    Map function to find k nearest neighbors for each test row from the broadcasted train data.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Get the broadcasted train data\n",
    "    train_data = train_rdd_broadcast.value\n",
    "    for test_row in test_data_partition:\n",
    "        k_neighbors = []\n",
    "        for train_row in train_data:\n",
    "            distance = calculate_distance(test_row, train_row)\n",
    "            if len(k_neighbors) < k:\n",
    "                heapq.heappush(k_neighbors, (-distance, train_row.is_injured))\n",
    "            else:\n",
    "                heapq.heappushpop(k_neighbors, (-distance, train_row.is_injured))\n",
    "        results.append((test_row.is_injured, k_neighbors))\n",
    "    return results\n",
    "\n",
    "# Reducer function to merge results\n",
    "def reducer(mapped_results_rdd, k):\n",
    "    \"\"\"\n",
    "    Merge k-nearest neighbors from all test partitions.\n",
    "    \"\"\"\n",
    "    def merge_neighbors(a, b):\n",
    "        combined = sorted(a + b, key=lambda x: x[0], reverse=True)\n",
    "        return combined[:k]\n",
    "\n",
    "    reduced_results = (\n",
    "        mapped_results_rdd\n",
    "        .flatMap(lambda x: [(x[0], x[1])])  # Expand into (test_id, neighbors)\n",
    "        .reduceByKey(merge_neighbors)  # Merge neighbors for each test_id\n",
    "    )\n",
    "\n",
    "    return reduced_results\n",
    "\n",
    "# Majority voting for final prediction\n",
    "def majority_vote(neighbors):\n",
    "    \"\"\"\n",
    "    Perform majority voting on neighbors to determine the predicted class.\n",
    "    \"\"\"\n",
    "    class_votes = {}\n",
    "    for _, label in neighbors:\n",
    "        class_votes[label] = class_votes.get(label, 0) + 1\n",
    "    return max(class_votes, key=class_votes.get)  # Class with the most votes\n",
    "\n",
    "# KNN Prediction without broadcasting\n",
    "def knn_predict(test_rdd, train_shards_rdd, k=5):\n",
    "    \"\"\"\n",
    "    Predict using KNN with broadcasting to avoid collecting training data.\n",
    "    \"\"\"\n",
    "    mapped_results_rdd = None\n",
    "\n",
    "    # Broadcast the training data for each partition\n",
    "    for train_rdd in train_shards_rdd:\n",
    "        train_data = train_rdd.collect()  # Collect the training data for each shard\n",
    "        train_rdd_broadcast = test_rdd.context.broadcast(train_data)  # Broadcast it to all workers\n",
    "\n",
    "        # Process each shard against test data\n",
    "        shard_results = test_rdd.mapPartitions(\n",
    "            lambda test_partition: mapper(test_partition, train_rdd_broadcast, k)\n",
    "        )\n",
    "        if mapped_results_rdd is None:\n",
    "            mapped_results_rdd = shard_results\n",
    "        else:\n",
    "            mapped_results_rdd = mapped_results_rdd.union(shard_results)\n",
    "\n",
    "    # Reduce phase: Merge k-nearest neighbors from all shards\n",
    "    reduced_results_rdd = reducer(mapped_results_rdd, k)\n",
    "\n",
    "    # Perform majority voting\n",
    "    predictions_rdd = reduced_results_rdd.map(lambda x: (x[0], majority_vote(x[1])))\n",
    "\n",
    "    return predictions_rdd\n",
    "\n",
    "# Example usage: Predicting with KNN\n",
    "k = 5\n",
    "# Assuming train_rdd and test_rdd are already defined\n",
    "predictions_rdd = knn_predict(test_rdd, train_shards_rdd, k)\n",
    "\n",
    "# Show results (you can now take the first 5 predictions)\n",
    "print(predictions_rdd.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdf6fd-4ffc-4e4b-9302-82162da4dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assuming predictions_rdd is the RDD containing the predicted results (test_id, predicted_label)\n",
    "# and the true labels from the test dataset (test_id, actual_label).\n",
    "\n",
    "# Convert predictions to a DataFrame for evaluation (this is easier with Spark ML evaluation methods)\n",
    "predictions_df = predictions_rdd.toDF([\"test_id\", \"predicted_label\"])\n",
    "\n",
    "# Join with the actual labels\n",
    "actual_labels_df = test.select(\"is_injured\").withColumnRenamed(\"is_injured\", \"actual_label\")\n",
    "predictions_with_labels_df = predictions_df.join(actual_labels_df, predictions_df.test_id == actual_labels_df.test_id, \"inner\")\n",
    "\n",
    "# Now calculate confusion matrix components (TP, TN, FP, FN)\n",
    "tp = predictions_with_labels_df.filter(\"predicted_label = 1 AND actual_label = 1\").count()\n",
    "tn = predictions_with_labels_df.filter(\"predicted_label = 0 AND actual_label = 0\").count()\n",
    "fp = predictions_with_labels_df.filter(\"predicted_label = 1 AND actual_label = 0\").count()\n",
    "fn = predictions_with_labels_df.filter(\"predicted_label = 0 AND actual_label = 1\").count()\n",
    "\n",
    "# Calculate Accuracy, Recall, Precision, F1 Score\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix Output\n",
    "print(f\"Confusion Matrix:\\nTP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "# You can also use Spark's MulticlassClassificationEvaluator for automated metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"actual_label\", predictionCol=\"predicted_label\")\n",
    "accuracy_spark = evaluator.evaluate(predictions_with_labels_df)\n",
    "print(f\"Accuracy (via Spark Evaluator): {accuracy_spark:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7bb1d-4289-47b5-a2c6-5452f5be92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Ensure predictions_rdd is in the correct format: [(test_id, predicted_label)]\n",
    "# Example: predictions_rdd = [(1, 0), (2, 1), (3, 0), ...]\n",
    "\n",
    "# Define a custom schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"test_id\", IntegerType(), True),\n",
    "    StructField(\"prediction\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Convert predictions RDD to DataFrame with explicit schema\n",
    "predictions_df = spark.createDataFrame(predictions_rdd, schema)\n",
    "\n",
    "# Add true labels from the test dataset\n",
    "# Assuming test DataFrame has the true labels with schema: [test_id, true_label]\n",
    "test_labels_df = test.select(col(\"collision_id\").alias(\"test_id\"), col(\"is_injured\").alias(\"true_label\"))\n",
    "\n",
    "# Join predictions with true labels\n",
    "predictions_with_labels = predictions_df.join(test_labels_df, on=\"test_id\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "total_count = predictions_with_labels.count()\n",
    "correct_count = predictions_with_labels.filter(col(\"prediction\") == col(\"true_label\")).count()\n",
    "accuracy = correct_count / total_count\n",
    "\n",
    "# Create Confusion Matrix\n",
    "confusion_matrix = (\n",
    "    predictions_with_labels.groupBy(\"true_label\", \"prediction\")\n",
    "    .count()\n",
    "    .orderBy(\"true_label\", \"prediction\")\n",
    ")\n",
    "\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Show Results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time of execution: {execution_time:.2f}s\")\n",
    "\n",
    "# Show Confusion Matrix\n",
    "confusion_matrix.show()\n",
    "\n",
    "# Optional: Visualize the confusion matrix (if using matplotlib and seaborn)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert Spark DataFrame to Pandas for visualization\n",
    "    cm_pd = confusion_matrix.toPandas()\n",
    "\n",
    "    # Create a pivot table for the heatmap\n",
    "    cm_matrix = cm_pd.pivot(index=\"true_label\", columns=\"prediction\", values=\"count\").fillna(0)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Seaborn or Matplotlib not installed, skipping visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90ebed-cf62-46ca-8d63-90c2a0f0a1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26347aca-2ef0-4f12-b7a4-fcc3a9b4bb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
