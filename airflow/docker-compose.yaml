
x-airflow-common:
  &airflow-common
  build:
    context: .

  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow

    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

x-spark-common: &spark-common
  image: bitnami/spark:3.4.1
  user: root
  volumes:
    - ./dags:/opt/bitnami/spark/jobs
    # - ./data:/opt/airflow/data
    # - ./data/output_data:/opt/airflow/data/output_data
    # - ./postgresql-42.2.18.jar:/opt/bitnami/spark/jars/postgresql-42.2.18.jar
  
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    # environment:
    #   - AIRFLOW__WEBSERVER__WORKER_TIMEOUT=300

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins /sources/logs/scheduler
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
      
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    command:
      - bash
      - -c
      - airflow
  
  spark-master:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE = master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_NETWORK_TIMEOUT=6000s 
      - SPARK_EXECUTOR_HEARTBEATINTERVAL=1000s  
      - SPARK_NETWORK_TIMEOUT=6000s
      - SPARK_RPC_MESSAGE_MAX_SIZE=1024
      - HADOOP_CONF_DIR=/opt/hadoop-3.2.1/etc/hadoop
    volumes:
      - hadoop-config:/opt/hadoop-3.2.1/etc/hadoop
    ports:
      - "9090:8080"
      - "7077:7077"
        

  spark-worker:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_NETWORK_TIMEOUT : 6000s  
      SPARK_EXECUTOR_HEARTBEATINTERVAL : 1000s  
      SPARK_RPC_MESSAGE_MAX_SIZE: 1024
      HADOOP_CONF_DIR: /opt/hadoop-3.2.1/etc/hadoop
      
    volumes:
      - hadoop-config:/opt/hadoop-3.2.1/etc/hadoop
  

  namenode:
    image: 'bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8'
    container_name: 'namenode'
    hostname: 'namenode'
    ports:
      - '9870:9870'
      - '9000:9000'
    volumes:
      - 'namenode:/hadoop/dfs/name'
      - hadoop-config:/opt/hadoop-3.2.1/etc/hadoop
    environment:
      CLUSTER_NAME: 'test'
      CORE_CONF_fs_defaultFS: 'hdfs://namenode:9000'
      CORE_CONF_hadoop_http_staticuser_user: 'root'
      CORE_CONF_hadoop_proxyuser_hue_hosts: '*'
      CORE_CONF_hadoop_proxyuser_hue_groups: '*'
      CORE_CONF_io_compression_codecs: 'org.apache.hadoop.io.compress.SnappyCodec'
      HDFS_CONF_dfs_webhdfs_enabled: 'true'
      HDFS_CONF_dfs_permissions_enabled: 'false'
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: 'false'
      YARN_CONF_yarn_log___aggregation___enable: 'true'
      YARN_CONF_yarn_log_server_url: 'http://historyserver:8188/applicationhistory/logs/'
      YARN_CONF_yarn_resourcemanager_recovery_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_store_class: 'org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore'
      YARN_CONF_yarn_resourcemanager_scheduler_class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: '8192'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: '4'
      YARN_CONF_yarn_resourcemanager_fs_state___store_uri: '/rmstate'
      YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_hostname: 'resourcemanager'
      YARN_CONF_yarn_resourcemanager_address: 'resourcemanager:8032'
      YARN_CONF_yarn_resourcemanager_scheduler_address: 'resourcemanager:8030'
      YARN_CONF_yarn_resourcemanager_resource__tracker_address: 'resourcemanager:8031'
      YARN_CONF_yarn_timeline___service_enabled: 'true'
      YARN_CONF_yarn_timeline___service_generic___application___history_enabled: 'true'
      YARN_CONF_yarn_timeline___service_hostname: 'historyserver'
      YARN_CONF_mapreduce_map_output_compress: 'true'
      YARN_CONF_mapred_map_output_compress_codec: 'org.apache.hadoop.io.compress.SnappyCodec'
      YARN_CONF_yarn_nodemanager_resource_memory___mb: '16384'
      YARN_CONF_yarn_nodemanager_resource_cpu___vcores: '8'
      YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage: '98.5'
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: '/app-logs'
      YARN_CONF_yarn_nodemanager_aux___services: 'mapreduce_shuffle'
      MAPRED_CONF_mapreduce_framework_name: 'yarn'
      MAPRED_CONF_mapred_child_java_opts: '-Xmx4096m'
      MAPRED_CONF_mapreduce_map_memory_mb: '4096'
      MAPRED_CONF_mapreduce_reduce_memory_mb: '8192'
      MAPRED_CONF_mapreduce_map_java_opts: '-Xmx3072m'
      MAPRED_CONF_mapreduce_reduce_java_opts: '-Xmx6144m'
      MAPRED_CONF_yarn_app_mapreduce_am_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_map_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_reduce_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'

  
  datanode1:
    image: 'bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8'
    container_name: 'datenode1'
    hostname: 'datanode1'
    depends_on:
      - 'namenode'
    ports:
      - '9864:9864'
    volumes:
      - 'datanode:/hadoop/dfs/data1'
    environment:
      SERVICE_PRECONDITION: 'namenode:9870'
      CORE_CONF_fs_defaultFS: 'hdfs://namenode:9000'
      CORE_CONF_hadoop_http_staticuser_user: 'root'
      CORE_CONF_hadoop_proxyuser_hue_hosts: '*'
      CORE_CONF_hadoop_proxyuser_hue_groups: '*'
      CORE_CONF_io_compression_codecs: 'org.apache.hadoop.io.compress.SnappyCodec'
      HDFS_CONF_dfs_webhdfs_enabled: 'true'
      HDFS_CONF_dfs_permissions_enabled: 'false'
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: 'false'
      YARN_CONF_yarn_log___aggregation___enable: 'true'
      YARN_CONF_yarn_log_server_url: 'http://historyserver:8188/applicationhistory/logs/'
      YARN_CONF_yarn_resourcemanager_recovery_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_store_class: 'org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore'
      YARN_CONF_yarn_resourcemanager_scheduler_class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: '8192'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: '4'
      YARN_CONF_yarn_resourcemanager_fs_state___store_uri: '/rmstate'
      YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_hostname: 'resourcemanager'
      YARN_CONF_yarn_resourcemanager_address: 'resourcemanager:8032'
      YARN_CONF_yarn_resourcemanager_scheduler_address: 'resourcemanager:8030'
      YARN_CONF_yarn_resourcemanager_resource__tracker_address: 'resourcemanager:8031'
      YARN_CONF_yarn_timeline___service_enabled: 'true'
      YARN_CONF_yarn_timeline___service_generic___application___history_enabled: 'true'
      YARN_CONF_yarn_timeline___service_hostname: 'historyserver'
      YARN_CONF_mapreduce_map_output_compress: 'true'
      YARN_CONF_mapred_map_output_compress_codec: 'org.apache.hadoop.io.compress.SnappyCodec'
      YARN_CONF_yarn_nodemanager_resource_memory___mb: '16384'
      YARN_CONF_yarn_nodemanager_resource_cpu___vcores: '8'
      YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage: '98.5'
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: '/app-logs'
      YARN_CONF_yarn_nodemanager_aux___services: 'mapreduce_shuffle'
      MAPRED_CONF_mapreduce_framework_name: 'yarn'
      MAPRED_CONF_mapred_child_java_opts: '-Xmx4096m'
      MAPRED_CONF_mapreduce_map_memory_mb: '4096'
      MAPRED_CONF_mapreduce_reduce_memory_mb: '8192'
      MAPRED_CONF_mapreduce_map_java_opts: '-Xmx3072m'
      MAPRED_CONF_mapreduce_reduce_java_opts: '-Xmx6144m'
      MAPRED_CONF_yarn_app_mapreduce_am_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_map_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_reduce_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
  datanode2:
    image: 'bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8'
    hostname: 'datanode2'
    container_name: 'datenode2'
    ports:
      - '9865:9864'
    volumes:
      - 'datanode:/hadoop/dfs/data2'
    environment:
      SERVICE_PRECONDITION: 'namenode:9870'
      CORE_CONF_fs_defaultFS: 'hdfs://namenode:9000'
      CORE_CONF_hadoop_http_staticuser_user: 'root'
      CORE_CONF_hadoop_proxyuser_hue_hosts: '*'
      CORE_CONF_hadoop_proxyuser_hue_groups: '*'
      CORE_CONF_io_compression_codecs: 'org.apache.hadoop.io.compress.SnappyCodec'
      HDFS_CONF_dfs_webhdfs_enabled: 'true'
      HDFS_CONF_dfs_permissions_enabled: 'false'
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: 'false'
      YARN_CONF_yarn_log___aggregation___enable: 'true'
      YARN_CONF_yarn_log_server_url: 'http://historyserver:8188/applicationhistory/logs/'
      YARN_CONF_yarn_resourcemanager_recovery_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_store_class: 'org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore'
      YARN_CONF_yarn_resourcemanager_scheduler_class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: '8192'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: '4'
      YARN_CONF_yarn_resourcemanager_fs_state___store_uri: '/rmstate'
      YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_hostname: 'resourcemanager'
      YARN_CONF_yarn_resourcemanager_address: 'resourcemanager:8032'
      YARN_CONF_yarn_resourcemanager_scheduler_address: 'resourcemanager:8030'
      YARN_CONF_yarn_resourcemanager_resource__tracker_address: 'resourcemanager:8031'
      YARN_CONF_yarn_timeline___service_enabled: 'true'
      YARN_CONF_yarn_timeline___service_generic___application___history_enabled: 'true'
      YARN_CONF_yarn_timeline___service_hostname: 'historyserver'
      YARN_CONF_mapreduce_map_output_compress: 'true'
      YARN_CONF_mapred_map_output_compress_codec: 'org.apache.hadoop.io.compress.SnappyCodec'
      YARN_CONF_yarn_nodemanager_resource_memory___mb: '16384'
      YARN_CONF_yarn_nodemanager_resource_cpu___vcores: '8'
      YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage: '98.5'
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: '/app-logs'
      YARN_CONF_yarn_nodemanager_aux___services: 'mapreduce_shuffle'
      MAPRED_CONF_mapreduce_framework_name: 'yarn'
      MAPRED_CONF_mapred_child_java_opts: '-Xmx4096m'
      MAPRED_CONF_mapreduce_map_memory_mb: '4096'
      MAPRED_CONF_mapreduce_reduce_memory_mb: '8192'
      MAPRED_CONF_mapreduce_map_java_opts: '-Xmx3072m'
      MAPRED_CONF_mapreduce_reduce_java_opts: '-Xmx6144m'
      MAPRED_CONF_yarn_app_mapreduce_am_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_map_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_reduce_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the Resource Manager is a key component of the YARN (Yet Another ResourceNegotiator)
  # framework. It is responsible for managing the allocation of computing resources in a Hadoop cluster,
  # such as CPU, memory, and disk, to various applications running on the cluster.
  #
  # The Resource Manager communicates with NodeManagers, which run on each machine in the cluster
  # and manage the actual resources on that machine. The Resource Manager receives resource requests
  # from applications running on the cluster and negotiates with the NodeManagers to allocate the
  # necessary resources to each application. It also monitors the resource usage of each application
  # and dynamically adjusts the resource allocation as needed.
  #
  # The Resource Manager also provides a web-based user interface for monitoring the status of
  # applications running on the cluster and their resource usage. It can also be configured to
  # use various scheduling policies, such as fair scheduling or capacity scheduling, to allocate
  # resources to applications.
  # ----------------------------------------------------------------------------------------------------
  resourcemanager:
    image: 'bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8'
    container_name: 'yarn'
    hostname: 'yarn'
    ports:
      - '8088:8088'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
    healthcheck:
      disable: true
    environment:
      SERVICE_PRECONDITION: 'namenode:9870 datanode1:9864 datanode2:9864'
      CORE_CONF_fs_defaultFS: 'hdfs://namenode:9000'
      CORE_CONF_hadoop_http_staticuser_user: 'root'
      CORE_CONF_hadoop_proxyuser_hue_hosts: '*'
      CORE_CONF_hadoop_proxyuser_hue_groups: '*'
      CORE_CONF_io_compression_codecs: 'org.apache.hadoop.io.compress.SnappyCodec'
      HDFS_CONF_dfs_webhdfs_enabled: 'true'
      HDFS_CONF_dfs_permissions_enabled: 'false'
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: 'false'
      YARN_CONF_yarn_log___aggregation___enable: 'true'
      YARN_CONF_yarn_log_server_url: 'http://historyserver:8188/applicationhistory/logs/'
      YARN_CONF_yarn_resourcemanager_recovery_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_store_class: 'org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore'
      YARN_CONF_yarn_resourcemanager_scheduler_class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: '8192'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: '4'
      YARN_CONF_yarn_resourcemanager_fs_state___store_uri: '/rmstate'
      YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_hostname: 'resourcemanager'
      YARN_CONF_yarn_resourcemanager_address: 'resourcemanager:8032'
      YARN_CONF_yarn_resourcemanager_scheduler_address: 'resourcemanager:8030'
      YARN_CONF_yarn_resourcemanager_resource__tracker_address: 'resourcemanager:8031'
      YARN_CONF_yarn_timeline___service_enabled: 'true'
      YARN_CONF_yarn_timeline___service_generic___application___history_enabled: 'true'
      YARN_CONF_yarn_timeline___service_hostname: 'historyserver'
      YARN_CONF_mapreduce_map_output_compress: 'true'
      YARN_CONF_mapred_map_output_compress_codec: 'org.apache.hadoop.io.compress.SnappyCodec'
      YARN_CONF_yarn_nodemanager_resource_memory___mb: '16384'
      YARN_CONF_yarn_nodemanager_resource_cpu___vcores: '8'
      YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage: '98.5'
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: '/app-logs'
      YARN_CONF_yarn_nodemanager_aux___services: 'mapreduce_shuffle'
      MAPRED_CONF_mapreduce_framework_name: 'yarn'
      MAPRED_CONF_mapred_child_java_opts: '-Xmx4096m'
      MAPRED_CONF_mapreduce_map_memory_mb: '4096'
      MAPRED_CONF_mapreduce_reduce_memory_mb: '8192'
      MAPRED_CONF_mapreduce_map_java_opts: '-Xmx3072m'
      MAPRED_CONF_mapreduce_reduce_java_opts: '-Xmx6144m'
      MAPRED_CONF_yarn_app_mapreduce_am_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_map_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_reduce_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, a NodeManager is a component of the YARN (Yet Another Resource Negotiator) framework,
  # and it is responsible for managing the resources, such as CPU, memory, and disk, on an individual
  # node in the Hadoop cluster.
  #
  # Each machine in the cluster runs a NodeManager, and it communicates with the Resource Manager to
  # obtain the resource allocation for that node. It is responsible for managing the containers that
  # run on that node, which are the units of resource allocation for YARN. The NodeManager launches
  # and monitors the containers, and it communicates with the Resource Manager to request additional
  # resources or release unused resources as needed.
  #
  # The NodeManager is also responsible for monitoring the health of the node, such as the disk usage
  # and the number of running processes, and it reports this information to the Resource Manager. If
  # a NodeManager fails or becomes unavailable, the Resource Manager will detect the failure and
  # redistribute the containers running on that node to other available nodes in the cluster.
  # ----------------------------------------------------------------------------------------------------
  nodemanager:
    image: 'bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8'
    hostname: 'nodemanager'
    ports:
      - '8042:8042'
    container_name: 'nodemanager'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
      - 'resourcemanager'
    environment:
      SERVICE_PRECONDITION: 'namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088'
      CORE_CONF_fs_defaultFS: 'hdfs://namenode:9000'
      CORE_CONF_hadoop_http_staticuser_user: 'root'
      CORE_CONF_hadoop_proxyuser_hue_hosts: '*'
      CORE_CONF_hadoop_proxyuser_hue_groups: '*'
      CORE_CONF_io_compression_codecs: 'org.apache.hadoop.io.compress.SnappyCodec'
      HDFS_CONF_dfs_webhdfs_enabled: 'true'
      HDFS_CONF_dfs_permissions_enabled: 'false'
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: 'false'
      YARN_CONF_yarn_log___aggregation___enable: 'true'
      YARN_CONF_yarn_log_server_url: 'http://historyserver:8188/applicationhistory/logs/'
      YARN_CONF_yarn_resourcemanager_recovery_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_store_class: 'org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore'
      YARN_CONF_yarn_resourcemanager_scheduler_class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: '8192'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: '4'
      YARN_CONF_yarn_resourcemanager_fs_state___store_uri: '/rmstate'
      YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_hostname: 'resourcemanager'
      YARN_CONF_yarn_resourcemanager_address: 'resourcemanager:8032'
      YARN_CONF_yarn_resourcemanager_scheduler_address: 'resourcemanager:8030'
      YARN_CONF_yarn_resourcemanager_resource__tracker_address: 'resourcemanager:8031'
      YARN_CONF_yarn_timeline___service_enabled: 'true'
      YARN_CONF_yarn_timeline___service_generic___application___history_enabled: 'true'
      YARN_CONF_yarn_timeline___service_hostname: 'historyserver'
      YARN_CONF_mapreduce_map_output_compress: 'true'
      YARN_CONF_mapred_map_output_compress_codec: 'org.apache.hadoop.io.compress.SnappyCodec'
      YARN_CONF_yarn_nodemanager_resource_memory___mb: '16384'
      YARN_CONF_yarn_nodemanager_resource_cpu___vcores: '8'
      YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage: '98.5'
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: '/app-logs'
      YARN_CONF_yarn_nodemanager_aux___services: 'mapreduce_shuffle'
      MAPRED_CONF_mapreduce_framework_name: 'yarn'
      MAPRED_CONF_mapred_child_java_opts: '-Xmx4096m'
      MAPRED_CONF_mapreduce_map_memory_mb: '4096'
      MAPRED_CONF_mapreduce_reduce_memory_mb: '8192'
      MAPRED_CONF_mapreduce_map_java_opts: '-Xmx3072m'
      MAPRED_CONF_mapreduce_reduce_java_opts: '-Xmx6144m'
      MAPRED_CONF_yarn_app_mapreduce_am_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_map_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_reduce_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the History Server is a component of the Hadoop MapReduce framework that provides a
  # web-based user interface for accessing the logs and job history of completed MapReduce jobs in
  # the Hadoop cluster.
  #
  # When a MapReduce job completes, the output is written to the Hadoop Distributed File System
  # (HDFS), along with detailed logs of the job execution. The History Server provides a user
  # interface for accessing this information and analyzing the performance of completed jobs.
  #
  # The History Server stores the job history information in a database, which can be queried
  # using the web-based user interface. The user interface provides information about the input
  # and output of each job, as well as detailed information about the execution of each task in
  # the job. It also provides charts and graphs for visualizing the performance of the job, such
  # as the time taken for each task and the resource usage of each task.
  # ----------------------------------------------------------------------------------------------------
  historyserver:
    image: 'bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8'
    container_name: 'historyserver'
    hostname: 'historyserver'
    ports:
      - '8188:8188'
    volumes:
      - 'hadoop_historyserver:/hadoop/yarn/timeline'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
      - 'resourcemanager'
    environment:
      SERVICE_PRECONDITION: 'namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088'
      CORE_CONF_fs_defaultFS: 'hdfs://namenode:9000'
      CORE_CONF_hadoop_http_staticuser_user: 'root'
      CORE_CONF_hadoop_proxyuser_hue_hosts: '*'
      CORE_CONF_hadoop_proxyuser_hue_groups: '*'
      CORE_CONF_io_compression_codecs: 'org.apache.hadoop.io.compress.SnappyCodec'
      HDFS_CONF_dfs_webhdfs_enabled: 'true'
      HDFS_CONF_dfs_permissions_enabled: 'false'
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: 'false'
      YARN_CONF_yarn_log___aggregation___enable: 'true'
      YARN_CONF_yarn_log_server_url: 'http://historyserver:8188/applicationhistory/logs/'
      YARN_CONF_yarn_resourcemanager_recovery_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_store_class: 'org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore'
      YARN_CONF_yarn_resourcemanager_scheduler_class: 'org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb: '8192'
      YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores: '4'
      YARN_CONF_yarn_resourcemanager_fs_state___store_uri: '/rmstate'
      YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: 'true'
      YARN_CONF_yarn_resourcemanager_hostname: 'resourcemanager'
      YARN_CONF_yarn_resourcemanager_address: 'resourcemanager:8032'
      YARN_CONF_yarn_resourcemanager_scheduler_address: 'resourcemanager:8030'
      YARN_CONF_yarn_resourcemanager_resource__tracker_address: 'resourcemanager:8031'
      YARN_CONF_yarn_timeline___service_enabled: 'true'
      YARN_CONF_yarn_timeline___service_generic___application___history_enabled: 'true'
      YARN_CONF_yarn_timeline___service_hostname: 'historyserver'
      YARN_CONF_mapreduce_map_output_compress: 'true'
      YARN_CONF_mapred_map_output_compress_codec: 'org.apache.hadoop.io.compress.SnappyCodec'
      YARN_CONF_yarn_nodemanager_resource_memory___mb: '16384'
      YARN_CONF_yarn_nodemanager_resource_cpu___vcores: '8'
      YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage: '98.5'
      YARN_CONF_yarn_nodemanager_remote___app___log___dir: '/app-logs'
      YARN_CONF_yarn_nodemanager_aux___services: 'mapreduce_shuffle'
      MAPRED_CONF_mapreduce_framework_name: 'yarn'
      MAPRED_CONF_mapred_child_java_opts: '-Xmx4096m'
      MAPRED_CONF_mapreduce_map_memory_mb: '4096'
      MAPRED_CONF_mapreduce_reduce_memory_mb: '8192'
      MAPRED_CONF_mapreduce_map_java_opts: '-Xmx3072m'
      MAPRED_CONF_mapreduce_reduce_java_opts: '-Xmx6144m'
      MAPRED_CONF_yarn_app_mapreduce_am_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_map_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'
      MAPRED_CONF_mapreduce_reduce_env: 'HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/'

   #Jupyter notebook
  jupyter-spark:
    image: jupyter/pyspark-notebook:spark-3.4.1
    ports:
      - "8888:8888"
      - "4040-4080:4040-4080"
    volumes:
      - ../notebooks:/home/jovyan/work/notebooks/
    environment:
      - SPARK_OPTS=--conf spark.driver.memory=4g \
                  --conf spark.executor.memory=4g \
                  --conf spark.driver.maxResultSize=2g \
                  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
                  --conf spark.sql.execution.arrow.pyspark.enabled=true


volumes:
  postgres-db-volume:
  datanode:
  namenode:
  hadoop_historyserver:
  hadoop-config:

