[2024-11-02T09:00:04.968+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-11-02T09:00:05.343+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-02T08:22:17.242733+00:00 [queued]>
[2024-11-02T09:00:05.378+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-02T08:22:17.242733+00:00 [queued]>
[2024-11-02T09:00:05.380+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 1
[2024-11-02T09:00:05.454+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): sensor_data_consumer> on 2024-11-02 08:22:17.242733+00:00
[2024-11-02T09:00:05.488+0000] {standard_task_runner.py:64} INFO - Started process 223 to run task
[2024-11-02T09:00:05.502+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sensor_data_consumer', 'sensor_data_consumer', 'manual__2024-11-02T08:22:17.242733+00:00', '--job-id', '696', '--raw', '--subdir', 'DAGS_FOLDER/***_consumer.py', '--cfg-path', '/tmp/tmp5nuqe344']
[2024-11-02T09:00:05.508+0000] {standard_task_runner.py:91} INFO - Job 696: Subtask sensor_data_consumer
[2024-11-02T09:00:05.844+0000] {task_command.py:426} INFO - Running <TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-02T08:22:17.242733+00:00 [running]> on host 7d12808d40db
[2024-11-02T09:00:06.967+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Bich Ly' AIRFLOW_CTX_DAG_ID='sensor_data_consumer' AIRFLOW_CTX_TASK_ID='sensor_data_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-11-02T08:22:17.242733+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-02T08:22:17.242733+00:00'
[2024-11-02T09:00:06.982+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-11-02T09:00:07.421+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2024-11-02T09:00:07.429+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2 --name KafkaSparkHDFS /opt/***/dags/spark_streaming_job.py
[2024-11-02T09:00:57.853+0000] {spark_submit.py:495} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-02T09:01:02.226+0000] {spark_submit.py:495} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-02T09:01:02.325+0000] {spark_submit.py:495} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-02T09:01:02.471+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency
[2024-11-02T09:01:02.498+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-11-02T09:01:02.544+0000] {spark_submit.py:495} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-6447b5f0-fb10-45f6-a400-3eeb98f01e51;1.0
[2024-11-02T09:01:02.576+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-02T09:01:06.498+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 in central
[2024-11-02T09:01:07.471+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 in central
[2024-11-02T09:01:08.020+0000] {spark_submit.py:495} INFO - found org.apache.kafka#kafka-clients;3.3.2 in central
[2024-11-02T09:01:08.428+0000] {spark_submit.py:495} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-11-02T09:01:08.760+0000] {spark_submit.py:495} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-11-02T09:01:09.354+0000] {spark_submit.py:495} INFO - found org.slf4j#slf4j-api;2.0.6 in central
[2024-11-02T09:01:10.010+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-11-02T09:01:10.527+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-11-02T09:01:10.776+0000] {spark_submit.py:495} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-11-02T09:01:10.958+0000] {spark_submit.py:495} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-11-02T09:01:11.633+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 in central
[2024-11-02T09:01:12.021+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-11-02T09:01:12.481+0000] {spark_submit.py:495} INFO - :: resolution report :: resolve 9657ms :: artifacts dl 176ms
[2024-11-02T09:01:12.506+0000] {spark_submit.py:495} INFO - :: modules in use:
[2024-11-02T09:01:12.510+0000] {spark_submit.py:495} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2024-11-02T09:01:12.511+0000] {spark_submit.py:495} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2024-11-02T09:01:12.513+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-11-02T09:01:12.514+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-11-02T09:01:12.515+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-11-02T09:01:12.516+0000] {spark_submit.py:495} INFO - org.apache.kafka#kafka-clients;3.3.2 from central in [default]
[2024-11-02T09:01:12.517+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-02T09:01:12.518+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-02T09:01:12.519+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-02T09:01:12.522+0000] {spark_submit.py:495} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-11-02T09:01:12.523+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;2.0.6 from central in [default]
[2024-11-02T09:01:12.524+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-11-02T09:01:12.525+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-02T09:01:12.526+0000] {spark_submit.py:495} INFO - |                  |            modules            ||   artifacts   |
[2024-11-02T09:01:12.527+0000] {spark_submit.py:495} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-02T09:01:12.527+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-02T09:01:12.528+0000] {spark_submit.py:495} INFO - |      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
[2024-11-02T09:01:12.528+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-02T09:01:12.579+0000] {spark_submit.py:495} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-6447b5f0-fb10-45f6-a400-3eeb98f01e51
[2024-11-02T09:01:12.586+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-02T09:01:13.227+0000] {spark_submit.py:495} INFO - 0 artifacts copied, 12 already retrieved (0kB/646ms)
[2024-11-02T09:01:16.335+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-02T09:01:27.395+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:27 INFO SparkContext: Running Spark version 3.4.2
[2024-11-02T09:01:27.792+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:27 INFO ResourceUtils: ==============================================================
[2024-11-02T09:01:27.795+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-02T09:01:27.799+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:27 INFO ResourceUtils: ==============================================================
[2024-11-02T09:01:27.816+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:27 INFO SparkContext: Submitted application: KafkaSparkStreaming
[2024-11-02T09:01:28.866+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-02T09:01:28.942+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:28 INFO ResourceProfile: Limiting resource is cpu
[2024-11-02T09:01:29.040+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-02T09:01:29.970+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:29 INFO SecurityManager: Changing view acls to: ***
[2024-11-02T09:01:29.979+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:29 INFO SecurityManager: Changing modify acls to: ***
[2024-11-02T09:01:29.988+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:29 INFO SecurityManager: Changing view acls groups to:
[2024-11-02T09:01:30.023+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:29 INFO SecurityManager: Changing modify acls groups to:
[2024-11-02T09:01:30.026+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-02T09:01:34.165+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:34 INFO Utils: Successfully started service 'sparkDriver' on port 43271.
[2024-11-02T09:01:35.338+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:35 INFO SparkEnv: Registering MapOutputTracker
[2024-11-02T09:01:36.007+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:36 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-02T09:01:36.342+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-02T09:01:36.344+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-02T09:01:36.436+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-02T09:01:36.981+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9648823b-6090-4200-878e-e17fea0cf333
[2024-11-02T09:01:37.252+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-02T09:01:37.536+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:37 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-02T09:01:41.083+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:40 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-02T09:01:41.957+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-02T09:01:42.177+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:43271/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.178+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:43271/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.187+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:43271/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.202+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://7d12808d40db:43271/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.229+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://7d12808d40db:43271/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1730538087257
[2024-11-02T09:01:42.235+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://7d12808d40db:43271/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1730538087257
[2024-11-02T09:01:42.238+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://7d12808d40db:43271/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1730538087257
[2024-11-02T09:01:42.244+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://7d12808d40db:43271/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1730538087257
[2024-11-02T09:01:42.246+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://7d12808d40db:43271/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1730538087257
[2024-11-02T09:01:42.262+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://7d12808d40db:43271/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1730538087257
[2024-11-02T09:01:42.264+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://7d12808d40db:43271/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1730538087257
[2024-11-02T09:01:42.269+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://7d12808d40db:43271/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1730538087257
[2024-11-02T09:01:42.274+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:43271/files/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.283+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar
[2024-11-02T09:01:42.451+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:43271/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.467+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar
[2024-11-02T09:01:42.529+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:43271/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.533+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar
[2024-11-02T09:01:42.574+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://7d12808d40db:43271/files/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1730538087257
[2024-11-02T09:01:42.582+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.kafka_kafka-clients-3.3.2.jar
[2024-11-02T09:01:42.750+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://7d12808d40db:43271/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1730538087257
[2024-11-02T09:01:42.753+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:42 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-11-02T09:01:43.530+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://7d12808d40db:43271/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1730538087257
[2024-11-02T09:01:43.535+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.lz4_lz4-java-1.8.0.jar
[2024-11-02T09:01:43.600+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://7d12808d40db:43271/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1730538087257
[2024-11-02T09:01:43.606+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-11-02T09:01:43.679+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://7d12808d40db:43271/files/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1730538087257
[2024-11-02T09:01:43.680+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.slf4j_slf4j-api-2.0.6.jar
[2024-11-02T09:01:43.737+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://7d12808d40db:43271/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1730538087257
[2024-11-02T09:01:43.754+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:43 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-11-02T09:01:44.934+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:44 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://7d12808d40db:43271/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1730538087257
[2024-11-02T09:01:44.939+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:44 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/commons-logging_commons-logging-1.1.3.jar
[2024-11-02T09:01:45.004+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:45 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://7d12808d40db:43271/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1730538087257
[2024-11-02T09:01:45.006+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:45 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-11-02T09:01:45.089+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:45 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://7d12808d40db:43271/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1730538087257
[2024-11-02T09:01:45.146+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:45 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-e3b28af1-4a24-4a39-9b34-d05c209f5bb2/userFiles-f3a13f93-f621-4988-938d-a9a78a6d49fe/org.apache.commons_commons-pool2-2.11.1.jar
[2024-11-02T09:01:46.214+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-02T09:01:47.120+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:47 INFO TransportClientFactory: Successfully created connection to spark-master/172.22.0.3:7077 after 414 ms (0 ms spent in bootstraps)
[2024-11-02T09:01:49.457+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241102090149-0000
[2024-11-02T09:01:49.578+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241102090149-0000/0 on worker-20241102085711-172.22.0.4-43987 (172.22.0.4:43987) with 1 core(s)
[2024-11-02T09:01:49.654+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20241102090149-0000/0 on hostPort 172.22.0.4:43987 with 1 core(s), 1024.0 MiB RAM
[2024-11-02T09:01:49.666+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46719.
[2024-11-02T09:01:49.666+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO NettyBlockTransferService: Server created on 7d12808d40db:46719
[2024-11-02T09:01:49.744+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-02T09:01:49.824+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7d12808d40db, 46719, None)
[2024-11-02T09:01:49.889+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO BlockManagerMasterEndpoint: Registering block manager 7d12808d40db:46719 with 434.4 MiB RAM, BlockManagerId(driver, 7d12808d40db, 46719, None)
[2024-11-02T09:01:49.911+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7d12808d40db, 46719, None)
[2024-11-02T09:01:49.917+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7d12808d40db, 46719, None)
[2024-11-02T09:01:50.689+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241102090149-0000/0 is now RUNNING
[2024-11-02T09:01:52.536+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:52 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-02T09:01:55.788+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-02T09:01:55.788+0000] {spark_submit.py:495} INFO - 24/11/02 09:01:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-02T09:02:32.782+0000] {spark_submit.py:495} INFO - 24/11/02 09:02:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.4:50380) with ID 0,  ResourceProfileId 0
[2024-11-02T09:02:34.576+0000] {spark_submit.py:495} INFO - 24/11/02 09:02:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.4:35927 with 434.4 MiB RAM, BlockManagerId(0, 172.22.0.4, 35927, None)
[2024-11-02T09:03:07.255+0000] {spark_submit.py:495} INFO - done
[2024-11-02T09:03:07.627+0000] {spark_submit.py:495} INFO - root
[2024-11-02T09:03:07.627+0000] {spark_submit.py:495} INFO - |-- sensor_id: integer (nullable = true)
[2024-11-02T09:03:07.628+0000] {spark_submit.py:495} INFO - |-- temperature: float (nullable = true)
[2024-11-02T09:03:07.628+0000] {spark_submit.py:495} INFO - |-- humidity: float (nullable = true)
[2024-11-02T09:03:07.629+0000] {spark_submit.py:495} INFO - |-- timestamp: string (nullable = true)
[2024-11-02T09:03:07.630+0000] {spark_submit.py:495} INFO - 
[2024-11-02T09:03:08.237+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:08 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-11-02T09:03:08.891+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-11-02T09:03:09.257+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:09 INFO ResolveWriteToStream: Checkpoint root /tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469 resolved to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469.
[2024-11-02T09:03:09.260+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-11-02T09:03:10.499+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/metadata using temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/.metadata.f4b62de1-f50f-4c51-948c-61d6095f55dd.tmp
[2024-11-02T09:03:13.503+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/.metadata.f4b62de1-f50f-4c51-948c-61d6095f55dd.tmp to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/metadata
[2024-11-02T09:03:13.996+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:13 INFO MicroBatchExecution: Starting [id = 740a8124-732a-4f76-8bd6-4519f89e2de6, runId = 4004685a-49bb-4217-af34-25a822c6835a]. Use file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469 to store the query checkpoint.
[2024-11-02T09:03:14.466+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:14 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4213ef93] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4acd7d8e]
[2024-11-02T09:03:15.264+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:15 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-02T09:03:15.301+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:15 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-02T09:03:15.306+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:15 INFO MicroBatchExecution: Starting new streaming query.
[2024-11-02T09:03:15.326+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:15 INFO MicroBatchExecution: Stream started from {}
[2024-11-02T09:03:21.664+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:21 INFO AdminClientConfig: AdminClientConfig values:
[2024-11-02T09:03:21.782+0000] {spark_submit.py:495} INFO - bootstrap.servers = [10.0.2.15:9092]
[2024-11-02T09:03:21.869+0000] {spark_submit.py:495} INFO - client.dns.lookup = use_all_dns_ips
[2024-11-02T09:03:21.882+0000] {spark_submit.py:495} INFO - client.id =
[2024-11-02T09:03:21.883+0000] {spark_submit.py:495} INFO - connections.max.idle.ms = 300000
[2024-11-02T09:03:21.908+0000] {spark_submit.py:495} INFO - default.api.timeout.ms = 60000
[2024-11-02T09:03:21.923+0000] {spark_submit.py:495} INFO - metadata.max.age.ms = 300000
[2024-11-02T09:03:21.923+0000] {spark_submit.py:495} INFO - metric.reporters = []
[2024-11-02T09:03:21.924+0000] {spark_submit.py:495} INFO - metrics.num.samples = 2
[2024-11-02T09:03:21.925+0000] {spark_submit.py:495} INFO - metrics.recording.level = INFO
[2024-11-02T09:03:21.947+0000] {spark_submit.py:495} INFO - metrics.sample.window.ms = 30000
[2024-11-02T09:03:21.948+0000] {spark_submit.py:495} INFO - receive.buffer.bytes = 65536
[2024-11-02T09:03:21.948+0000] {spark_submit.py:495} INFO - reconnect.backoff.max.ms = 1000
[2024-11-02T09:03:21.948+0000] {spark_submit.py:495} INFO - reconnect.backoff.ms = 50
[2024-11-02T09:03:21.949+0000] {spark_submit.py:495} INFO - request.timeout.ms = 30000
[2024-11-02T09:03:21.949+0000] {spark_submit.py:495} INFO - retries = 2147483647
[2024-11-02T09:03:22.012+0000] {spark_submit.py:495} INFO - retry.backoff.ms = 100
[2024-11-02T09:03:22.028+0000] {spark_submit.py:495} INFO - sasl.client.callback.handler.class = null
[2024-11-02T09:03:22.029+0000] {spark_submit.py:495} INFO - sasl.jaas.config = null
[2024-11-02T09:03:22.029+0000] {spark_submit.py:495} INFO - sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2024-11-02T09:03:22.030+0000] {spark_submit.py:495} INFO - sasl.kerberos.min.time.before.relogin = 60000
[2024-11-02T09:03:22.030+0000] {spark_submit.py:495} INFO - sasl.kerberos.service.name = null
[2024-11-02T09:03:22.031+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.jitter = 0.05
[2024-11-02T09:03:22.056+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.window.factor = 0.8
[2024-11-02T09:03:22.056+0000] {spark_submit.py:495} INFO - sasl.login.callback.handler.class = null
[2024-11-02T09:03:22.056+0000] {spark_submit.py:495} INFO - sasl.login.class = null
[2024-11-02T09:03:22.056+0000] {spark_submit.py:495} INFO - sasl.login.connect.timeout.ms = null
[2024-11-02T09:03:22.057+0000] {spark_submit.py:495} INFO - sasl.login.read.timeout.ms = null
[2024-11-02T09:03:22.057+0000] {spark_submit.py:495} INFO - sasl.login.refresh.buffer.seconds = 300
[2024-11-02T09:03:22.058+0000] {spark_submit.py:495} INFO - sasl.login.refresh.min.period.seconds = 60
[2024-11-02T09:03:22.058+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.factor = 0.8
[2024-11-02T09:03:22.058+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.jitter = 0.05
[2024-11-02T09:03:22.058+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.max.ms = 10000
[2024-11-02T09:03:22.059+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.ms = 100
[2024-11-02T09:03:22.059+0000] {spark_submit.py:495} INFO - sasl.mechanism = GSSAPI
[2024-11-02T09:03:22.059+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.clock.skew.seconds = 30
[2024-11-02T09:03:22.060+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.audience = null
[2024-11-02T09:03:22.060+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.issuer = null
[2024-11-02T09:03:22.060+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2024-11-02T09:03:22.061+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2024-11-02T09:03:22.061+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2024-11-02T09:03:22.061+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.url = null
[2024-11-02T09:03:22.061+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.scope.claim.name = scope
[2024-11-02T09:03:22.062+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.sub.claim.name = sub
[2024-11-02T09:03:22.062+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.token.endpoint.url = null
[2024-11-02T09:03:22.062+0000] {spark_submit.py:495} INFO - security.protocol = PLAINTEXT
[2024-11-02T09:03:22.062+0000] {spark_submit.py:495} INFO - security.providers = null
[2024-11-02T09:03:22.063+0000] {spark_submit.py:495} INFO - send.buffer.bytes = 131072
[2024-11-02T09:03:22.063+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.max.ms = 30000
[2024-11-02T09:03:22.063+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.ms = 10000
[2024-11-02T09:03:22.063+0000] {spark_submit.py:495} INFO - ssl.cipher.suites = null
[2024-11-02T09:03:22.064+0000] {spark_submit.py:495} INFO - ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2024-11-02T09:03:22.064+0000] {spark_submit.py:495} INFO - ssl.endpoint.identification.algorithm = https
[2024-11-02T09:03:22.064+0000] {spark_submit.py:495} INFO - ssl.engine.factory.class = null
[2024-11-02T09:03:22.065+0000] {spark_submit.py:495} INFO - ssl.key.password = null
[2024-11-02T09:03:22.065+0000] {spark_submit.py:495} INFO - ssl.keymanager.algorithm = SunX509
[2024-11-02T09:03:22.065+0000] {spark_submit.py:495} INFO - ssl.keystore.certificate.chain = null
[2024-11-02T09:03:22.066+0000] {spark_submit.py:495} INFO - ssl.keystore.key = null
[2024-11-02T09:03:22.066+0000] {spark_submit.py:495} INFO - ssl.keystore.location = null
[2024-11-02T09:03:22.066+0000] {spark_submit.py:495} INFO - ssl.keystore.password = null
[2024-11-02T09:03:22.067+0000] {spark_submit.py:495} INFO - ssl.keystore.type = JKS
[2024-11-02T09:03:22.067+0000] {spark_submit.py:495} INFO - ssl.protocol = TLSv1.3
[2024-11-02T09:03:22.067+0000] {spark_submit.py:495} INFO - ssl.provider = null
[2024-11-02T09:03:22.067+0000] {spark_submit.py:495} INFO - ssl.secure.random.implementation = null
[2024-11-02T09:03:22.068+0000] {spark_submit.py:495} INFO - ssl.trustmanager.algorithm = PKIX
[2024-11-02T09:03:22.068+0000] {spark_submit.py:495} INFO - ssl.truststore.certificates = null
[2024-11-02T09:03:22.068+0000] {spark_submit.py:495} INFO - ssl.truststore.location = null
[2024-11-02T09:03:22.069+0000] {spark_submit.py:495} INFO - ssl.truststore.password = null
[2024-11-02T09:03:22.069+0000] {spark_submit.py:495} INFO - ssl.truststore.type = JKS
[2024-11-02T09:03:22.069+0000] {spark_submit.py:495} INFO - 
[2024-11-02T09:03:23.187+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:23 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-11-02T09:03:23.239+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:23 INFO AppInfoParser: Kafka version: 3.3.2
[2024-11-02T09:03:23.290+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:23 INFO AppInfoParser: Kafka commitId: b66af662e61082cb
[2024-11-02T09:03:23.335+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:23 INFO AppInfoParser: Kafka startTimeMs: 1730538203182
[2024-11-02T09:03:30.099+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/sources/0/0 using temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/sources/0/.0.34f6aca4-becd-4603-9aff-1cc3941c97c0.tmp
[2024-11-02T09:03:33.441+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/sources/0/.0.34f6aca4-becd-4603-9aff-1cc3941c97c0.tmp to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/sources/0/0
[2024-11-02T09:03:33.441+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:33 INFO KafkaMicroBatchStream: Initial offsets: {"raw_data":{"0":6232}}
[2024-11-02T09:03:34.980+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/offsets/0 using temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/offsets/.0.5c03e9d2-4557-4d18-be84-570e7680c304.tmp
[2024-11-02T09:03:36.049+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/offsets/.0.5c03e9d2-4557-4d18-be84-570e7680c304.tmp to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/offsets/0
[2024-11-02T09:03:36.107+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:36 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1730538214520,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T09:03:42.697+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T09:03:43.439+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T09:03:44.528+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T09:03:44.626+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T09:03:45.616+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T09:03:45.639+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T09:03:50.416+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:50 INFO CodeGenerator: Code generated in 2937.660326 ms
[2024-11-02T09:03:51.182+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T09:03:51.215+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T09:03:51.393+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T09:03:51.410+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T09:03:51.416+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T09:03:51.433+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO DAGScheduler: Missing parents: List()
[2024-11-02T09:03:51.485+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:51 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T09:03:52.616+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
[2024-11-02T09:03:52.930+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2036.0 B, free 434.4 MiB)
[2024-11-02T09:03:52.957+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7d12808d40db:46719 (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T09:03:52.988+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2024-11-02T09:03:53.397+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T09:03:53.430+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-02T09:03:54.152+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 7388 bytes)
[2024-11-02T09:03:57.960+0000] {spark_submit.py:495} INFO - 24/11/02 09:03:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.4:35927 (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T09:04:02.868+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9100 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T09:04:02.933+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-02T09:04:03.043+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 11.348 s
[2024-11-02T09:04:03.061+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T09:04:03.086+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-02T09:04:03.087+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 11.853399 s
[2024-11-02T09:04:03.088+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T09:04:03.111+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T09:04:03.113+0000] {spark_submit.py:495} INFO - Batch: 0
[2024-11-02T09:04:03.113+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T09:04:03.597+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+---------+
[2024-11-02T09:04:03.598+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp|
[2024-11-02T09:04:03.606+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+---------+
[2024-11-02T09:04:03.607+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+---------+
[2024-11-02T09:04:03.607+0000] {spark_submit.py:495} INFO - 
[2024-11-02T09:04:03.607+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T09:04:03.950+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/commits/0 using temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/commits/.0.464763dd-decf-4f90-85e2-b6eea0198dee.tmp
[2024-11-02T09:04:04.945+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/commits/.0.464763dd-decf-4f90-85e2-b6eea0198dee.tmp to file:/tmp/temporary-03bb0d95-1f73-4a02-9d2f-1766987a0469/commits/0
[2024-11-02T09:04:05.084+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:04:05.099+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:04:05.100+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:04:05.100+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:04:05.100+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:03:15.249Z",
[2024-11-02T09:04:05.100+0000] {spark_submit.py:495} INFO - "batchId" : 0,
[2024-11-02T09:04:05.101+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:05.101+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:05.101+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:05.101+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:04:05.102+0000] {spark_submit.py:495} INFO - "addBatch" : 19550,
[2024-11-02T09:04:05.102+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1355,
[2024-11-02T09:04:05.102+0000] {spark_submit.py:495} INFO - "getBatch" : 252,
[2024-11-02T09:04:05.102+0000] {spark_submit.py:495} INFO - "latestOffset" : 19081,
[2024-11-02T09:04:05.103+0000] {spark_submit.py:495} INFO - "queryPlanning" : 7656,
[2024-11-02T09:04:05.103+0000] {spark_submit.py:495} INFO - "triggerExecution" : 49674,
[2024-11-02T09:04:05.103+0000] {spark_submit.py:495} INFO - "walCommit" : 1496
[2024-11-02T09:04:05.103+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:05.103+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:04:05.104+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:04:05.104+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:04:05.104+0000] {spark_submit.py:495} INFO - "startOffset" : null,
[2024-11-02T09:04:05.104+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:04:05.105+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:05.105+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:05.105+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:05.105+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:05.106+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:04:05.106+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:05.106+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:05.106+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:05.106+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:05.107+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:05.107+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:05.107+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:05.107+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:04:05.108+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:04:05.108+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:04:05.108+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:04:05.108+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:05.109+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:04:05.109+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:04:05.109+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:04:05.109+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:04:05.109+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:05.110+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:16.418+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:04:16.418+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:04:16.418+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:04:16.418+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:04:16.419+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:04:14.762Z",
[2024-11-02T09:04:16.419+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:04:16.419+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:16.419+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:16.419+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:16.419+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:04:16.420+0000] {spark_submit.py:495} INFO - "latestOffset" : 1342,
[2024-11-02T09:04:16.420+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1357
[2024-11-02T09:04:16.420+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:16.420+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:04:16.420+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:04:16.437+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:04:16.438+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:04:16.438+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:16.438+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:16.438+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:16.438+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:16.439+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:04:16.439+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:16.439+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:16.439+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:16.439+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:16.439+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:04:16.448+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:16.449+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:16.449+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:16.449+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:16.449+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:16.449+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:16.449+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:16.450+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:04:16.450+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:04:16.450+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:04:16.450+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:04:16.450+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:16.450+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:04:16.451+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:04:16.451+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:04:16.451+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:04:16.451+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:16.451+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:26.289+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:04:26.291+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:04:26.292+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:04:26.292+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:04:26.292+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:04:26.115Z",
[2024-11-02T09:04:26.293+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:04:26.293+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:26.293+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:26.293+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:26.294+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:04:26.294+0000] {spark_submit.py:495} INFO - "latestOffset" : 154,
[2024-11-02T09:04:26.295+0000] {spark_submit.py:495} INFO - "triggerExecution" : 154
[2024-11-02T09:04:26.295+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:26.295+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:04:26.296+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:04:26.296+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:04:26.296+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:04:26.296+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:26.297+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:26.297+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:26.297+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:26.297+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:04:26.297+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:26.298+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:26.298+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:26.298+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:26.298+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:04:26.299+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:26.299+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:26.299+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:26.300+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:26.300+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:26.300+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:26.300+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:26.301+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:04:26.301+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:04:26.301+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:04:26.301+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:04:26.302+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:26.302+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:04:26.302+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:04:26.302+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:04:26.303+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:04:26.303+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:26.303+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:36.365+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:04:36.366+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:04:36.366+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:04:36.366+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:04:36.367+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:04:36.240Z",
[2024-11-02T09:04:36.367+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:04:36.367+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:36.367+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:36.367+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:36.368+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:04:36.368+0000] {spark_submit.py:495} INFO - "latestOffset" : 93,
[2024-11-02T09:04:36.368+0000] {spark_submit.py:495} INFO - "triggerExecution" : 93
[2024-11-02T09:04:36.368+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:36.369+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:04:36.383+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:04:36.384+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:04:36.384+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:04:36.384+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:36.391+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:36.392+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:36.392+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:36.392+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:04:36.401+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:36.402+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:36.402+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:36.403+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:36.403+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:04:36.403+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:36.404+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:36.404+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:36.404+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:36.404+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:36.405+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:36.405+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:36.405+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:04:36.405+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:04:36.406+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:04:36.406+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:04:36.406+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:36.407+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:04:36.407+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:04:36.430+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:04:36.434+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:04:36.435+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:36.435+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:47.137+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:04:47.210+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:04:47.211+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:04:47.212+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:04:47.212+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:04:46.342Z",
[2024-11-02T09:04:47.213+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:04:47.213+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:47.214+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:47.247+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:47.247+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:04:47.248+0000] {spark_submit.py:495} INFO - "latestOffset" : 554,
[2024-11-02T09:04:47.248+0000] {spark_submit.py:495} INFO - "triggerExecution" : 555
[2024-11-02T09:04:47.248+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:47.248+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:04:47.249+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:04:47.249+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:04:47.249+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:04:47.249+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:47.250+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:47.250+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:47.250+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:47.251+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:04:47.251+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:47.252+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:47.252+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:47.252+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:47.253+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:04:47.253+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:47.254+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:47.254+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:47.255+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:47.255+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:47.256+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:47.256+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:47.256+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:04:47.257+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:04:47.257+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:04:47.258+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:04:47.258+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:47.258+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:04:47.259+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:04:47.259+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:04:47.260+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:04:47.260+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:47.260+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:57.118+0000] {spark_submit.py:495} INFO - 24/11/02 09:04:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:04:57.119+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:04:57.143+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:04:57.144+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:04:57.144+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:04:56.666Z",
[2024-11-02T09:04:57.144+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:04:57.145+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:57.145+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:57.145+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:57.145+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:04:57.146+0000] {spark_submit.py:495} INFO - "latestOffset" : 430,
[2024-11-02T09:04:57.146+0000] {spark_submit.py:495} INFO - "triggerExecution" : 438
[2024-11-02T09:04:57.146+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:57.146+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:04:57.160+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:04:57.171+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:04:57.172+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:04:57.172+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:57.172+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:57.172+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:57.173+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:57.173+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:04:57.173+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:57.174+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:57.174+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:57.174+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:57.174+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:04:57.174+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:04:57.175+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:04:57.175+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:57.175+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:04:57.175+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:04:57.176+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:04:57.176+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:04:57.176+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:04:57.176+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:04:57.176+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:04:57.177+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:04:57.177+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:57.177+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:04:57.177+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:04:57.177+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:04:57.178+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:04:57.178+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:04:57.178+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:07.156+0000] {spark_submit.py:495} INFO - 24/11/02 09:05:07 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:05:07.161+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:05:07.162+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:05:07.162+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:05:07.162+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:05:07.055Z",
[2024-11-02T09:05:07.162+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:05:07.162+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:07.163+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:07.163+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:07.163+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:05:07.163+0000] {spark_submit.py:495} INFO - "latestOffset" : 71,
[2024-11-02T09:05:07.163+0000] {spark_submit.py:495} INFO - "triggerExecution" : 78
[2024-11-02T09:05:07.164+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:07.164+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:05:07.164+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:05:07.164+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:05:07.164+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:05:07.165+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:07.165+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:07.165+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:07.165+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:07.165+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:05:07.166+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:07.166+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:07.166+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:07.166+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:07.166+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:05:07.166+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:07.167+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:07.167+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:07.167+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:07.167+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:07.167+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:07.168+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:07.168+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:05:07.168+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:05:07.168+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:05:07.168+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:05:07.169+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:07.169+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:05:07.169+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:05:07.169+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:05:07.169+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:05:07.170+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:07.170+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:17.176+0000] {spark_submit.py:495} INFO - 24/11/02 09:05:17 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:05:17.180+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:05:17.203+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:05:17.204+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:05:17.204+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:05:17.065Z",
[2024-11-02T09:05:17.204+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:05:17.204+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:17.204+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:17.205+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:17.205+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:05:17.205+0000] {spark_submit.py:495} INFO - "latestOffset" : 103,
[2024-11-02T09:05:17.205+0000] {spark_submit.py:495} INFO - "triggerExecution" : 103
[2024-11-02T09:05:17.206+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:17.217+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:05:17.231+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:05:17.232+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:05:17.232+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:05:17.232+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:17.232+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:17.232+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:17.233+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:17.233+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:05:17.233+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:17.233+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:17.233+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:17.233+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:17.234+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:05:17.234+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:17.234+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:17.234+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:17.234+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:17.234+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:17.235+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:17.235+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:17.235+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:05:17.235+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:05:17.235+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:05:17.236+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:05:17.236+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:17.236+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:05:17.236+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:05:17.261+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:05:17.262+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:05:17.262+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:17.262+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:27.450+0000] {spark_submit.py:495} INFO - 24/11/02 09:05:27 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:05:27.467+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:05:27.468+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:05:27.468+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:05:27.468+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:05:26.960Z",
[2024-11-02T09:05:27.468+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:05:27.475+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:27.475+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:27.475+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:27.476+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:05:27.476+0000] {spark_submit.py:495} INFO - "latestOffset" : 377,
[2024-11-02T09:05:27.539+0000] {spark_submit.py:495} INFO - "triggerExecution" : 378
[2024-11-02T09:05:27.539+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:27.540+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:05:27.540+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:05:27.540+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:05:27.540+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:05:27.541+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:27.541+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:27.541+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:27.541+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:27.541+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:05:27.542+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:27.542+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:27.542+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:27.542+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:27.542+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:05:27.543+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:27.543+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:27.543+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:27.544+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:27.544+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:27.544+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:27.544+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:27.544+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:05:27.545+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:05:27.545+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:05:27.545+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:05:27.545+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:27.545+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:05:27.546+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:05:27.546+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:05:27.546+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:05:27.546+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:27.546+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:37.503+0000] {spark_submit.py:495} INFO - 24/11/02 09:05:37 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:05:37.503+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:05:37.503+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:05:37.504+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:05:37.518+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:05:37.282Z",
[2024-11-02T09:05:37.518+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:05:37.518+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:37.518+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:37.519+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:37.519+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:05:37.519+0000] {spark_submit.py:495} INFO - "latestOffset" : 165,
[2024-11-02T09:05:37.519+0000] {spark_submit.py:495} INFO - "triggerExecution" : 166
[2024-11-02T09:05:37.519+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:37.519+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:05:37.532+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:05:37.532+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:05:37.532+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:05:37.532+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:37.533+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:37.533+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:37.533+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:37.533+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:05:37.533+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:37.533+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:37.534+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:37.534+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:37.534+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:05:37.534+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:37.534+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:37.534+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:37.535+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:37.535+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:37.535+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:37.535+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:37.535+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:05:37.535+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:05:37.536+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:05:37.537+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:37.537+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:47.797+0000] {spark_submit.py:495} INFO - 24/11/02 09:05:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:05:47.959+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:05:47.961+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:05:47.979+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:05:47.992+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:05:47.300Z",
[2024-11-02T09:05:48.003+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:05:48.004+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:48.004+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:48.004+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:48.004+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:05:48.005+0000] {spark_submit.py:495} INFO - "latestOffset" : 330,
[2024-11-02T09:05:48.005+0000] {spark_submit.py:495} INFO - "triggerExecution" : 341
[2024-11-02T09:05:48.005+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:48.005+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:05:48.006+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:05:48.006+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:05:48.006+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:05:48.006+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:48.007+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:48.007+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:48.007+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:48.007+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:05:48.008+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:48.008+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:48.008+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:48.009+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:48.009+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:05:48.009+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:48.009+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:48.010+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:48.010+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:48.010+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:48.010+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:48.011+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:48.011+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:05:48.011+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:05:48.011+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:05:48.012+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:05:48.012+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:48.012+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:05:48.012+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:05:48.013+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:05:48.013+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:05:48.013+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:48.013+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:57.643+0000] {spark_submit.py:495} INFO - 24/11/02 09:05:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:05:57.654+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:05:57.654+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:05:57.655+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:05:57.655+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:05:57.385Z",
[2024-11-02T09:05:57.655+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:05:57.656+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:57.656+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:57.656+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:57.656+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:05:57.657+0000] {spark_submit.py:495} INFO - "latestOffset" : 229,
[2024-11-02T09:05:57.657+0000] {spark_submit.py:495} INFO - "triggerExecution" : 255
[2024-11-02T09:05:57.657+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:57.658+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:05:57.658+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:05:57.658+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:05:57.658+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:05:57.659+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:57.659+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:57.659+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:57.660+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:57.660+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:05:57.660+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:57.661+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:57.661+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:57.661+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:57.678+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:05:57.682+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:05:57.683+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:05:57.683+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:57.683+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:05:57.684+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:05:57.684+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:05:57.684+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:05:57.684+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:05:57.693+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:05:57.701+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:05:57.702+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:05:57.702+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:57.702+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:05:57.702+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:05:57.703+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:05:57.703+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:05:57.703+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:05:57.703+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:07.677+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:07 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:06:07.679+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:06:07.691+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:06:07.696+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:06:07.697+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:06:07.609Z",
[2024-11-02T09:06:07.697+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:06:07.698+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:07.698+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:07.699+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:07.699+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:06:07.709+0000] {spark_submit.py:495} INFO - "latestOffset" : 39,
[2024-11-02T09:06:07.710+0000] {spark_submit.py:495} INFO - "triggerExecution" : 39
[2024-11-02T09:06:07.710+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:07.710+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:06:07.711+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:06:07.711+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:06:07.711+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:06:07.711+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:07.712+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:07.712+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:07.712+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:07.712+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:06:07.713+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:07.713+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:07.713+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:07.713+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:07.713+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:06:07.714+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:07.714+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:07.714+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:07.714+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:07.714+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:07.715+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:07.715+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:07.715+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:06:07.715+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:06:07.715+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:06:07.716+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:06:07.716+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:07.716+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:06:07.716+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:06:07.716+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:06:07.717+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:06:07.717+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:07.717+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:17.906+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:17 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:06:17.918+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:06:17.918+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:06:17.977+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:06:17.998+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:06:17.488Z",
[2024-11-02T09:06:18.003+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:06:18.003+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:18.004+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:18.019+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:18.020+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:06:18.020+0000] {spark_submit.py:495} INFO - "latestOffset" : 379,
[2024-11-02T09:06:18.043+0000] {spark_submit.py:495} INFO - "triggerExecution" : 379
[2024-11-02T09:06:18.044+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:18.051+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:06:18.061+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:06:18.071+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:06:18.072+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:06:18.072+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:18.072+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:18.073+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:18.073+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:18.073+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:06:18.074+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:18.074+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:18.074+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:18.074+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:18.075+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:06:18.075+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:18.075+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:18.075+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:18.075+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:18.076+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:18.076+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:18.076+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:18.077+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:06:18.077+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:06:18.077+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:06:18.077+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:06:18.077+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:18.078+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:06:18.078+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:06:18.078+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:06:18.078+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:06:18.078+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:18.079+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:27.925+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:27 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:06:27.941+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:06:27.941+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:06:27.941+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:06:27.942+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:06:27.623Z",
[2024-11-02T09:06:27.942+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:06:27.942+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:27.942+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:27.943+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:27.943+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:06:27.943+0000] {spark_submit.py:495} INFO - "latestOffset" : 264,
[2024-11-02T09:06:27.943+0000] {spark_submit.py:495} INFO - "triggerExecution" : 265
[2024-11-02T09:06:27.943+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:27.944+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:06:27.944+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:06:27.944+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:06:27.944+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:06:27.944+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:27.944+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:27.945+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:27.945+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:27.945+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:06:27.945+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:27.945+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:27.946+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:27.946+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:27.946+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:06:27.946+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:27.946+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:27.947+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:27.947+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:27.947+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:27.947+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:27.947+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:27.948+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:06:27.948+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:06:27.948+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:06:27.948+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:06:27.948+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:27.949+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:06:27.949+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:06:27.949+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:06:27.949+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:06:27.949+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:27.950+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:29.829+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.22.0.4:35927 in memory (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T09:06:29.902+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7d12808d40db:46719 in memory (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T09:06:37.923+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:37 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:06:37.924+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:06:37.924+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:06:37.924+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:06:37.924+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:06:37.876Z",
[2024-11-02T09:06:37.925+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:06:37.925+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:37.925+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:37.949+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:37.962+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:06:37.970+0000] {spark_submit.py:495} INFO - "latestOffset" : 42,
[2024-11-02T09:06:37.970+0000] {spark_submit.py:495} INFO - "triggerExecution" : 42
[2024-11-02T09:06:37.971+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:37.971+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:06:37.971+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:06:37.972+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:06:37.972+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:06:37.972+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:37.972+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:37.972+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:37.981+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:37.993+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:06:37.994+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:37.994+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:37.994+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:37.995+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:37.995+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:06:37.995+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:37.995+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:37.995+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:37.996+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:37.996+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:37.996+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:37.996+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:37.996+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:06:37.997+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:06:37.997+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:06:37.997+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:06:37.997+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:37.997+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:06:37.998+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:06:37.998+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:06:37.998+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:06:37.998+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:37.998+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:47.951+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:47 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:06:47.986+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:06:47.987+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:06:47.987+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:06:47.987+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:06:47.888Z",
[2024-11-02T09:06:47.987+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:06:47.988+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:47.988+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:48.033+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:48.035+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:06:48.044+0000] {spark_submit.py:495} INFO - "latestOffset" : 55,
[2024-11-02T09:06:48.047+0000] {spark_submit.py:495} INFO - "triggerExecution" : 55
[2024-11-02T09:06:48.048+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:48.048+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:06:48.048+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:06:48.048+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:06:48.049+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:06:48.049+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:48.049+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:48.049+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:48.049+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:48.050+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:06:48.050+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:48.050+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:48.050+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:48.051+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:48.051+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:06:48.051+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:48.051+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:48.051+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:48.073+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:48.083+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:48.085+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:48.086+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:48.086+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:06:48.086+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:06:48.086+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:06:48.087+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:06:48.087+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:48.087+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:06:48.087+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:06:48.087+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:06:48.088+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:06:48.088+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:48.088+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:57.988+0000] {spark_submit.py:495} INFO - 24/11/02 09:06:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:06:57.989+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:06:57.991+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:06:57.991+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:06:57.992+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:06:57.893Z",
[2024-11-02T09:06:57.992+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:06:57.992+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:58.005+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:58.007+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:58.008+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:06:58.008+0000] {spark_submit.py:495} INFO - "latestOffset" : 76,
[2024-11-02T09:06:58.008+0000] {spark_submit.py:495} INFO - "triggerExecution" : 80
[2024-11-02T09:06:58.008+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:58.008+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:06:58.008+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:06:58.015+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:06:58.016+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:06:58.016+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:58.019+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:58.019+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:58.020+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:58.020+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:06:58.020+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:58.020+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:58.020+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:58.020+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:58.027+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:06:58.027+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:06:58.039+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:06:58.040+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:58.040+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:06:58.052+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:06:58.053+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:06:58.053+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:06:58.053+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:06:58.053+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:06:58.054+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:06:58.054+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:06:58.054+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:58.055+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:06:58.055+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:06:58.055+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:06:58.055+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:06:58.056+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:06:58.056+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:08.101+0000] {spark_submit.py:495} INFO - 24/11/02 09:07:08 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:07:08.102+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:07:08.103+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:07:08.103+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:07:08.103+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:07:07.914Z",
[2024-11-02T09:07:08.104+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:07:08.104+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:08.104+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:08.104+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:08.105+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:07:08.105+0000] {spark_submit.py:495} INFO - "latestOffset" : 139,
[2024-11-02T09:07:08.105+0000] {spark_submit.py:495} INFO - "triggerExecution" : 141
[2024-11-02T09:07:08.133+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:08.133+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:07:08.134+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:07:08.134+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:07:08.134+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:07:08.134+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:08.134+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:08.135+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:08.135+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:08.135+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:07:08.135+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:08.135+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:08.136+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:08.136+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:08.136+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:07:08.136+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:08.136+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:08.137+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:08.137+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:08.137+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:08.137+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:08.137+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:08.138+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:07:08.138+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:07:08.138+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:07:08.138+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:07:08.139+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:08.139+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:07:08.139+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:07:08.139+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:07:08.140+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:07:08.140+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:08.140+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:18.069+0000] {spark_submit.py:495} INFO - 24/11/02 09:07:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:07:18.073+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:07:18.101+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:07:18.102+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:07:18.102+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:07:17.951Z",
[2024-11-02T09:07:18.103+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:07:18.103+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:18.104+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:18.104+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:18.105+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:07:18.105+0000] {spark_submit.py:495} INFO - "latestOffset" : 112,
[2024-11-02T09:07:18.106+0000] {spark_submit.py:495} INFO - "triggerExecution" : 112
[2024-11-02T09:07:18.106+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:18.107+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:07:18.107+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:07:18.108+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:07:18.108+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:07:18.109+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:18.109+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:18.110+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:18.110+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:18.111+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:07:18.111+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:18.112+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:18.112+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:18.113+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:18.113+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:07:18.114+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:18.114+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:18.115+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:18.115+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:18.115+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:18.115+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:18.115+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:18.115+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:07:18.116+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:07:18.116+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:07:18.116+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:07:18.116+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:18.117+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:07:18.117+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:07:18.117+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:07:18.117+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:07:18.118+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:18.118+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:28.170+0000] {spark_submit.py:495} INFO - 24/11/02 09:07:28 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:07:28.171+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:07:28.177+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:07:28.178+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:07:28.178+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:07:28.068Z",
[2024-11-02T09:07:28.178+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:07:28.179+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:28.179+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:28.179+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:28.179+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:07:28.179+0000] {spark_submit.py:495} INFO - "latestOffset" : 62,
[2024-11-02T09:07:28.180+0000] {spark_submit.py:495} INFO - "triggerExecution" : 66
[2024-11-02T09:07:28.180+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:28.180+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:07:28.180+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:07:28.180+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:07:28.181+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:07:28.181+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:28.181+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:28.181+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:28.181+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:28.207+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:07:28.214+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:28.214+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:28.215+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:28.215+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:28.215+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:07:28.215+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:28.216+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:28.216+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:28.217+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:28.217+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:28.217+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:28.217+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:28.230+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:07:28.231+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:07:28.231+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:07:28.231+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:07:28.232+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:28.232+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:07:28.253+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:07:28.254+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:07:28.254+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:07:28.254+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:28.255+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:38.636+0000] {spark_submit.py:495} INFO - 24/11/02 09:07:38 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:07:38.654+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:07:38.657+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:07:38.733+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:07:38.753+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:07:38.137Z",
[2024-11-02T09:07:38.754+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:07:38.754+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:38.782+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:38.783+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:38.783+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:07:38.783+0000] {spark_submit.py:495} INFO - "latestOffset" : 375,
[2024-11-02T09:07:38.783+0000] {spark_submit.py:495} INFO - "triggerExecution" : 376
[2024-11-02T09:07:38.784+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:38.784+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:07:38.784+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:07:38.784+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:07:38.784+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:07:38.785+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:38.785+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:38.785+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:38.785+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:38.786+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:07:38.786+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:38.786+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:38.786+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:38.787+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:38.787+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:07:38.787+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:38.940+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:38.941+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:38.984+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:38.999+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:38.999+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:39.000+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:39.000+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:07:39.000+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:07:39.000+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:07:39.000+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:07:39.001+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:39.001+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:07:39.001+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:07:39.001+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:07:39.036+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:07:39.036+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:39.036+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:49.125+0000] {spark_submit.py:495} INFO - 24/11/02 09:07:49 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:07:49.160+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:07:49.166+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:07:49.167+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:07:49.168+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:07:48.582Z",
[2024-11-02T09:07:49.169+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:07:49.169+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:49.169+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:49.169+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:49.170+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:07:49.170+0000] {spark_submit.py:495} INFO - "latestOffset" : 445,
[2024-11-02T09:07:49.170+0000] {spark_submit.py:495} INFO - "triggerExecution" : 445
[2024-11-02T09:07:49.170+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:49.279+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:07:49.280+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:07:49.291+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:07:49.300+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:07:49.303+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:49.304+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:49.308+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:49.315+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:49.319+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:07:49.324+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:49.326+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:49.327+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:49.330+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:49.330+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:07:49.331+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:49.331+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:49.331+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:49.332+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:49.332+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:49.332+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:49.333+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:49.334+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:07:49.334+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:07:49.335+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:07:49.336+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:07:49.336+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:49.337+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:07:49.338+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:07:49.338+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:07:49.339+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:07:49.339+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:49.339+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:59.101+0000] {spark_submit.py:495} INFO - 24/11/02 09:07:59 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:07:59.101+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:07:59.102+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:07:59.102+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:07:59.102+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:07:58.996Z",
[2024-11-02T09:07:59.103+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:07:59.103+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:59.103+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:59.103+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:59.104+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:07:59.104+0000] {spark_submit.py:495} INFO - "latestOffset" : 81,
[2024-11-02T09:07:59.104+0000] {spark_submit.py:495} INFO - "triggerExecution" : 81
[2024-11-02T09:07:59.105+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:59.105+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:07:59.105+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:07:59.105+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:07:59.106+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:07:59.106+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:59.106+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:59.106+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:59.106+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:59.107+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:07:59.107+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:59.107+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:59.107+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:59.108+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:59.108+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:07:59.108+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:07:59.108+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:07:59.108+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:59.109+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:07:59.109+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:07:59.109+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:07:59.109+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:07:59.109+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:07:59.110+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:07:59.110+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:07:59.110+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:07:59.110+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:59.110+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:07:59.111+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:07:59.111+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:07:59.111+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:07:59.111+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:07:59.111+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:09.189+0000] {spark_submit.py:495} INFO - 24/11/02 09:08:09 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:08:09.190+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:08:09.190+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:08:09.190+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:08:09.191+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:08:09.091Z",
[2024-11-02T09:08:09.191+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:08:09.191+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:09.191+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:09.191+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:09.192+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:08:09.192+0000] {spark_submit.py:495} INFO - "latestOffset" : 91,
[2024-11-02T09:08:09.192+0000] {spark_submit.py:495} INFO - "triggerExecution" : 91
[2024-11-02T09:08:09.192+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:09.192+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:08:09.193+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:08:09.193+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:08:09.193+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:08:09.193+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:09.193+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:09.194+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:09.194+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:09.194+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:08:09.194+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:09.194+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:09.195+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:09.195+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:09.195+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:08:09.195+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:09.195+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:09.196+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:09.196+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:09.196+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:09.196+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:09.196+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:09.197+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:08:09.197+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:08:09.197+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:08:09.197+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:08:09.198+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:09.198+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:08:09.198+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:08:09.198+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:08:09.198+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:08:09.199+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:09.199+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:19.273+0000] {spark_submit.py:495} INFO - 24/11/02 09:08:19 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:08:19.274+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:08:19.344+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:08:19.349+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:08:19.357+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:08:19.073Z",
[2024-11-02T09:08:19.398+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:08:19.404+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:19.405+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:19.406+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:19.406+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:08:19.406+0000] {spark_submit.py:495} INFO - "latestOffset" : 165,
[2024-11-02T09:08:19.408+0000] {spark_submit.py:495} INFO - "triggerExecution" : 168
[2024-11-02T09:08:19.408+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:19.409+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:08:19.409+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:08:19.410+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:08:19.410+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:08:19.410+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:19.411+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:19.411+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:19.411+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:19.412+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:08:19.412+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:19.460+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:19.461+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:19.461+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:19.461+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:08:19.461+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:19.461+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:19.462+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:19.462+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:19.462+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:19.462+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:19.462+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:19.462+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:08:19.463+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:08:19.463+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:08:19.463+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:08:19.463+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:19.463+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:08:19.464+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:08:19.464+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:08:19.464+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:08:19.464+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:19.465+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:28.742+0000] {spark_submit.py:495} INFO - 24/11/02 09:08:28 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2024-11-02T09:08:29.282+0000] {spark_submit.py:495} INFO - 24/11/02 09:08:29 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:08:29.283+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:08:29.284+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:08:29.284+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:08:29.284+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:08:29.178Z",
[2024-11-02T09:08:29.284+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:08:29.285+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:29.285+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:29.285+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:29.285+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:08:29.286+0000] {spark_submit.py:495} INFO - "latestOffset" : 98,
[2024-11-02T09:08:29.286+0000] {spark_submit.py:495} INFO - "triggerExecution" : 98
[2024-11-02T09:08:29.286+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:29.286+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:08:29.287+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:08:29.287+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:08:29.287+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:08:29.287+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:29.287+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:29.288+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:29.288+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:29.288+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:08:29.288+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:29.319+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:29.319+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:29.320+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:29.320+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:08:29.320+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:29.320+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:29.320+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:29.321+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:29.376+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:29.388+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:29.388+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:29.389+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:08:29.389+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:08:29.390+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:08:29.390+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:08:29.391+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:29.391+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:08:29.392+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:08:29.392+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:08:29.393+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:08:29.393+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:29.393+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:39.696+0000] {spark_submit.py:495} INFO - 24/11/02 09:08:39 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T09:08:39.808+0000] {spark_submit.py:495} INFO - "id" : "740a8124-732a-4f76-8bd6-4519f89e2de6",
[2024-11-02T09:08:39.808+0000] {spark_submit.py:495} INFO - "runId" : "4004685a-49bb-4217-af34-25a822c6835a",
[2024-11-02T09:08:39.808+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T09:08:39.808+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T09:08:39.329Z",
[2024-11-02T09:08:39.809+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T09:08:39.809+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:39.809+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:39.809+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:39.809+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T09:08:39.810+0000] {spark_submit.py:495} INFO - "latestOffset" : 284,
[2024-11-02T09:08:39.810+0000] {spark_submit.py:495} INFO - "triggerExecution" : 324
[2024-11-02T09:08:39.810+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:39.810+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T09:08:39.810+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T09:08:39.810+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T09:08:39.811+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T09:08:39.811+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:39.811+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:39.811+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:39.811+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:39.811+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T09:08:39.812+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:39.812+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:39.812+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:39.812+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:39.812+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T09:08:39.812+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T09:08:39.813+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T09:08:39.813+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:39.813+0000] {spark_submit.py:495} INFO - },
[2024-11-02T09:08:39.813+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T09:08:39.813+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T09:08:39.813+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T09:08:39.814+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T09:08:39.814+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T09:08:39.814+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T09:08:39.814+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T09:08:39.814+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:39.815+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T09:08:39.815+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T09:08:39.815+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@76f627dd",
[2024-11-02T09:08:39.815+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T09:08:39.815+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:39.815+0000] {spark_submit.py:495} INFO - }
[2024-11-02T09:08:41.560+0000] {local_task_job_runner.py:313} WARNING - State of this instance has been externally set to success. Terminating instance.
[2024-11-02T09:08:41.705+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-11-02T09:08:41.820+0000] {process_utils.py:132} INFO - Sending 15 to group 223. PIDs of all processes in the group: [224, 314, 223]
[2024-11-02T09:08:41.821+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 223
[2024-11-02T09:08:41.880+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-11-02T09:08:41.880+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2024-11-02T09:08:41.884+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-11-02T09:08:42.893+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=223, status='terminated', exitcode=0, started='09:00:05') (223) terminated with exit code 0
[2024-11-02T09:08:42.894+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=314, status='terminated', started='09:01:19') (314) terminated with exit code None
[2024-11-02T09:08:42.894+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=224, status='terminated', started='09:00:07') (224) terminated with exit code None
