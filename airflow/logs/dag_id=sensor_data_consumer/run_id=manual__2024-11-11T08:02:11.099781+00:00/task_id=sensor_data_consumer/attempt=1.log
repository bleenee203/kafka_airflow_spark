[2024-11-11T08:02:13.905+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-11-11T08:02:13.938+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-11T08:02:11.099781+00:00 [queued]>
[2024-11-11T08:02:13.948+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-11T08:02:11.099781+00:00 [queued]>
[2024-11-11T08:02:13.949+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-11-11T08:02:14.108+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): sensor_data_consumer> on 2024-11-11 08:02:11.099781+00:00
[2024-11-11T08:02:14.114+0000] {standard_task_runner.py:64} INFO - Started process 569 to run task
[2024-11-11T08:02:14.119+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sensor_data_consumer', 'sensor_data_consumer', 'manual__2024-11-11T08:02:11.099781+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/***_consumer.py', '--cfg-path', '/tmp/tmpfdbtkfno']
[2024-11-11T08:02:14.121+0000] {standard_task_runner.py:91} INFO - Job 18: Subtask sensor_data_consumer
[2024-11-11T08:02:14.202+0000] {task_command.py:426} INFO - Running <TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-11T08:02:11.099781+00:00 [running]> on host 4456ca8d9d4c
[2024-11-11T08:02:14.294+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Bich Ly' AIRFLOW_CTX_DAG_ID='sensor_data_consumer' AIRFLOW_CTX_TASK_ID='sensor_data_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-11-11T08:02:11.099781+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-11T08:02:11.099781+00:00'
[2024-11-11T08:02:14.294+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-11-11T08:02:14.316+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2024-11-11T08:02:14.317+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2,org.apache.hadoop:hadoop-client:3.2.1 --name KafkaSparkHDFS /opt/***/dags/spark_streaming_job.py
[2024-11-11T08:02:38.548+0000] {spark_submit.py:495} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-11T08:02:38.652+0000] {spark_submit.py:495} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-11T08:02:38.652+0000] {spark_submit.py:495} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-11T08:02:38.655+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency
[2024-11-11T08:02:38.656+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-11-11T08:02:38.657+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client added as a dependency
[2024-11-11T08:02:38.658+0000] {spark_submit.py:495} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-57bb6aeb-2db4-43e0-a718-96642b8755a5;1.0
[2024-11-11T08:02:38.658+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-11T08:02:53.891+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 in central
[2024-11-11T08:02:55.357+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 in central
[2024-11-11T08:02:56.650+0000] {spark_submit.py:495} INFO - found org.apache.kafka#kafka-clients;3.3.2 in central
[2024-11-11T08:02:57.496+0000] {spark_submit.py:495} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-11-11T08:02:58.433+0000] {spark_submit.py:495} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-11-11T08:03:04.871+0000] {spark_submit.py:495} INFO - found org.slf4j#slf4j-api;2.0.6 in central
[2024-11-11T08:03:11.296+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-11-11T08:03:12.758+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-11-11T08:03:22.157+0000] {spark_submit.py:495} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-11-11T08:03:23.789+0000] {spark_submit.py:495} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-11-11T08:03:25.274+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 in central
[2024-11-11T08:03:39.335+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-11-11T08:03:52.596+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client;3.2.1 in central
[2024-11-11T08:03:54.142+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-common;3.2.1 in central
[2024-11-11T08:03:55.618+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-annotations;3.2.1 in central
[2024-11-11T08:04:04.829+0000] {spark_submit.py:495} INFO - found com.google.guava#guava;27.0-jre in central
[2024-11-11T08:04:09.254+0000] {spark_submit.py:495} INFO - found com.google.guava#failureaccess;1.0 in central
[2024-11-11T08:04:10.874+0000] {spark_submit.py:495} INFO - found com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central
[2024-11-11T08:04:12.344+0000] {spark_submit.py:495} INFO - found org.checkerframework#checker-qual;2.5.2 in central
[2024-11-11T08:04:21.805+0000] {spark_submit.py:495} INFO - found com.google.errorprone#error_prone_annotations;2.2.0 in central
[2024-11-11T08:04:23.367+0000] {spark_submit.py:495} INFO - found com.google.j2objc#j2objc-annotations;1.1 in central
[2024-11-11T08:04:33.091+0000] {spark_submit.py:495} INFO - found org.codehaus.mojo#animal-sniffer-annotations;1.17 in central
[2024-11-11T08:04:41.333+0000] {spark_submit.py:495} INFO - found commons-cli#commons-cli;1.2 in central
[2024-11-11T08:04:51.363+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-math3;3.1.1 in central
[2024-11-11T08:05:02.033+0000] {spark_submit.py:495} INFO - found org.apache.httpcomponents#httpclient;4.5.6 in central
[2024-11-11T08:05:07.210+0000] {spark_submit.py:495} INFO - found org.apache.httpcomponents#httpcore;4.4.10 in central
[2024-11-11T08:05:12.365+0000] {spark_submit.py:495} INFO - found commons-codec#commons-codec;1.11 in central
[2024-11-11T08:05:18.605+0000] {spark_submit.py:495} INFO - found commons-io#commons-io;2.5 in central
[2024-11-11T08:05:20.136+0000] {spark_submit.py:495} INFO - found commons-net#commons-net;3.6 in central
[2024-11-11T08:05:21.680+0000] {spark_submit.py:495} INFO - found commons-collections#commons-collections;3.2.2 in central
[2024-11-11T08:05:31.547+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-servlet;9.3.24.v20180605 in central
[2024-11-11T08:05:33.042+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-security;9.3.24.v20180605 in central
[2024-11-11T08:05:34.603+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-webapp;9.3.24.v20180605 in central
[2024-11-11T08:05:36.245+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-xml;9.3.24.v20180605 in central
[2024-11-11T08:05:44.487+0000] {spark_submit.py:495} INFO - found com.sun.jersey#jersey-servlet;1.19 in central
[2024-11-11T08:05:46.141+0000] {spark_submit.py:495} INFO - found log4j#log4j;1.2.17 in central
[2024-11-11T08:05:52.431+0000] {spark_submit.py:495} INFO - found commons-beanutils#commons-beanutils;1.9.3 in central
[2024-11-11T08:05:53.824+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-configuration2;2.1.1 in central
[2024-11-11T08:05:55.345+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-lang3;3.7 in central
[2024-11-11T08:06:05.354+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-text;1.4 in central
[2024-11-11T08:06:16.290+0000] {spark_submit.py:495} INFO - found org.apache.avro#avro;1.7.7 in central
[2024-11-11T08:06:17.803+0000] {spark_submit.py:495} INFO - found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
[2024-11-11T08:06:19.295+0000] {spark_submit.py:495} INFO - found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
[2024-11-11T08:06:27.296+0000] {spark_submit.py:495} INFO - found com.thoughtworks.paranamer#paranamer;2.3 in central
[2024-11-11T08:06:32.505+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-compress;1.18 in central
[2024-11-11T08:06:33.960+0000] {spark_submit.py:495} INFO - found com.google.re2j#re2j;1.1 in central
[2024-11-11T08:06:39.659+0000] {spark_submit.py:495} INFO - found com.google.protobuf#protobuf-java;2.5.0 in central
[2024-11-11T08:06:41.117+0000] {spark_submit.py:495} INFO - found com.google.code.gson#gson;2.2.4 in central
[2024-11-11T08:06:42.692+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-auth;3.2.1 in central
[2024-11-11T08:06:44.242+0000] {spark_submit.py:495} INFO - found com.nimbusds#nimbus-jose-jwt;4.41.1 in central
[2024-11-11T08:06:45.745+0000] {spark_submit.py:495} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-11-11T08:06:51.545+0000] {spark_submit.py:495} INFO - found net.minidev#json-smart;2.3 in central
[2024-11-11T08:06:53.108+0000] {spark_submit.py:495} INFO - found net.minidev#accessors-smart;1.2 in central
[2024-11-11T08:07:04.084+0000] {spark_submit.py:495} INFO - found org.ow2.asm#asm;5.0.4 in central
[2024-11-11T08:07:08.937+0000] {spark_submit.py:495} INFO - found org.apache.curator#curator-framework;2.13.0 in central
[2024-11-11T08:07:10.430+0000] {spark_submit.py:495} INFO - found org.apache.curator#curator-client;2.13.0 in central
[2024-11-11T08:07:20.760+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-simplekdc;1.0.1 in central
[2024-11-11T08:07:22.231+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-client;1.0.1 in central
[2024-11-11T08:07:28.260+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-config;1.0.1 in central
[2024-11-11T08:07:29.275+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-core;1.0.1 in central
[2024-11-11T08:07:30.895+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-pkix;1.0.1 in central
[2024-11-11T08:07:32.394+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-asn1;1.0.1 in central
[2024-11-11T08:07:33.812+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-util;1.0.1 in central
[2024-11-11T08:07:35.293+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-common;1.0.1 in central
[2024-11-11T08:07:36.805+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-crypto;1.0.1 in central
[2024-11-11T08:07:38.403+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-util;1.0.1 in central
[2024-11-11T08:07:45.113+0000] {spark_submit.py:495} INFO - found org.apache.kerby#token-provider;1.0.1 in central
[2024-11-11T08:07:46.672+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-admin;1.0.1 in central
[2024-11-11T08:07:48.258+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-server;1.0.1 in central
[2024-11-11T08:07:49.803+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerb-identity;1.0.1 in central
[2024-11-11T08:07:50.536+0000] {spark_submit.py:495} INFO - found org.apache.kerby#kerby-xdr;1.0.1 in central
[2024-11-11T08:07:52.195+0000] {spark_submit.py:495} INFO - found org.apache.curator#curator-recipes;2.13.0 in central
[2024-11-11T08:08:02.450+0000] {spark_submit.py:495} INFO - found org.apache.htrace#htrace-core4;4.1.0-incubating in central
[2024-11-11T08:08:19.245+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.core#jackson-databind;2.9.8 in central
[2024-11-11T08:08:20.712+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.core#jackson-annotations;2.9.8 in central
[2024-11-11T08:08:22.212+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.core#jackson-core;2.9.8 in central
[2024-11-11T08:08:23.642+0000] {spark_submit.py:495} INFO - found org.codehaus.woodstox#stax2-api;3.1.4 in central
[2024-11-11T08:08:29.676+0000] {spark_submit.py:495} INFO - found com.fasterxml.woodstox#woodstox-core;5.0.3 in central
[2024-11-11T08:08:31.203+0000] {spark_submit.py:495} INFO - found dnsjava#dnsjava;2.1.7 in central
[2024-11-11T08:08:32.660+0000] {spark_submit.py:495} INFO - found javax.servlet.jsp#jsp-api;2.1 in central
[2024-11-11T08:08:34.195+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-hdfs-client;3.2.1 in central
[2024-11-11T08:08:40.907+0000] {spark_submit.py:495} INFO - found com.squareup.okhttp#okhttp;2.7.5 in central
[2024-11-11T08:08:45.977+0000] {spark_submit.py:495} INFO - found com.squareup.okio#okio;1.6.0 in central
[2024-11-11T08:08:50.972+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-yarn-api;3.2.1 in central
[2024-11-11T08:08:52.663+0000] {spark_submit.py:495} INFO - found javax.xml.bind#jaxb-api;2.2.11 in central
[2024-11-11T08:08:54.203+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-yarn-client;3.2.1 in central
[2024-11-11T08:09:01.793+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-mapreduce-client-core;3.2.1 in central
[2024-11-11T08:09:03.286+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-yarn-common;3.2.1 in central
[2024-11-11T08:09:06.264+0000] {spark_submit.py:495} INFO - found javax.servlet#javax.servlet-api;3.1.0 in central
[2024-11-11T08:09:06.817+0000] {spark_submit.py:495} INFO - found org.eclipse.jetty#jetty-util;9.3.24.v20180605 in central
[2024-11-11T08:09:08.086+0000] {spark_submit.py:495} INFO - found com.sun.jersey#jersey-core;1.19 in central
[2024-11-11T08:09:09.595+0000] {spark_submit.py:495} INFO - found javax.ws.rs#jsr311-api;1.1.1 in central
[2024-11-11T08:09:11.061+0000] {spark_submit.py:495} INFO - found com.sun.jersey#jersey-client;1.19 in central
[2024-11-11T08:09:16.740+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.8 in central
[2024-11-11T08:09:21.767+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.8 in central
[2024-11-11T08:09:23.222+0000] {spark_submit.py:495} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.8 in central
[2024-11-11T08:09:24.733+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.1 in central
[2024-11-11T08:09:26.134+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-mapreduce-client-common;3.2.1 in central
[2024-11-11T08:09:26.892+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.4.2/spark-streaming-kafka-0-10_2.12-3.4.2.jar ...
[2024-11-11T08:09:28.099+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2!spark-streaming-kafka-0-10_2.12.jar (1889ms)
[2024-11-11T08:09:28.905+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.2/spark-sql-kafka-0-10_2.12-3.4.2.jar ...
[2024-11-11T08:09:31.181+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2!spark-sql-kafka-0-10_2.12.jar (3083ms)
[2024-11-11T08:09:31.998+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/3.2.1/hadoop-client-3.2.1.jar ...
[2024-11-11T08:09:32.834+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client;3.2.1!hadoop-client.jar (1652ms)
[2024-11-11T08:09:33.580+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.4.2/spark-token-provider-kafka-0-10_2.12-3.4.2.jar ...
[2024-11-11T08:09:34.453+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2!spark-token-provider-kafka-0-10_2.12.jar (1618ms)
[2024-11-11T08:09:35.184+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.2/kafka-clients-3.3.2.jar ...
[2024-11-11T08:09:43.507+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.3.2!kafka-clients.jar (9052ms)
[2024-11-11T08:09:44.216+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2024-11-11T08:13:11.933+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (208425ms)
[2024-11-11T08:13:12.687+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-11-11T08:13:16.193+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (4254ms)
[2024-11-11T08:13:16.893+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2024-11-11T08:13:24.101+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (7904ms)
[2024-11-11T08:13:24.822+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.6/slf4j-api-2.0.6.jar ...
[2024-11-11T08:13:26.762+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.6!slf4j-api.jar (2656ms)
[2024-11-11T08:13:27.485+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2024-11-11T08:15:33.343+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (126557ms)
[2024-11-11T08:15:34.090+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-11-11T08:15:35.387+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (2040ms)
[2024-11-11T08:15:36.095+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-11-11T08:15:37.072+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (1681ms)
[2024-11-11T08:15:37.793+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-11-11T08:15:39.708+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (2635ms)
[2024-11-11T08:15:40.435+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar ...
[2024-11-11T08:15:58.155+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-common;3.2.1!hadoop-common.jar (18446ms)
[2024-11-11T08:15:58.858+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs-client/3.2.1/hadoop-hdfs-client-3.2.1.jar ...
[2024-11-11T08:16:38.863+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-hdfs-client;3.2.1!hadoop-hdfs-client.jar (40704ms)
[2024-11-11T08:16:39.687+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/3.2.1/hadoop-yarn-api-3.2.1.jar ...
[2024-11-11T08:16:48.142+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-api;3.2.1!hadoop-yarn-api.jar (9278ms)
[2024-11-11T08:16:48.847+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/3.2.1/hadoop-yarn-client-3.2.1.jar ...
[2024-11-11T08:16:49.594+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-client;3.2.1!hadoop-yarn-client.jar (1449ms)
[2024-11-11T08:16:50.316+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/3.2.1/hadoop-mapreduce-client-core-3.2.1.jar ...
[2024-11-11T08:17:03.192+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-core;3.2.1!hadoop-mapreduce-client-core.jar (13597ms)
[2024-11-11T08:17:03.953+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.1/hadoop-mapreduce-client-jobclient-3.2.1.jar ...
[2024-11-11T08:17:05.486+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.1!hadoop-mapreduce-client-jobclient.jar (2293ms)
[2024-11-11T08:17:06.193+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.2.1/hadoop-annotations-3.2.1.jar ...
[2024-11-11T08:17:07.101+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.2.1!hadoop-annotations.jar (1612ms)
[2024-11-11T08:17:07.847+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar ...
[2024-11-11T08:17:15.024+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.guava#guava;27.0-jre!guava.jar(bundle) (7922ms)
[2024-11-11T08:17:15.727+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...
[2024-11-11T08:17:16.492+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (1465ms)
[2024-11-11T08:17:17.275+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar ...
[2024-11-11T08:17:27.600+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (11106ms)
[2024-11-11T08:17:28.358+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar ...
[2024-11-11T08:17:32.374+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.6!httpclient.jar (4771ms)
[2024-11-11T08:17:33.077+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar ...
[2024-11-11T08:17:35.022+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-codec#commons-codec;1.11!commons-codec.jar (2642ms)
[2024-11-11T08:17:35.780+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.5/commons-io-2.5.jar ...
[2024-11-11T08:17:37.457+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-io#commons-io;2.5!commons-io.jar (2433ms)
[2024-11-11T08:17:38.222+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.6/commons-net-3.6.jar ...
[2024-11-11T08:17:40.774+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-net#commons-net;3.6!commons-net.jar (3316ms)
[2024-11-11T08:17:41.489+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar ...
[2024-11-11T08:17:44.958+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-collections#commons-collections;3.2.2!commons-collections.jar (4182ms)
[2024-11-11T08:17:45.663+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar ...
[2024-11-11T08:17:46.891+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-servlet;9.3.24.v20180605!jetty-servlet.jar (1931ms)
[2024-11-11T08:17:47.633+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar ...
[2024-11-11T08:17:48.933+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-webapp;9.3.24.v20180605!jetty-webapp.jar (2039ms)
[2024-11-11T08:17:49.683+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar ...
[2024-11-11T08:17:50.987+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.sun.jersey#jersey-servlet;1.19!jersey-servlet.jar (2052ms)
[2024-11-11T08:17:51.688+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...
[2024-11-11T08:17:54.018+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (3027ms)
[2024-11-11T08:17:54.728+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar ...
[2024-11-11T08:17:56.096+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.3!commons-beanutils.jar (2076ms)
[2024-11-11T08:17:56.806+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar ...
[2024-11-11T08:17:58.756+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.commons#commons-configuration2;2.1.1!commons-configuration2.jar (2657ms)
[2024-11-11T08:17:59.460+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar ...
[2024-11-11T08:18:00.880+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.commons#commons-lang3;3.7!commons-lang3.jar (2122ms)
[2024-11-11T08:18:01.588+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-text/1.4/commons-text-1.4.jar ...
[2024-11-11T08:18:02.568+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.commons#commons-text;1.4!commons-text.jar (1685ms)
[2024-11-11T08:18:03.270+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar ...
[2024-11-11T08:18:04.010+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (1437ms)
[2024-11-11T08:18:04.410+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/re2j/re2j/1.1/re2j-1.1.jar ...
[2024-11-11T08:18:04.994+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.re2j#re2j;1.1!re2j.jar (983ms)
[2024-11-11T08:18:05.486+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...
[2024-11-11T08:18:08.987+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (3991ms)
[2024-11-11T08:18:09.703+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar ...
[2024-11-11T08:18:11.396+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.code.gson#gson;2.2.4!gson.jar (2400ms)
[2024-11-11T08:18:12.105+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/3.2.1/hadoop-auth-3.2.1.jar ...
[2024-11-11T08:18:13.274+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-auth;3.2.1!hadoop-auth.jar (1875ms)
[2024-11-11T08:18:14.085+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar ...
[2024-11-11T08:18:20.084+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.curator#curator-client;2.13.0!curator-client.jar(bundle) (6807ms)
[2024-11-11T08:18:20.792+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar ...
[2024-11-11T08:18:22.060+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.curator#curator-recipes;2.13.0!curator-recipes.jar(bundle) (1974ms)
[2024-11-11T08:18:22.764+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...
[2024-11-11T08:18:29.329+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (7267ms)
[2024-11-11T08:18:30.033+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.18/commons-compress-1.18.jar ...
[2024-11-11T08:18:31.641+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.commons#commons-compress;1.18!commons-compress.jar (2310ms)
[2024-11-11T08:18:32.350+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar ...
[2024-11-11T08:18:33.127+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-simplekdc;1.0.1!kerb-simplekdc.jar (1483ms)
[2024-11-11T08:18:33.870+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.9.8/jackson-databind-2.9.8.jar ...
[2024-11-11T08:18:45.033+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.9.8!jackson-databind.jar(bundle) (11904ms)
[2024-11-11T08:18:45.739+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar ...
[2024-11-11T08:18:46.853+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.codehaus.woodstox#stax2-api;3.1.4!stax2-api.jar(bundle) (1818ms)
[2024-11-11T08:18:47.558+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar ...
[2024-11-11T08:18:49.640+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.woodstox#woodstox-core;5.0.3!woodstox-core.jar(bundle) (2785ms)
[2024-11-11T08:18:50.348+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar ...
[2024-11-11T08:18:51.961+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] dnsjava#dnsjava;2.1.7!dnsjava.jar (2319ms)
[2024-11-11T08:18:52.664+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar ...
[2024-11-11T08:18:53.405+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.guava#failureaccess;1.0!failureaccess.jar (1436ms)
[2024-11-11T08:18:54.184+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar ...
[2024-11-11T08:18:54.996+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava!listenablefuture.jar (1589ms)
[2024-11-11T08:18:55.745+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar ...
[2024-11-11T08:18:56.968+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;2.5.2!checker-qual.jar (1969ms)
[2024-11-11T08:18:57.673+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.2.0/error_prone_annotations-2.2.0.jar ...
[2024-11-11T08:18:58.401+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.errorprone#error_prone_annotations;2.2.0!error_prone_annotations.jar (1429ms)
[2024-11-11T08:18:59.322+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar ...
[2024-11-11T08:19:00.101+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.1!j2objc-annotations.jar (1698ms)
[2024-11-11T08:19:00.810+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar ...
[2024-11-11T08:19:01.530+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.17!animal-sniffer-annotations.jar (1424ms)
[2024-11-11T08:19:02.291+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar ...
[2024-11-11T08:19:03.935+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.10!httpcore.jar (2404ms)
[2024-11-11T08:19:04.640+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar ...
[2024-11-11T08:19:05.568+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-security;9.3.24.v20180605!jetty-security.jar (1628ms)
[2024-11-11T08:19:06.316+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar ...
[2024-11-11T08:19:08.373+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-xml;9.3.24.v20180605!jetty-xml.jar (2804ms)
[2024-11-11T08:19:09.090+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...
[2024-11-11T08:19:10.527+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (2151ms)
[2024-11-11T08:19:11.241+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...
[2024-11-11T08:19:13.884+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (3356ms)
[2024-11-11T08:19:14.595+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...
[2024-11-11T08:19:15.365+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (1477ms)
[2024-11-11T08:19:16.070+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar ...
[2024-11-11T08:19:17.758+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.nimbusds#nimbus-jose-jwt;4.41.1!nimbus-jose-jwt.jar (2385ms)
[2024-11-11T08:19:18.486+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/net/minidev/json-smart/2.3/json-smart-2.3.jar ...
[2024-11-11T08:19:19.498+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] net.minidev#json-smart;2.3!json-smart.jar(bundle) (1737ms)
[2024-11-11T08:19:20.266+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar ...
[2024-11-11T08:19:22.493+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.curator#curator-framework;2.13.0!curator-framework.jar(bundle) (2990ms)
[2024-11-11T08:19:23.301+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...
[2024-11-11T08:19:24.036+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (1541ms)
[2024-11-11T08:19:24.747+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar ...
[2024-11-11T08:19:25.558+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] net.minidev#accessors-smart;1.2!accessors-smart.jar(bundle) (1517ms)
[2024-11-11T08:19:26.263+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar ...
[2024-11-11T08:19:27.103+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.ow2.asm#asm;5.0.4!asm.jar (1544ms)
[2024-11-11T08:19:27.815+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar ...
[2024-11-11T08:19:29.456+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-client;1.0.1!kerb-client.jar (2350ms)
[2024-11-11T08:19:30.226+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar ...
[2024-11-11T08:19:31.301+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-admin;1.0.1!kerb-admin.jar (1843ms)
[2024-11-11T08:19:32.025+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar ...
[2024-11-11T08:19:32.875+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerby-config;1.0.1!kerby-config.jar (1570ms)
[2024-11-11T08:19:33.644+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar ...
[2024-11-11T08:19:35.313+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-core;1.0.1!kerb-core.jar (2435ms)
[2024-11-11T08:19:36.016+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar ...
[2024-11-11T08:19:36.959+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-common;1.0.1!kerb-common.jar (1645ms)
[2024-11-11T08:19:37.664+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar ...
[2024-11-11T08:19:38.534+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-util;1.0.1!kerb-util.jar (1570ms)
[2024-11-11T08:19:39.238+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar ...
[2024-11-11T08:19:40.085+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#token-provider;1.0.1!token-provider.jar (1548ms)
[2024-11-11T08:19:40.800+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar ...
[2024-11-11T08:19:42.657+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerby-pkix;1.0.1!kerby-pkix.jar (2568ms)
[2024-11-11T08:19:43.371+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar ...
[2024-11-11T08:19:44.540+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerby-asn1;1.0.1!kerby-asn1.jar (1883ms)
[2024-11-11T08:19:45.242+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar ...
[2024-11-11T08:19:46.098+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerby-util;1.0.1!kerby-util.jar (1556ms)
[2024-11-11T08:19:46.805+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar ...
[2024-11-11T08:19:47.987+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-crypto;1.0.1!kerb-crypto.jar (1887ms)
[2024-11-11T08:19:48.689+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar ...
[2024-11-11T08:19:49.672+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-server;1.0.1!kerb-server.jar (1683ms)
[2024-11-11T08:19:50.377+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar ...
[2024-11-11T08:19:51.171+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerby-xdr;1.0.1!kerby-xdr.jar (1498ms)
[2024-11-11T08:19:51.872+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar ...
[2024-11-11T08:19:52.638+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.kerby#kerb-identity;1.0.1!kerb-identity.jar (1464ms)
[2024-11-11T08:19:53.380+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.9.8/jackson-annotations-2.9.8.jar ...
[2024-11-11T08:19:54.384+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.9.8!jackson-annotations.jar(bundle) (1744ms)
[2024-11-11T08:19:55.100+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.8/jackson-core-2.9.8.jar ...
[2024-11-11T08:19:57.984+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.9.8!jackson-core.jar(bundle) (3599ms)
[2024-11-11T08:19:58.687+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...
[2024-11-11T08:20:00.549+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (2560ms)
[2024-11-11T08:20:01.407+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar ...
[2024-11-11T08:20:03.691+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.squareup.okhttp#okhttp;2.7.5!okhttp.jar (3141ms)
[2024-11-11T08:20:04.447+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar ...
[2024-11-11T08:20:05.508+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.squareup.okio#okio;1.6.0!okio.jar (1815ms)
[2024-11-11T08:20:06.296+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...
[2024-11-11T08:20:07.702+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (2191ms)
[2024-11-11T08:20:08.449+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/3.2.1/hadoop-yarn-common-3.2.1.jar ...
[2024-11-11T08:20:16.587+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-common;3.2.1!hadoop-yarn-common.jar (8884ms)
[2024-11-11T08:20:17.289+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar ...
[2024-11-11T08:20:18.061+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] javax.servlet#javax.servlet-api;3.1.0!javax.servlet-api.jar (1473ms)
[2024-11-11T08:20:18.784+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar ...
[2024-11-11T08:20:22.911+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.3.24.v20180605!jetty-util.jar (4847ms)
[2024-11-11T08:20:23.633+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar ...
[2024-11-11T08:20:27.770+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.sun.jersey#jersey-core;1.19!jersey-core.jar (4856ms)
[2024-11-11T08:20:28.500+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar ...
[2024-11-11T08:20:29.791+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.sun.jersey#jersey-client;1.19!jersey-client.jar (2019ms)
[2024-11-11T08:20:30.593+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.9.8/jackson-module-jaxb-annotations-2.9.8.jar ...
[2024-11-11T08:20:31.386+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.8!jackson-module-jaxb-annotations.jar(bundle) (1592ms)
[2024-11-11T08:20:32.093+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.9.8/jackson-jaxrs-json-provider-2.9.8.jar ...
[2024-11-11T08:20:32.833+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.8!jackson-jaxrs-json-provider.jar(bundle) (1445ms)
[2024-11-11T08:20:33.558+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar ...
[2024-11-11T08:20:35.305+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] javax.ws.rs#jsr311-api;1.1.1!jsr311-api.jar (2470ms)
[2024-11-11T08:20:36.086+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.9.8/jackson-jaxrs-base-2.9.8.jar ...
[2024-11-11T08:20:36.956+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.8!jackson-jaxrs-base.jar(bundle) (1649ms)
[2024-11-11T08:20:37.738+0000] {spark_submit.py:495} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/3.2.1/hadoop-mapreduce-client-common-3.2.1.jar ...
[2024-11-11T08:20:43.750+0000] {spark_submit.py:495} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-common;3.2.1!hadoop-mapreduce-client-common.jar (6788ms)
[2024-11-11T08:20:43.754+0000] {spark_submit.py:495} INFO - :: resolution report :: resolve 407548ms :: artifacts dl 677542ms
[2024-11-11T08:20:43.754+0000] {spark_submit.py:495} INFO - :: modules in use:
[2024-11-11T08:20:43.754+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.core#jackson-annotations;2.9.8 from central in [default]
[2024-11-11T08:20:43.754+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.core#jackson-core;2.9.8 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.core#jackson-databind;2.9.8 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.9.8 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.9.8 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.9.8 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-11-11T08:20:43.755+0000] {spark_submit.py:495} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2024-11-11T08:20:43.756+0000] {spark_submit.py:495} INFO - com.google.code.gson#gson;2.2.4 from central in [default]
[2024-11-11T08:20:43.758+0000] {spark_submit.py:495} INFO - com.google.errorprone#error_prone_annotations;2.2.0 from central in [default]
[2024-11-11T08:20:43.759+0000] {spark_submit.py:495} INFO - com.google.guava#failureaccess;1.0 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.google.guava#guava;27.0-jre from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.google.j2objc#j2objc-annotations;1.1 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.google.protobuf#protobuf-java;2.5.0 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.google.re2j#re2j;1.1 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.squareup.okhttp#okhttp;2.7.5 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.squareup.okio#okio;1.6.0 from central in [default]
[2024-11-11T08:20:43.760+0000] {spark_submit.py:495} INFO - com.sun.jersey#jersey-client;1.19 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - com.sun.jersey#jersey-core;1.19 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - com.sun.jersey#jersey-servlet;1.19 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - com.thoughtworks.paranamer#paranamer;2.3 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - commons-beanutils#commons-beanutils;1.9.3 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - commons-cli#commons-cli;1.2 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - commons-codec#commons-codec;1.11 from central in [default]
[2024-11-11T08:20:43.761+0000] {spark_submit.py:495} INFO - commons-collections#commons-collections;3.2.2 from central in [default]
[2024-11-11T08:20:43.762+0000] {spark_submit.py:495} INFO - commons-io#commons-io;2.5 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - commons-net#commons-net;3.6 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - dnsjava#dnsjava;2.1.7 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - javax.servlet#javax.servlet-api;3.1.0 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - javax.servlet.jsp#jsp-api;2.1 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - javax.ws.rs#jsr311-api;1.1.1 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - javax.xml.bind#jaxb-api;2.2.11 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - log4j#log4j;1.2.17 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - net.minidev#accessors-smart;1.2 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - net.minidev#json-smart;2.3 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - org.apache.avro#avro;1.7.7 from central in [default]
[2024-11-11T08:20:43.763+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-compress;1.18 from central in [default]
[2024-11-11T08:20:43.764+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-configuration2;2.1.1 from central in [default]
[2024-11-11T08:20:43.764+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-lang3;3.7 from central in [default]
[2024-11-11T08:20:43.764+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-math3;3.1.1 from central in [default]
[2024-11-11T08:20:43.765+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-11-11T08:20:43.765+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-text;1.4 from central in [default]
[2024-11-11T08:20:43.769+0000] {spark_submit.py:495} INFO - org.apache.curator#curator-client;2.13.0 from central in [default]
[2024-11-11T08:20:43.769+0000] {spark_submit.py:495} INFO - org.apache.curator#curator-framework;2.13.0 from central in [default]
[2024-11-11T08:20:43.769+0000] {spark_submit.py:495} INFO - org.apache.curator#curator-recipes;2.13.0 from central in [default]
[2024-11-11T08:20:43.770+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-annotations;3.2.1 from central in [default]
[2024-11-11T08:20:43.770+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-auth;3.2.1 from central in [default]
[2024-11-11T08:20:43.770+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client;3.2.1 from central in [default]
[2024-11-11T08:20:43.771+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-11-11T08:20:43.773+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-common;3.2.1 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-hdfs-client;3.2.1 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-mapreduce-client-common;3.2.1 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-mapreduce-client-core;3.2.1 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.2.1 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-yarn-api;3.2.1 from central in [default]
[2024-11-11T08:20:43.774+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-yarn-client;3.2.1 from central in [default]
[2024-11-11T08:20:43.775+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-yarn-common;3.2.1 from central in [default]
[2024-11-11T08:20:43.775+0000] {spark_submit.py:495} INFO - org.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]
[2024-11-11T08:20:43.775+0000] {spark_submit.py:495} INFO - org.apache.httpcomponents#httpclient;4.5.6 from central in [default]
[2024-11-11T08:20:43.775+0000] {spark_submit.py:495} INFO - org.apache.httpcomponents#httpcore;4.4.10 from central in [default]
[2024-11-11T08:20:43.775+0000] {spark_submit.py:495} INFO - org.apache.kafka#kafka-clients;3.3.2 from central in [default]
[2024-11-11T08:20:43.775+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-admin;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-client;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-common;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-core;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-crypto;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-identity;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-server;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerb-util;1.0.1 from central in [default]
[2024-11-11T08:20:43.776+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-asn1;1.0.1 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-config;1.0.1 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-pkix;1.0.1 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-util;1.0.1 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.kerby#kerby-xdr;1.0.1 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.kerby#token-provider;1.0.1 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-11T08:20:43.777+0000] {spark_submit.py:495} INFO - org.checkerframework#checker-qual;2.5.2 from central in [default]
[2024-11-11T08:20:43.778+0000] {spark_submit.py:495} INFO - org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
[2024-11-11T08:20:43.778+0000] {spark_submit.py:495} INFO - org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
[2024-11-11T08:20:43.778+0000] {spark_submit.py:495} INFO - org.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]
[2024-11-11T08:20:43.778+0000] {spark_submit.py:495} INFO - org.codehaus.woodstox#stax2-api;3.1.4 from central in [default]
[2024-11-11T08:20:43.779+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-security;9.3.24.v20180605 from central in [default]
[2024-11-11T08:20:43.779+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-servlet;9.3.24.v20180605 from central in [default]
[2024-11-11T08:20:43.779+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-util;9.3.24.v20180605 from central in [default]
[2024-11-11T08:20:43.780+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-webapp;9.3.24.v20180605 from central in [default]
[2024-11-11T08:20:43.781+0000] {spark_submit.py:495} INFO - org.eclipse.jetty#jetty-xml;9.3.24.v20180605 from central in [default]
[2024-11-11T08:20:43.781+0000] {spark_submit.py:495} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-11-11T08:20:43.781+0000] {spark_submit.py:495} INFO - org.ow2.asm#asm;5.0.4 from central in [default]
[2024-11-11T08:20:43.782+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;2.0.6 from central in [default]
[2024-11-11T08:20:43.782+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-11-11T08:20:43.782+0000] {spark_submit.py:495} INFO - :: evicted modules:
[2024-11-11T08:20:43.782+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;1.7.25 by [org.slf4j#slf4j-api;2.0.6] in [default]
[2024-11-11T08:20:43.782+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.0.5 by [org.xerial.snappy#snappy-java;1.1.10.3] in [default]
[2024-11-11T08:20:43.783+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-11T08:20:43.783+0000] {spark_submit.py:495} INFO - |                  |            modules            ||   artifacts   |
[2024-11-11T08:20:43.786+0000] {spark_submit.py:495} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-11T08:20:43.786+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-11T08:20:43.788+0000] {spark_submit.py:495} INFO - |      default     |  100  |   98  |   98  |   2   ||   98  |   98  |
[2024-11-11T08:20:43.789+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-11T08:20:43.815+0000] {spark_submit.py:495} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-57bb6aeb-2db4-43e0-a718-96642b8755a5
[2024-11-11T08:20:43.817+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-11T08:20:44.377+0000] {spark_submit.py:495} INFO - 98 artifacts copied, 0 already retrieved (97732kB/562ms)
[2024-11-11T08:20:45.142+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-11T08:20:47.624+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SparkContext: Running Spark version 3.4.2
[2024-11-11T08:20:47.668+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO ResourceUtils: ==============================================================
[2024-11-11T08:20:47.668+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-11T08:20:47.668+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO ResourceUtils: ==============================================================
[2024-11-11T08:20:47.669+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SparkContext: Submitted application: KafkaSparkStreaming
[2024-11-11T08:20:47.700+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-11T08:20:47.712+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO ResourceProfile: Limiting resource is cpu
[2024-11-11T08:20:47.712+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-11T08:20:47.812+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SecurityManager: Changing view acls to: ***
[2024-11-11T08:20:47.812+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SecurityManager: Changing modify acls to: ***
[2024-11-11T08:20:47.813+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SecurityManager: Changing view acls groups to:
[2024-11-11T08:20:47.813+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SecurityManager: Changing modify acls groups to:
[2024-11-11T08:20:47.814+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-11T08:20:48.169+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Successfully started service 'sparkDriver' on port 36423.
[2024-11-11T08:20:48.269+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkEnv: Registering MapOutputTracker
[2024-11-11T08:20:48.331+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-11T08:20:48.361+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-11T08:20:48.363+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-11T08:20:48.368+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-11T08:20:48.428+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e22a4304-02dd-4495-8680-887f875a1177
[2024-11-11T08:20:48.453+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-11T08:20:48.474+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-11T08:20:48.659+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-11T08:20:48.747+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-11T08:20:48.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://4456ca8d9d4c:36423/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://4456ca8d9d4c:36423/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-client-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://4456ca8d9d4c:36423/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.791+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://4456ca8d9d4c:36423/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.791+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://4456ca8d9d4c:36423/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.791+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://4456ca8d9d4c:36423/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://4456ca8d9d4c:36423/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://4456ca8d9d4c:36423/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-common-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-common-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.796+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.guava_guava-27.0-jre.jar at spark://4456ca8d9d4c:36423/jars/com.google.guava_guava-27.0-jre.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-cli_commons-cli-1.2.jar at spark://4456ca8d9d4c:36423/jars/commons-cli_commons-cli-1.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.commons_commons-math3-3.1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar at spark://4456ca8d9d4c:36423/jars/org.apache.httpcomponents_httpclient-4.5.6.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.11.jar at spark://4456ca8d9d4c:36423/jars/commons-codec_commons-codec-1.11.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-io_commons-io-2.5.jar at spark://4456ca8d9d4c:36423/jars/commons-io_commons-io-2.5.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-net_commons-net-3.6.jar at spark://4456ca8d9d4c:36423/jars/commons-net_commons-net-3.6.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar at spark://4456ca8d9d4c:36423/jars/commons-collections_commons-collections-3.2.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:48.798+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:48.802+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.sun.jersey_jersey-servlet-1.19.jar at spark://4456ca8d9d4c:36423/jars/com.sun.jersey_jersey-servlet-1.19.jar with timestamp 1731313247607
[2024-11-11T08:20:48.802+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://4456ca8d9d4c:36423/jars/log4j_log4j-1.2.17.jar with timestamp 1731313247607
[2024-11-11T08:20:48.802+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar at spark://4456ca8d9d4c:36423/jars/commons-beanutils_commons-beanutils-1.9.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.802+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-configuration2-2.1.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.commons_commons-configuration2-2.1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.802+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.7.jar at spark://4456ca8d9d4c:36423/jars/org.apache.commons_commons-lang3-3.7.jar with timestamp 1731313247607
[2024-11-11T08:20:48.802+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-text-1.4.jar at spark://4456ca8d9d4c:36423/jars/org.apache.commons_commons-text-1.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.avro_avro-1.7.7.jar at spark://4456ca8d9d4c:36423/jars/org.apache.avro_avro-1.7.7.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.re2j_re2j-1.1.jar at spark://4456ca8d9d4c:36423/jars/com.google.re2j_re2j-1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar at spark://4456ca8d9d4c:36423/jars/com.google.protobuf_protobuf-java-2.5.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar at spark://4456ca8d9d4c:36423/jars/com.google.code.gson_gson-2.2.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.curator_curator-client-2.13.0.jar at spark://4456ca8d9d4c:36423/jars/org.apache.curator_curator-client-2.13.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.curator_curator-recipes-2.13.0.jar at spark://4456ca8d9d4c:36423/jars/org.apache.curator_curator-recipes-2.13.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at spark://4456ca8d9d4c:36423/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-compress-1.18.jar at spark://4456ca8d9d4c:36423/jars/org.apache.commons_commons-compress-1.18.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:48.803+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar at spark://4456ca8d9d4c:36423/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/dnsjava_dnsjava-2.1.7.jar at spark://4456ca8d9d4c:36423/jars/dnsjava_dnsjava-2.1.7.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.guava_failureaccess-1.0.jar at spark://4456ca8d9d4c:36423/jars/com.google.guava_failureaccess-1.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar at spark://4456ca8d9d4c:36423/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.checkerframework_checker-qual-2.5.2.jar at spark://4456ca8d9d4c:36423/jars/org.checkerframework_checker-qual-2.5.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar at spark://4456ca8d9d4c:36423/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.1.jar at spark://4456ca8d9d4c:36423/jars/com.google.j2objc_j2objc-annotations-1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar at spark://4456ca8d9d4c:36423/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar at spark://4456ca8d9d4c:36423/jars/org.apache.httpcomponents_httpcore-4.4.10.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://4456ca8d9d4c:36423/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://4456ca8d9d4c:36423/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1731313247607
[2024-11-11T08:20:48.804+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar at spark://4456ca8d9d4c:36423/jars/com.thoughtworks.paranamer_paranamer-2.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar at spark://4456ca8d9d4c:36423/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/net.minidev_json-smart-2.3.jar at spark://4456ca8d9d4c:36423/jars/net.minidev_json-smart-2.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.curator_curator-framework-2.13.0.jar at spark://4456ca8d9d4c:36423/jars/org.apache.curator_curator-framework-2.13.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://4456ca8d9d4c:36423/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/net.minidev_accessors-smart-1.2.jar at spark://4456ca8d9d4c:36423/jars/net.minidev_accessors-smart-1.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.ow2.asm_asm-5.0.4.jar at spark://4456ca8d9d4c:36423/jars/org.ow2.asm_asm-5.0.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-client-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-client-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-admin-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-admin-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-config-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerby-config-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-core-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-core-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-common-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-common-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-util-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-util-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.805+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_token-provider-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_token-provider-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-pkix-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerby-pkix-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-asn1-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerby-asn1-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-util-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerby-util-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-crypto-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-crypto-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-server-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-server-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerby-xdr-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerby-xdr-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kerby_kerb-identity-1.0.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.kerby_kerb-identity-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar at spark://4456ca8d9d4c:36423/jars/javax.servlet.jsp_jsp-api-2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.squareup.okhttp_okhttp-2.7.5.jar at spark://4456ca8d9d4c:36423/jars/com.squareup.okhttp_okhttp-2.7.5.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.squareup.okio_okio-1.6.0.jar at spark://4456ca8d9d4c:36423/jars/com.squareup.okio_okio-1.6.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar at spark://4456ca8d9d4c:36423/jars/javax.xml.bind_jaxb-api-2.2.11.jar with timestamp 1731313247607
[2024-11-11T08:20:48.806+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.servlet_javax.servlet-api-3.1.0.jar at spark://4456ca8d9d4c:36423/jars/javax.servlet_javax.servlet-api-3.1.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.sun.jersey_jersey-core-1.19.jar at spark://4456ca8d9d4c:36423/jars/com.sun.jersey_jersey-core-1.19.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.sun.jersey_jersey-client-1.19.jar at spark://4456ca8d9d4c:36423/jars/com.sun.jersey_jersey-client-1.19.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.ws.rs_jsr311-api-1.1.1.jar at spark://4456ca8d9d4c:36423/jars/javax.ws.rs_jsr311-api-1.1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.807+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar at spark://4456ca8d9d4c:36423/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:48.808+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar at spark://4456ca8d9d4c:36423/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.808+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://4456ca8d9d4c:36423/files/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.808+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar
[2024-11-11T08:20:48.831+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://4456ca8d9d4c:36423/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.831+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar
[2024-11-11T08:20:48.837+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-client-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:48.837+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-client-3.2.1.jar
[2024-11-11T08:20:48.841+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://4456ca8d9d4c:36423/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.842+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar
[2024-11-11T08:20:48.848+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://4456ca8d9d4c:36423/files/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1731313247607
[2024-11-11T08:20:48.848+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kafka_kafka-clients-3.3.2.jar
[2024-11-11T08:20:48.855+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.856+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-11-11T08:20:48.916+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://4456ca8d9d4c:36423/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1731313247607
[2024-11-11T08:20:48.929+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.lz4_lz4-java-1.8.0.jar
[2024-11-11T08:20:48.938+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://4456ca8d9d4c:36423/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1731313247607
[2024-11-11T08:20:48.941+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-11-11T08:20:48.963+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://4456ca8d9d4c:36423/files/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1731313247607
[2024-11-11T08:20:48.964+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.slf4j_slf4j-api-2.0.6.jar
[2024-11-11T08:20:48.983+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1731313247607
[2024-11-11T08:20:48.984+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:48 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-11-11T08:20:49.072+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://4456ca8d9d4c:36423/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1731313247607
[2024-11-11T08:20:49.073+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-logging_commons-logging-1.1.3.jar
[2024-11-11T08:20:49.090+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://4456ca8d9d4c:36423/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1731313247607
[2024-11-11T08:20:49.091+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-11-11T08:20:49.119+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.121+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.commons_commons-pool2-2.11.1.jar
[2024-11-11T08:20:49.147+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-common-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-common-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.154+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-common-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-common-3.2.1.jar
[2024-11-11T08:20:49.188+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.188+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-hdfs-client-3.2.1.jar
[2024-11-11T08:20:49.231+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.231+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-yarn-api-3.2.1.jar
[2024-11-11T08:20:49.257+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.260+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-yarn-client-3.2.1.jar
[2024-11-11T08:20:49.275+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.276+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-mapreduce-client-core-3.2.1.jar
[2024-11-11T08:20:49.300+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.301+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-mapreduce-client-jobclient-3.2.1.jar
[2024-11-11T08:20:49.321+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-annotations-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.322+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-annotations-3.2.1.jar
[2024-11-11T08:20:49.335+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.guava_guava-27.0-jre.jar at spark://4456ca8d9d4c:36423/files/com.google.guava_guava-27.0-jre.jar with timestamp 1731313247607
[2024-11-11T08:20:49.336+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.google.guava_guava-27.0-jre.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.guava_guava-27.0-jre.jar
[2024-11-11T08:20:49.392+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-cli_commons-cli-1.2.jar at spark://4456ca8d9d4c:36423/files/commons-cli_commons-cli-1.2.jar with timestamp 1731313247607
[2024-11-11T08:20:49.393+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-cli_commons-cli-1.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-cli_commons-cli-1.2.jar
[2024-11-11T08:20:49.421+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.commons_commons-math3-3.1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.421+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-math3-3.1.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.commons_commons-math3-3.1.1.jar
[2024-11-11T08:20:49.468+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar at spark://4456ca8d9d4c:36423/files/org.apache.httpcomponents_httpclient-4.5.6.jar with timestamp 1731313247607
[2024-11-11T08:20:49.470+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.httpcomponents_httpclient-4.5.6.jar
[2024-11-11T08:20:49.486+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.11.jar at spark://4456ca8d9d4c:36423/files/commons-codec_commons-codec-1.11.jar with timestamp 1731313247607
[2024-11-11T08:20:49.487+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-codec_commons-codec-1.11.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-codec_commons-codec-1.11.jar
[2024-11-11T08:20:49.508+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-io_commons-io-2.5.jar at spark://4456ca8d9d4c:36423/files/commons-io_commons-io-2.5.jar with timestamp 1731313247607
[2024-11-11T08:20:49.509+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-io_commons-io-2.5.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-io_commons-io-2.5.jar
[2024-11-11T08:20:49.522+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-net_commons-net-3.6.jar at spark://4456ca8d9d4c:36423/files/commons-net_commons-net-3.6.jar with timestamp 1731313247607
[2024-11-11T08:20:49.523+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-net_commons-net-3.6.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-net_commons-net-3.6.jar
[2024-11-11T08:20:49.533+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar at spark://4456ca8d9d4c:36423/files/commons-collections_commons-collections-3.2.2.jar with timestamp 1731313247607
[2024-11-11T08:20:49.534+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-collections_commons-collections-3.2.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-collections_commons-collections-3.2.2.jar
[2024-11-11T08:20:49.550+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/files/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:49.551+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.eclipse.jetty_jetty-servlet-9.3.24.v20180605.jar
[2024-11-11T08:20:49.559+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/files/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:49.561+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.eclipse.jetty_jetty-webapp-9.3.24.v20180605.jar
[2024-11-11T08:20:49.576+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.sun.jersey_jersey-servlet-1.19.jar at spark://4456ca8d9d4c:36423/files/com.sun.jersey_jersey-servlet-1.19.jar with timestamp 1731313247607
[2024-11-11T08:20:49.577+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.sun.jersey_jersey-servlet-1.19.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.sun.jersey_jersey-servlet-1.19.jar
[2024-11-11T08:20:49.589+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://4456ca8d9d4c:36423/files/log4j_log4j-1.2.17.jar with timestamp 1731313247607
[2024-11-11T08:20:49.590+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/log4j_log4j-1.2.17.jar
[2024-11-11T08:20:49.607+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar at spark://4456ca8d9d4c:36423/files/commons-beanutils_commons-beanutils-1.9.3.jar with timestamp 1731313247607
[2024-11-11T08:20:49.607+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/commons-beanutils_commons-beanutils-1.9.3.jar
[2024-11-11T08:20:49.624+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-configuration2-2.1.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.commons_commons-configuration2-2.1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.629+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-configuration2-2.1.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.commons_commons-configuration2-2.1.1.jar
[2024-11-11T08:20:49.643+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.7.jar at spark://4456ca8d9d4c:36423/files/org.apache.commons_commons-lang3-3.7.jar with timestamp 1731313247607
[2024-11-11T08:20:49.644+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.7.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.commons_commons-lang3-3.7.jar
[2024-11-11T08:20:49.667+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-text-1.4.jar at spark://4456ca8d9d4c:36423/files/org.apache.commons_commons-text-1.4.jar with timestamp 1731313247607
[2024-11-11T08:20:49.667+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-text-1.4.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.commons_commons-text-1.4.jar
[2024-11-11T08:20:49.681+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.avro_avro-1.7.7.jar at spark://4456ca8d9d4c:36423/files/org.apache.avro_avro-1.7.7.jar with timestamp 1731313247607
[2024-11-11T08:20:49.686+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.avro_avro-1.7.7.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.avro_avro-1.7.7.jar
[2024-11-11T08:20:49.704+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.re2j_re2j-1.1.jar at spark://4456ca8d9d4c:36423/files/com.google.re2j_re2j-1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.707+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.google.re2j_re2j-1.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.re2j_re2j-1.1.jar
[2024-11-11T08:20:49.724+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar at spark://4456ca8d9d4c:36423/files/com.google.protobuf_protobuf-java-2.5.0.jar with timestamp 1731313247607
[2024-11-11T08:20:49.726+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.protobuf_protobuf-java-2.5.0.jar
[2024-11-11T08:20:49.740+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar at spark://4456ca8d9d4c:36423/files/com.google.code.gson_gson-2.2.4.jar with timestamp 1731313247607
[2024-11-11T08:20:49.741+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.gson_gson-2.2.4.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.code.gson_gson-2.2.4.jar
[2024-11-11T08:20:49.757+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-auth-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.761+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-auth-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-auth-3.2.1.jar
[2024-11-11T08:20:49.773+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.curator_curator-client-2.13.0.jar at spark://4456ca8d9d4c:36423/files/org.apache.curator_curator-client-2.13.0.jar with timestamp 1731313247607
[2024-11-11T08:20:49.774+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.curator_curator-client-2.13.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.curator_curator-client-2.13.0.jar
[2024-11-11T08:20:49.797+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.curator_curator-recipes-2.13.0.jar at spark://4456ca8d9d4c:36423/files/org.apache.curator_curator-recipes-2.13.0.jar with timestamp 1731313247607
[2024-11-11T08:20:49.798+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.curator_curator-recipes-2.13.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.curator_curator-recipes-2.13.0.jar
[2024-11-11T08:20:49.821+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at spark://4456ca8d9d4c:36423/files/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1731313247607
[2024-11-11T08:20:49.822+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.htrace_htrace-core4-4.1.0-incubating.jar
[2024-11-11T08:20:49.855+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-compress-1.18.jar at spark://4456ca8d9d4c:36423/files/org.apache.commons_commons-compress-1.18.jar with timestamp 1731313247607
[2024-11-11T08:20:49.856+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-compress-1.18.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.commons_commons-compress-1.18.jar
[2024-11-11T08:20:49.883+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-simplekdc-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:49.885+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-simplekdc-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-simplekdc-1.0.1.jar
[2024-11-11T08:20:49.901+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:49.903+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.jackson.core_jackson-databind-2.9.8.jar
[2024-11-11T08:20:49.924+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar at spark://4456ca8d9d4c:36423/files/org.codehaus.woodstox_stax2-api-3.1.4.jar with timestamp 1731313247607
[2024-11-11T08:20:49.932+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.woodstox_stax2-api-3.1.4.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.codehaus.woodstox_stax2-api-3.1.4.jar
[2024-11-11T08:20:49.956+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.woodstox_woodstox-core-5.0.3.jar with timestamp 1731313247607
[2024-11-11T08:20:49.963+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.woodstox_woodstox-core-5.0.3.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.woodstox_woodstox-core-5.0.3.jar
[2024-11-11T08:20:49.992+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/dnsjava_dnsjava-2.1.7.jar at spark://4456ca8d9d4c:36423/files/dnsjava_dnsjava-2.1.7.jar with timestamp 1731313247607
[2024-11-11T08:20:49.995+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:49 INFO Utils: Copying /home/***/.ivy2/jars/dnsjava_dnsjava-2.1.7.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/dnsjava_dnsjava-2.1.7.jar
[2024-11-11T08:20:50.019+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.guava_failureaccess-1.0.jar at spark://4456ca8d9d4c:36423/files/com.google.guava_failureaccess-1.0.jar with timestamp 1731313247607
[2024-11-11T08:20:50.021+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.google.guava_failureaccess-1.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.guava_failureaccess-1.0.jar
[2024-11-11T08:20:50.042+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar at spark://4456ca8d9d4c:36423/files/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar with timestamp 1731313247607
[2024-11-11T08:20:50.043+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar
[2024-11-11T08:20:50.057+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.checkerframework_checker-qual-2.5.2.jar at spark://4456ca8d9d4c:36423/files/org.checkerframework_checker-qual-2.5.2.jar with timestamp 1731313247607
[2024-11-11T08:20:50.060+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.checkerframework_checker-qual-2.5.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.checkerframework_checker-qual-2.5.2.jar
[2024-11-11T08:20:50.074+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar at spark://4456ca8d9d4c:36423/files/com.google.errorprone_error_prone_annotations-2.2.0.jar with timestamp 1731313247607
[2024-11-11T08:20:50.075+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.2.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.errorprone_error_prone_annotations-2.2.0.jar
[2024-11-11T08:20:50.086+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.1.jar at spark://4456ca8d9d4c:36423/files/com.google.j2objc_j2objc-annotations-1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.086+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.google.j2objc_j2objc-annotations-1.1.jar
[2024-11-11T08:20:50.095+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar at spark://4456ca8d9d4c:36423/files/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar with timestamp 1731313247607
[2024-11-11T08:20:50.096+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.codehaus.mojo_animal-sniffer-annotations-1.17.jar
[2024-11-11T08:20:50.103+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar at spark://4456ca8d9d4c:36423/files/org.apache.httpcomponents_httpcore-4.4.10.jar with timestamp 1731313247607
[2024-11-11T08:20:50.106+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.httpcomponents_httpcore-4.4.10.jar
[2024-11-11T08:20:50.123+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/files/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:50.124+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.eclipse.jetty_jetty-security-9.3.24.v20180605.jar
[2024-11-11T08:20:50.136+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/files/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:50.138+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.eclipse.jetty_jetty-xml-9.3.24.v20180605.jar
[2024-11-11T08:20:50.155+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://4456ca8d9d4c:36423/files/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1731313247607
[2024-11-11T08:20:50.157+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.codehaus.jackson_jackson-core-asl-1.9.13.jar
[2024-11-11T08:20:50.178+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://4456ca8d9d4c:36423/files/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1731313247607
[2024-11-11T08:20:50.179+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar
[2024-11-11T08:20:50.189+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar at spark://4456ca8d9d4c:36423/files/com.thoughtworks.paranamer_paranamer-2.3.jar with timestamp 1731313247607
[2024-11-11T08:20:50.189+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.thoughtworks.paranamer_paranamer-2.3.jar
[2024-11-11T08:20:50.208+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar at spark://4456ca8d9d4c:36423/files/com.nimbusds_nimbus-jose-jwt-4.41.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.209+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.nimbusds_nimbus-jose-jwt-4.41.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.nimbusds_nimbus-jose-jwt-4.41.1.jar
[2024-11-11T08:20:50.221+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/net.minidev_json-smart-2.3.jar at spark://4456ca8d9d4c:36423/files/net.minidev_json-smart-2.3.jar with timestamp 1731313247607
[2024-11-11T08:20:50.223+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/net.minidev_json-smart-2.3.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/net.minidev_json-smart-2.3.jar
[2024-11-11T08:20:50.241+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.curator_curator-framework-2.13.0.jar at spark://4456ca8d9d4c:36423/files/org.apache.curator_curator-framework-2.13.0.jar with timestamp 1731313247607
[2024-11-11T08:20:50.243+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.curator_curator-framework-2.13.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.curator_curator-framework-2.13.0.jar
[2024-11-11T08:20:50.256+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://4456ca8d9d4c:36423/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.258+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-11-11T08:20:50.272+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/net.minidev_accessors-smart-1.2.jar at spark://4456ca8d9d4c:36423/files/net.minidev_accessors-smart-1.2.jar with timestamp 1731313247607
[2024-11-11T08:20:50.273+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/net.minidev_accessors-smart-1.2.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/net.minidev_accessors-smart-1.2.jar
[2024-11-11T08:20:50.284+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.ow2.asm_asm-5.0.4.jar at spark://4456ca8d9d4c:36423/files/org.ow2.asm_asm-5.0.4.jar with timestamp 1731313247607
[2024-11-11T08:20:50.285+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.ow2.asm_asm-5.0.4.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.ow2.asm_asm-5.0.4.jar
[2024-11-11T08:20:50.306+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-client-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-client-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.308+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-client-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-client-1.0.1.jar
[2024-11-11T08:20:50.328+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-admin-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-admin-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.329+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-admin-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-admin-1.0.1.jar
[2024-11-11T08:20:50.344+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-config-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerby-config-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.345+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-config-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerby-config-1.0.1.jar
[2024-11-11T08:20:50.361+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-core-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-core-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.362+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-core-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-core-1.0.1.jar
[2024-11-11T08:20:50.375+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-common-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-common-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.376+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-common-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-common-1.0.1.jar
[2024-11-11T08:20:50.390+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-util-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-util-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.390+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-util-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-util-1.0.1.jar
[2024-11-11T08:20:50.401+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_token-provider-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_token-provider-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.402+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_token-provider-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_token-provider-1.0.1.jar
[2024-11-11T08:20:50.419+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-pkix-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerby-pkix-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.419+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-pkix-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerby-pkix-1.0.1.jar
[2024-11-11T08:20:50.433+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-asn1-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerby-asn1-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.434+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-asn1-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerby-asn1-1.0.1.jar
[2024-11-11T08:20:50.447+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-util-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerby-util-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.449+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-util-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerby-util-1.0.1.jar
[2024-11-11T08:20:50.463+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-crypto-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-crypto-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.464+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-crypto-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-crypto-1.0.1.jar
[2024-11-11T08:20:50.478+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-server-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-server-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.479+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-server-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-server-1.0.1.jar
[2024-11-11T08:20:50.490+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerby-xdr-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerby-xdr-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.491+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerby-xdr-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerby-xdr-1.0.1.jar
[2024-11-11T08:20:50.506+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kerby_kerb-identity-1.0.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.kerby_kerb-identity-1.0.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.507+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kerby_kerb-identity-1.0.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.kerby_kerb-identity-1.0.1.jar
[2024-11-11T08:20:50.519+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:50.520+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.jackson.core_jackson-annotations-2.9.8.jar
[2024-11-11T08:20:50.541+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.jackson.core_jackson-core-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:50.542+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.9.8.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.jackson.core_jackson-core-2.9.8.jar
[2024-11-11T08:20:50.555+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar at spark://4456ca8d9d4c:36423/files/javax.servlet.jsp_jsp-api-2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.556+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/javax.servlet.jsp_jsp-api-2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/javax.servlet.jsp_jsp-api-2.1.jar
[2024-11-11T08:20:50.566+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.squareup.okhttp_okhttp-2.7.5.jar at spark://4456ca8d9d4c:36423/files/com.squareup.okhttp_okhttp-2.7.5.jar with timestamp 1731313247607
[2024-11-11T08:20:50.567+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.squareup.okhttp_okhttp-2.7.5.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.squareup.okhttp_okhttp-2.7.5.jar
[2024-11-11T08:20:50.586+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.squareup.okio_okio-1.6.0.jar at spark://4456ca8d9d4c:36423/files/com.squareup.okio_okio-1.6.0.jar with timestamp 1731313247607
[2024-11-11T08:20:50.587+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.squareup.okio_okio-1.6.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.squareup.okio_okio-1.6.0.jar
[2024-11-11T08:20:50.598+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar at spark://4456ca8d9d4c:36423/files/javax.xml.bind_jaxb-api-2.2.11.jar with timestamp 1731313247607
[2024-11-11T08:20:50.599+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/javax.xml.bind_jaxb-api-2.2.11.jar
[2024-11-11T08:20:50.611+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.612+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-yarn-common-3.2.1.jar
[2024-11-11T08:20:50.640+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.servlet_javax.servlet-api-3.1.0.jar at spark://4456ca8d9d4c:36423/files/javax.servlet_javax.servlet-api-3.1.0.jar with timestamp 1731313247607
[2024-11-11T08:20:50.643+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/javax.servlet_javax.servlet-api-3.1.0.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/javax.servlet_javax.servlet-api-3.1.0.jar
[2024-11-11T08:20:50.656+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar at spark://4456ca8d9d4c:36423/files/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar with timestamp 1731313247607
[2024-11-11T08:20:50.656+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.eclipse.jetty_jetty-util-9.3.24.v20180605.jar
[2024-11-11T08:20:50.676+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.sun.jersey_jersey-core-1.19.jar at spark://4456ca8d9d4c:36423/files/com.sun.jersey_jersey-core-1.19.jar with timestamp 1731313247607
[2024-11-11T08:20:50.677+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.sun.jersey_jersey-core-1.19.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.sun.jersey_jersey-core-1.19.jar
[2024-11-11T08:20:50.699+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.sun.jersey_jersey-client-1.19.jar at spark://4456ca8d9d4c:36423/files/com.sun.jersey_jersey-client-1.19.jar with timestamp 1731313247607
[2024-11-11T08:20:50.699+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.sun.jersey_jersey-client-1.19.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.sun.jersey_jersey-client-1.19.jar
[2024-11-11T08:20:50.711+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:50.711+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.jackson.module_jackson-module-jaxb-annotations-2.9.8.jar
[2024-11-11T08:20:50.720+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:50.721+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.jackson.jaxrs_jackson-jaxrs-json-provider-2.9.8.jar
[2024-11-11T08:20:50.732+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.ws.rs_jsr311-api-1.1.1.jar at spark://4456ca8d9d4c:36423/files/javax.ws.rs_jsr311-api-1.1.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.732+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/javax.ws.rs_jsr311-api-1.1.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/javax.ws.rs_jsr311-api-1.1.1.jar
[2024-11-11T08:20:50.742+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar at spark://4456ca8d9d4c:36423/files/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar with timestamp 1731313247607
[2024-11-11T08:20:50.750+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/com.fasterxml.jackson.jaxrs_jackson-jaxrs-base-2.9.8.jar
[2024-11-11T08:20:50.763+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar at spark://4456ca8d9d4c:36423/files/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar with timestamp 1731313247607
[2024-11-11T08:20:50.764+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar to /tmp/spark-d642f64a-7923-46dc-9402-ff619550b441/userFiles-9fa52cce-0282-47af-9f02-eb1638f70df6/org.apache.hadoop_hadoop-mapreduce-client-common-3.2.1.jar
[2024-11-11T08:20:51.113+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-11T08:20:51.327+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.3:7077 after 132 ms (0 ms spent in bootstraps)
[2024-11-11T08:20:51.910+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241111082051-0000
[2024-11-11T08:20:51.939+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39349.
[2024-11-11T08:20:51.940+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO NettyBlockTransferService: Server created on 4456ca8d9d4c:39349
[2024-11-11T08:20:51.945+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-11T08:20:51.987+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4456ca8d9d4c, 39349, None)
[2024-11-11T08:20:52.011+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO BlockManagerMasterEndpoint: Registering block manager 4456ca8d9d4c:39349 with 434.4 MiB RAM, BlockManagerId(driver, 4456ca8d9d4c, 39349, None)
[2024-11-11T08:20:52.031+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241111082051-0000/0 on worker-20241111075032-172.19.0.8-42007 (172.19.0.8:42007) with 1 core(s)
[2024-11-11T08:20:52.035+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4456ca8d9d4c, 39349, None)
[2024-11-11T08:20:52.039+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4456ca8d9d4c, 39349, None)
[2024-11-11T08:20:52.040+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20241111082051-0000/0 on hostPort 172.19.0.8:42007 with 1 core(s), 1024.0 MiB RAM
[2024-11-11T08:20:52.333+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241111082051-0000/0 is now RUNNING
[2024-11-11T08:20:52.390+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-11T08:20:52.833+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-11T08:20:52.835+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:52 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-11T08:20:58.351+0000] {spark_submit.py:495} INFO - done
[2024-11-11T08:20:58.500+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:58 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.8:39404) with ID 0,  ResourceProfileId 0
[2024-11-11T08:20:58.616+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.8:45299 with 434.4 MiB RAM, BlockManagerId(0, 172.19.0.8, 45299, None)
[2024-11-11T08:20:59.596+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:59 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-11-11T08:20:59.649+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:59 INFO ResolveWriteToStream: Checkpoint root hdfs://namenode:9000/spark_checkpoint resolved to hdfs://namenode:9000/spark_checkpoint.
[2024-11-11T08:20:59.650+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-11-11T08:20:59.971+0000] {spark_submit.py:495} INFO - 24/11/11 08:20:59 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/metadata using temp file hdfs://namenode:9000/spark_checkpoint/.metadata.b81068a2-4428-4eb1-a213-521e4899a152.tmp
[2024-11-11T08:21:01.384+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/.metadata.b81068a2-4428-4eb1-a213-521e4899a152.tmp to hdfs://namenode:9000/spark_checkpoint/metadata
[2024-11-11T08:21:01.609+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO MicroBatchExecution: Starting [id = 78874c98-4dee-4b92-96e3-5cfffbc9705a, runId = ce66f919-ccb7-4244-8460-a0d82da7d70b]. Use hdfs://namenode:9000/spark_checkpoint to store the query checkpoint.
[2024-11-11T08:21:01.687+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@742f0c3c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@72194e5c]
[2024-11-11T08:21:01.929+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-11T08:21:01.968+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-11T08:21:01.969+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO MicroBatchExecution: Starting new streaming query.
[2024-11-11T08:21:01.982+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:01 INFO MicroBatchExecution: Stream started from {}
[2024-11-11T08:21:03.937+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:03 INFO AdminClientConfig: AdminClientConfig values:
[2024-11-11T08:21:03.937+0000] {spark_submit.py:495} INFO - bootstrap.servers = [10.0.2.15:9092]
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - client.dns.lookup = use_all_dns_ips
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - client.id =
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - connections.max.idle.ms = 300000
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - default.api.timeout.ms = 60000
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - metadata.max.age.ms = 300000
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - metric.reporters = []
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - metrics.num.samples = 2
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - metrics.recording.level = INFO
[2024-11-11T08:21:03.938+0000] {spark_submit.py:495} INFO - metrics.sample.window.ms = 30000
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - receive.buffer.bytes = 65536
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - reconnect.backoff.max.ms = 1000
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - reconnect.backoff.ms = 50
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - request.timeout.ms = 30000
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - retries = 2147483647
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - retry.backoff.ms = 100
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.client.callback.handler.class = null
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.jaas.config = null
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.kerberos.min.time.before.relogin = 60000
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.kerberos.service.name = null
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.jitter = 0.05
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.window.factor = 0.8
[2024-11-11T08:21:03.939+0000] {spark_submit.py:495} INFO - sasl.login.callback.handler.class = null
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.class = null
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.connect.timeout.ms = null
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.read.timeout.ms = null
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.refresh.buffer.seconds = 300
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.refresh.min.period.seconds = 60
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.factor = 0.8
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.jitter = 0.05
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.max.ms = 10000
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.ms = 100
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.mechanism = GSSAPI
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.clock.skew.seconds = 30
[2024-11-11T08:21:03.940+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.audience = null
[2024-11-11T08:21:03.941+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.issuer = null
[2024-11-11T08:21:03.943+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2024-11-11T08:21:03.947+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2024-11-11T08:21:03.948+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2024-11-11T08:21:03.949+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.url = null
[2024-11-11T08:21:03.950+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.scope.claim.name = scope
[2024-11-11T08:21:03.950+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.sub.claim.name = sub
[2024-11-11T08:21:03.951+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.token.endpoint.url = null
[2024-11-11T08:21:03.951+0000] {spark_submit.py:495} INFO - security.protocol = PLAINTEXT
[2024-11-11T08:21:03.953+0000] {spark_submit.py:495} INFO - security.providers = null
[2024-11-11T08:21:03.953+0000] {spark_submit.py:495} INFO - send.buffer.bytes = 131072
[2024-11-11T08:21:03.954+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.max.ms = 30000
[2024-11-11T08:21:03.954+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.ms = 10000
[2024-11-11T08:21:03.955+0000] {spark_submit.py:495} INFO - ssl.cipher.suites = null
[2024-11-11T08:21:03.955+0000] {spark_submit.py:495} INFO - ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2024-11-11T08:21:03.955+0000] {spark_submit.py:495} INFO - ssl.endpoint.identification.algorithm = https
[2024-11-11T08:21:03.956+0000] {spark_submit.py:495} INFO - ssl.engine.factory.class = null
[2024-11-11T08:21:03.956+0000] {spark_submit.py:495} INFO - ssl.key.password = null
[2024-11-11T08:21:03.956+0000] {spark_submit.py:495} INFO - ssl.keymanager.algorithm = SunX509
[2024-11-11T08:21:03.957+0000] {spark_submit.py:495} INFO - ssl.keystore.certificate.chain = null
[2024-11-11T08:21:03.957+0000] {spark_submit.py:495} INFO - ssl.keystore.key = null
[2024-11-11T08:21:03.957+0000] {spark_submit.py:495} INFO - ssl.keystore.location = null
[2024-11-11T08:21:03.958+0000] {spark_submit.py:495} INFO - ssl.keystore.password = null
[2024-11-11T08:21:03.958+0000] {spark_submit.py:495} INFO - ssl.keystore.type = JKS
[2024-11-11T08:21:03.959+0000] {spark_submit.py:495} INFO - ssl.protocol = TLSv1.3
[2024-11-11T08:21:03.961+0000] {spark_submit.py:495} INFO - ssl.provider = null
[2024-11-11T08:21:03.962+0000] {spark_submit.py:495} INFO - ssl.secure.random.implementation = null
[2024-11-11T08:21:03.964+0000] {spark_submit.py:495} INFO - ssl.trustmanager.algorithm = PKIX
[2024-11-11T08:21:03.964+0000] {spark_submit.py:495} INFO - ssl.truststore.certificates = null
[2024-11-11T08:21:03.964+0000] {spark_submit.py:495} INFO - ssl.truststore.location = null
[2024-11-11T08:21:03.965+0000] {spark_submit.py:495} INFO - ssl.truststore.password = null
[2024-11-11T08:21:03.966+0000] {spark_submit.py:495} INFO - ssl.truststore.type = JKS
[2024-11-11T08:21:03.968+0000] {spark_submit.py:495} INFO - 
[2024-11-11T08:21:04.163+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:04 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-11-11T08:21:04.175+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:04 INFO AppInfoParser: Kafka version: 3.3.2
[2024-11-11T08:21:04.178+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:04 INFO AppInfoParser: Kafka commitId: b66af662e61082cb
[2024-11-11T08:21:04.178+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:04 INFO AppInfoParser: Kafka startTimeMs: 1731313264163
[2024-11-11T08:21:05.039+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/sources/0/0 using temp file hdfs://namenode:9000/spark_checkpoint/sources/0/.0.09bddabe-554b-499c-bebd-48f399abb257.tmp
[2024-11-11T08:21:05.166+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/sources/0/.0.09bddabe-554b-499c-bebd-48f399abb257.tmp to hdfs://namenode:9000/spark_checkpoint/sources/0/0
[2024-11-11T08:21:05.167+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:05 INFO KafkaMicroBatchStream: Initial offsets: {"raw_data":{"0":545}}
[2024-11-11T08:21:05.208+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:05 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/0 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.0.ad551ff4-35ea-45dc-a438-8a83aac01ecb.tmp
[2024-11-11T08:21:05.259+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:05 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.0.ad551ff4-35ea-45dc-a438-8a83aac01ecb.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/0
[2024-11-11T08:21:05.261+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:05 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1731313265190,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:06.257+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:06.362+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:06.455+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:06.458+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:06.494+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:06 INFO FileStreamSinkLog: BatchIds found from listing:
[2024-11-11T08:21:06.572+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:06 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:07.039+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO CodeGenerator: Code generated in 214.235895 ms
[2024-11-11T08:21:07.231+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:07.275+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:07.276+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:07.276+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:07.277+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:07.286+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:07.477+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.3 KiB, free 434.1 MiB)
[2024-11-11T08:21:07.624+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 96.6 KiB, free 434.0 MiB)
[2024-11-11T08:21:07.630+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4456ca8d9d4c:39349 (size: 96.6 KiB, free: 434.3 MiB)
[2024-11-11T08:21:07.644+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:07.692+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:07.697+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-11T08:21:07.844+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 7388 bytes)
[2024-11-11T08:21:08.735+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.0.8:45299 (size: 96.6 KiB, free: 434.3 MiB)
[2024-11-11T08:21:13.112+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5313 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:13.129+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-11T08:21:13.138+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 5.802 s
[2024-11-11T08:21:13.141+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:13.141+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-11T08:21:13.146+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 5.914665 s
[2024-11-11T08:21:13.148+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO FileFormatWriter: Start to commit write Job 6939abce-307b-4983-8557-f40e15ff00bb.
[2024-11-11T08:21:13.154+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
[2024-11-11T08:21:13.161+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/0 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.0.278f53c2-3c99-441a-be08-9d497f4abf31.tmp
[2024-11-11T08:21:13.219+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.0.278f53c2-3c99-441a-be08-9d497f4abf31.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/0
[2024-11-11T08:21:13.221+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO ManifestFileCommitProtocol: Committed batch 0
[2024-11-11T08:21:13.221+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO FileFormatWriter: Write Job 6939abce-307b-4983-8557-f40e15ff00bb committed. Elapsed time: 71 ms.
[2024-11-11T08:21:13.226+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO FileFormatWriter: Finished processing stats for write job 6939abce-307b-4983-8557-f40e15ff00bb.
[2024-11-11T08:21:13.246+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/0 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.0.ad330ff7-98a9-44ec-8cb4-990525871c8d.tmp
[2024-11-11T08:21:13.283+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.0.ad330ff7-98a9-44ec-8cb4-990525871c8d.tmp to hdfs://namenode:9000/spark_checkpoint/commits/0
[2024-11-11T08:21:13.333+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:13.334+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:13.334+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:13.334+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:13.334+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:01.827Z",
[2024-11-11T08:21:13.334+0000] {spark_submit.py:495} INFO - "batchId" : 0,
[2024-11-11T08:21:13.334+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-11T08:21:13.335+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-11T08:21:13.335+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "addBatch" : 6824,
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "commitOffsets" : 42,
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "getBatch" : 372,
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "latestOffset" : 3184,
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "queryPlanning" : 756,
[2024-11-11T08:21:13.336+0000] {spark_submit.py:495} INFO - "triggerExecution" : 11454,
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "walCommit" : 67
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "startOffset" : null,
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - "0" : 545
[2024-11-11T08:21:13.337+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "0" : 545
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:13.338+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:13.339+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:13.339+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:13.339+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:13.339+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:13.339+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:13.339+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:13.352+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/1 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.1.debbd2b5-fe82-4155-9457-b0d154e3c5d5.tmp
[2024-11-11T08:21:13.389+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.1.debbd2b5-fe82-4155-9457-b0d154e3c5d5.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/1
[2024-11-11T08:21:13.390+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1731313273344,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:13.423+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:13.427+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:13.458+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:13.460+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:13.471+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO FileStreamSinkLog: BatchIds found from listing: 0, 0
[2024-11-11T08:21:13.489+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:13.532+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:13.535+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:13.536+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:13.536+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:13.537+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:13.537+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:13.615+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 320.7 KiB, free 433.7 MiB)
[2024-11-11T08:21:13.627+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 106.1 KiB, free 433.6 MiB)
[2024-11-11T08:21:13.629+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4456ca8d9d4c:39349 (size: 106.1 KiB, free: 434.2 MiB)
[2024-11-11T08:21:13.630+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:13.630+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:13.631+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-11-11T08:21:13.638+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-11T08:21:13.824+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.0.8:45299 (size: 106.1 KiB, free: 434.2 MiB)
[2024-11-11T08:21:18.009+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4375 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:18.025+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-11-11T08:21:18.025+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 4.471 s
[2024-11-11T08:21:18.026+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:18.026+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-11-11T08:21:18.026+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 4.482603 s
[2024-11-11T08:21:18.026+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO FileFormatWriter: Start to commit write Job 91fea0c2-f899-40c3-bae1-1dd6c818fb2c.
[2024-11-11T08:21:18.057+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/1 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.1.a28b54a0-4c6c-41b6-8aec-ee2560fff869.tmp
[2024-11-11T08:21:18.114+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.1.a28b54a0-4c6c-41b6-8aec-ee2560fff869.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/1
[2024-11-11T08:21:18.115+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO ManifestFileCommitProtocol: Committed batch 1
[2024-11-11T08:21:18.115+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO FileFormatWriter: Write Job 91fea0c2-f899-40c3-bae1-1dd6c818fb2c committed. Elapsed time: 96 ms.
[2024-11-11T08:21:18.115+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO FileFormatWriter: Finished processing stats for write job 91fea0c2-f899-40c3-bae1-1dd6c818fb2c.
[2024-11-11T08:21:18.123+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/1 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.1.1de80750-e7fb-42e0-a052-d57ca7d87e82.tmp
[2024-11-11T08:21:18.164+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.1.1de80750-e7fb-42e0-a052-d57ca7d87e82.tmp to hdfs://namenode:9000/spark_checkpoint/commits/1
[2024-11-11T08:21:18.167+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:18.167+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:13.333Z",
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "numInputRows" : 3,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.2607335303320007,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6209894431794659,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "addBatch" : 4676,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "commitOffsets" : 48,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-11T08:21:18.168+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-11T08:21:18.169+0000] {spark_submit.py:495} INFO - "queryPlanning" : 40,
[2024-11-11T08:21:18.169+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4831,
[2024-11-11T08:21:18.169+0000] {spark_submit.py:495} INFO - "walCommit" : 46
[2024-11-11T08:21:18.169+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:18.169+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:18.170+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:18.170+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:18.170+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-11T08:21:18.171+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:18.171+0000] {spark_submit.py:495} INFO - "0" : 545
[2024-11-11T08:21:18.171+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - "0" : 548
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:18.172+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "0" : 548
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "numInputRows" : 3,
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.2607335303320007,
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.6209894431794659,
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:18.173+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:18.174+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:18.221+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/2 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.2.92472c59-3511-4f89-94e1-89607e0c6d7f.tmp
[2024-11-11T08:21:18.267+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.2.92472c59-3511-4f89-94e1-89607e0c6d7f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/2
[2024-11-11T08:21:18.271+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1731313278200,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:18.313+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:18.316+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:18.347+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:18.350+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:18.364+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO FileStreamSinkLog: BatchIds found from listing: 0, 0, 1, 1
[2024-11-11T08:21:18.371+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:18.422+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:18.424+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:18.424+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:18.425+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:18.425+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:18.427+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:18.464+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 320.7 KiB, free 433.3 MiB)
[2024-11-11T08:21:18.476+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 106.1 KiB, free 433.2 MiB)
[2024-11-11T08:21:18.477+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4456ca8d9d4c:39349 (size: 106.1 KiB, free: 434.1 MiB)
[2024-11-11T08:21:18.479+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:18.479+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:18.479+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-11-11T08:21:18.481+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-11T08:21:18.576+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.0.8:45299 (size: 106.1 KiB, free: 434.1 MiB)
[2024-11-11T08:21:19.445+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 963 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:19.466+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-11-11T08:21:19.468+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.021 s
[2024-11-11T08:21:19.468+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:19.468+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-11-11T08:21:19.469+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.026139 s
[2024-11-11T08:21:19.469+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO FileFormatWriter: Start to commit write Job e280c649-7816-4ab4-8397-bec6c2a0e25d.
[2024-11-11T08:21:19.469+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/2 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.2.bdc472de-f61a-469a-80db-38a7ea27282d.tmp
[2024-11-11T08:21:19.507+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.2.bdc472de-f61a-469a-80db-38a7ea27282d.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/2
[2024-11-11T08:21:19.508+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO ManifestFileCommitProtocol: Committed batch 2
[2024-11-11T08:21:19.509+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO FileFormatWriter: Write Job e280c649-7816-4ab4-8397-bec6c2a0e25d committed. Elapsed time: 56 ms.
[2024-11-11T08:21:19.509+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO FileFormatWriter: Finished processing stats for write job e280c649-7816-4ab4-8397-bec6c2a0e25d.
[2024-11-11T08:21:19.514+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/2 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.2.7893b313-577d-4033-a7c6-104e91e7c014.tmp
[2024-11-11T08:21:19.552+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.2.7893b313-577d-4033-a7c6-104e91e7c014.tmp to hdfs://namenode:9000/spark_checkpoint/commits/2
[2024-11-11T08:21:19.554+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:19.555+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:19.556+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:19.556+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:19.556+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:18.167Z",
[2024-11-11T08:21:19.556+0000] {spark_submit.py:495} INFO - "batchId" : 2,
[2024-11-11T08:21:19.557+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-11T08:21:19.557+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8274720728175424,
[2024-11-11T08:21:19.557+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.888086642599278,
[2024-11-11T08:21:19.558+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:19.558+0000] {spark_submit.py:495} INFO - "addBatch" : 1179,
[2024-11-11T08:21:19.559+0000] {spark_submit.py:495} INFO - "commitOffsets" : 45,
[2024-11-11T08:21:19.559+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-11T08:21:19.560+0000] {spark_submit.py:495} INFO - "latestOffset" : 33,
[2024-11-11T08:21:19.560+0000] {spark_submit.py:495} INFO - "queryPlanning" : 47,
[2024-11-11T08:21:19.560+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1385,
[2024-11-11T08:21:19.561+0000] {spark_submit.py:495} INFO - "walCommit" : 72
[2024-11-11T08:21:19.561+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:19.561+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:19.561+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:19.562+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:19.562+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - "0" : 548
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:19.563+0000] {spark_submit.py:495} INFO - "0" : 552
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - "0" : 552
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:19.564+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-11T08:21:19.566+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8274720728175424,
[2024-11-11T08:21:19.567+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 2.888086642599278,
[2024-11-11T08:21:19.567+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:19.567+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:19.567+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:19.568+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:19.573+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/3 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.3.85a1d5d6-c41d-4924-b4a3-0c4a5f48b07f.tmp
[2024-11-11T08:21:19.615+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.3.85a1d5d6-c41d-4924-b4a3-0c4a5f48b07f.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/3
[2024-11-11T08:21:19.616+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1731313279564,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:19.637+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:19.639+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:19.670+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:19.672+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:19.678+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 1, 2, 2
[2024-11-11T08:21:19.681+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:19.711+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:19.712+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:19.712+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:19.712+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:19.712+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:19.713+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:19.728+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 320.7 KiB, free 432.9 MiB)
[2024-11-11T08:21:19.735+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 106.1 KiB, free 432.8 MiB)
[2024-11-11T08:21:19.736+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4456ca8d9d4c:39349 (size: 106.1 KiB, free: 434.0 MiB)
[2024-11-11T08:21:19.737+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:19.737+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:19.738+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-11-11T08:21:19.739+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-11T08:21:19.862+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.0.8:45299 (size: 106.1 KiB, free: 434.0 MiB)
[2024-11-11T08:21:20.840+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1101 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:20.854+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-11-11T08:21:20.855+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.127 s
[2024-11-11T08:21:20.855+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:20.855+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-11-11T08:21:20.856+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.133275 s
[2024-11-11T08:21:20.856+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO FileFormatWriter: Start to commit write Job f4862fe8-7e6c-49e0-b040-5d48e4897326.
[2024-11-11T08:21:20.871+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/3 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.3.f0b61129-dbc7-4f6e-b0a3-b129816c431c.tmp
[2024-11-11T08:21:20.925+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.3.f0b61129-dbc7-4f6e-b0a3-b129816c431c.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/3
[2024-11-11T08:21:20.926+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO ManifestFileCommitProtocol: Committed batch 3
[2024-11-11T08:21:20.926+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO FileFormatWriter: Write Job f4862fe8-7e6c-49e0-b040-5d48e4897326 committed. Elapsed time: 80 ms.
[2024-11-11T08:21:20.929+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO FileFormatWriter: Finished processing stats for write job f4862fe8-7e6c-49e0-b040-5d48e4897326.
[2024-11-11T08:21:20.939+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/3 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.3.50f6b43c-160a-4f34-b806-2446c29c9d17.tmp
[2024-11-11T08:21:20.970+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.3.50f6b43c-160a-4f34-b806-2446c29c9d17.tmp to hdfs://namenode:9000/spark_checkpoint/commits/3
[2024-11-11T08:21:20.972+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:20.972+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:20.973+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:20.973+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:20.973+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:19.554Z",
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "batchId" : 3,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.4419610670511895,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4124293785310735,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "addBatch" : 1283,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "commitOffsets" : 41,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "queryPlanning" : 22,
[2024-11-11T08:21:20.974+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1416,
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "walCommit" : 53
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:20.975+0000] {spark_submit.py:495} INFO - "0" : 552
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "0" : 554
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "0" : 554
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.4419610670511895,
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.4124293785310735,
[2024-11-11T08:21:20.976+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:20.977+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:20.978+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:20.998+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:20 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/4 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.4.7eb376ab-0090-4224-b66c-d529b4f8a5e3.tmp
[2024-11-11T08:21:21.075+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.4.7eb376ab-0090-4224-b66c-d529b4f8a5e3.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/4
[2024-11-11T08:21:21.075+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1731313280990,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:21.119+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:21.122+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:21.150+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:21.151+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:21.168+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 2, 3, 3
[2024-11-11T08:21:21.173+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:21.220+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:21.222+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:21.223+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:21.223+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:21.224+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:21.226+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:21.252+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 320.7 KiB, free 432.5 MiB)
[2024-11-11T08:21:21.263+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 106.1 KiB, free 432.4 MiB)
[2024-11-11T08:21:21.265+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4456ca8d9d4c:39349 (size: 106.1 KiB, free: 433.9 MiB)
[2024-11-11T08:21:21.268+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:21.269+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:21.270+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-11-11T08:21:21.271+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-11T08:21:21.375+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.0.8:45299 (size: 106.1 KiB, free: 433.9 MiB)
[2024-11-11T08:21:21.756+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 484 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:21.789+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-11-11T08:21:21.789+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.532 s
[2024-11-11T08:21:21.789+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:21.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-11-11T08:21:21.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.539389 s
[2024-11-11T08:21:21.790+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO FileFormatWriter: Start to commit write Job 49eb39f1-f33f-4e99-ba0e-0309105fc38d.
[2024-11-11T08:21:21.798+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/4 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.4.7f3326c3-b9d1-49f2-a60c-a12b7ded13be.tmp
[2024-11-11T08:21:21.859+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.4.7f3326c3-b9d1-49f2-a60c-a12b7ded13be.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/4
[2024-11-11T08:21:21.863+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO ManifestFileCommitProtocol: Committed batch 4
[2024-11-11T08:21:21.863+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO FileFormatWriter: Write Job 49eb39f1-f33f-4e99-ba0e-0309105fc38d committed. Elapsed time: 102 ms.
[2024-11-11T08:21:21.864+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO FileFormatWriter: Finished processing stats for write job 49eb39f1-f33f-4e99-ba0e-0309105fc38d.
[2024-11-11T08:21:21.879+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/4 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.4.cf44d2f6-7f82-4669-b71b-8ffc0719481e.tmp
[2024-11-11T08:21:21.925+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.4.cf44d2f6-7f82-4669-b71b-8ffc0719481e.tmp to hdfs://namenode:9000/spark_checkpoint/commits/4
[2024-11-11T08:21:21.928+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:21.929+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:21.929+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:21.929+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:21.929+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:20.972Z",
[2024-11-11T08:21:21.930+0000] {spark_submit.py:495} INFO - "batchId" : 4,
[2024-11-11T08:21:21.930+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-11T08:21:21.930+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7052186177715092,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0493179433368311,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "addBatch" : 735,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "commitOffsets" : 60,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "queryPlanning" : 45,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "triggerExecution" : 953,
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "walCommit" : 87
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:21.931+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - "0" : 554
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:21.932+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "0" : 555
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "0" : 555
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7052186177715092,
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.0493179433368311,
[2024-11-11T08:21:21.933+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:21.934+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:21.934+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:21.934+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:21.934+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:21.934+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:21.935+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:21.935+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:21.935+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:21.935+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:21.935+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:21.956+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:21 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/5 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.5.2c83e610-a585-451b-aeab-f31001b1fbae.tmp
[2024-11-11T08:21:22.015+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.5.2c83e610-a585-451b-aeab-f31001b1fbae.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/5
[2024-11-11T08:21:22.027+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1731313281947,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:22.052+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:22.055+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:22.079+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:22.081+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:22.089+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 3, 4, 4
[2024-11-11T08:21:22.094+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:22.136+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:22.137+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:22.137+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:22.137+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:22.137+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:22.138+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:22.156+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 320.7 KiB, free 432.0 MiB)
[2024-11-11T08:21:22.168+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 106.1 KiB, free 431.9 MiB)
[2024-11-11T08:21:22.170+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4456ca8d9d4c:39349 (size: 106.1 KiB, free: 433.8 MiB)
[2024-11-11T08:21:22.171+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:22.172+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:22.172+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-11-11T08:21:22.176+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-11T08:21:22.300+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.0.8:45299 (size: 106.1 KiB, free: 433.8 MiB)
[2024-11-11T08:21:22.856+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 678 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:22.893+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-11-11T08:21:22.895+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.715 s
[2024-11-11T08:21:22.895+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:22.895+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-11-11T08:21:22.895+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.720270 s
[2024-11-11T08:21:22.895+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO FileFormatWriter: Start to commit write Job 28863c85-d321-490f-ac4a-1b7da7bbcc99.
[2024-11-11T08:21:22.896+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/5 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.5.35adbbce-821f-4da7-b38f-23524924f084.tmp
[2024-11-11T08:21:22.952+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.5.35adbbce-821f-4da7-b38f-23524924f084.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/5
[2024-11-11T08:21:22.954+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO ManifestFileCommitProtocol: Committed batch 5
[2024-11-11T08:21:22.954+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO FileFormatWriter: Write Job 28863c85-d321-490f-ac4a-1b7da7bbcc99 committed. Elapsed time: 97 ms.
[2024-11-11T08:21:22.955+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO FileFormatWriter: Finished processing stats for write job 28863c85-d321-490f-ac4a-1b7da7bbcc99.
[2024-11-11T08:21:22.967+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:22 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/5 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.5.9250a654-8819-4694-8c63-4c4b8493c492.tmp
[2024-11-11T08:21:23.029+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.5.9250a654-8819-4694-8c63-4c4b8493c492.tmp to hdfs://namenode:9000/spark_checkpoint/commits/5
[2024-11-11T08:21:23.043+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:21.928Z",
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "batchId" : 5,
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-11T08:21:23.044+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0460251046025104,
[2024-11-11T08:21:23.045+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9074410163339383,
[2024-11-11T08:21:23.046+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:23.046+0000] {spark_submit.py:495} INFO - "addBatch" : 894,
[2024-11-11T08:21:23.047+0000] {spark_submit.py:495} INFO - "commitOffsets" : 73,
[2024-11-11T08:21:23.047+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-11T08:21:23.048+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-11T08:21:23.048+0000] {spark_submit.py:495} INFO - "queryPlanning" : 39,
[2024-11-11T08:21:23.048+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1100,
[2024-11-11T08:21:23.048+0000] {spark_submit.py:495} INFO - "walCommit" : 70
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "0" : 555
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - "0" : 556
[2024-11-11T08:21:23.049+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "0" : 556
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0460251046025104,
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.9074410163339383,
[2024-11-11T08:21:23.050+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:23.051+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:23.122+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/6 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.6.84f52a0b-28bf-4ac6-a1b1-096b649fa85d.tmp
[2024-11-11T08:21:23.237+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/offsets/.6.84f52a0b-28bf-4ac6-a1b1-096b649fa85d.tmp to hdfs://namenode:9000/spark_checkpoint/offsets/6
[2024-11-11T08:21:23.254+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1731313283082,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-11T08:21:23.307+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:23.309+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:23.346+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:23.350+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-11T08:21:23.394+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 4, 5, 5
[2024-11-11T08:21:23.407+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-11T08:21:23.528+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-11T08:21:23.546+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-11T08:21:23.546+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
[2024-11-11T08:21:23.546+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO DAGScheduler: Parents of final stage: List()
[2024-11-11T08:21:23.546+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO DAGScheduler: Missing parents: List()
[2024-11-11T08:21:23.546+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[28] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-11T08:21:23.555+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 320.7 KiB, free 431.6 MiB)
[2024-11-11T08:21:23.566+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 106.1 KiB, free 431.5 MiB)
[2024-11-11T08:21:23.638+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4456ca8d9d4c:39349 (size: 106.1 KiB, free: 433.7 MiB)
[2024-11-11T08:21:23.649+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2024-11-11T08:21:23.650+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-11T08:21:23.651+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-11-11T08:21:23.658+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.19.0.8, executor 0, partition 0, PROCESS_LOCAL, 8332 bytes)
[2024-11-11T08:21:23.691+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4456ca8d9d4c:39349 in memory (size: 106.1 KiB, free: 433.8 MiB)
[2024-11-11T08:21:23.866+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.0.8:45299 in memory (size: 106.1 KiB, free: 433.9 MiB)
[2024-11-11T08:21:23.911+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.0.8:45299 (size: 106.1 KiB, free: 433.8 MiB)
[2024-11-11T08:21:23.933+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4456ca8d9d4c:39349 in memory (size: 106.1 KiB, free: 433.9 MiB)
[2024-11-11T08:21:23.942+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.19.0.8:45299 in memory (size: 106.1 KiB, free: 433.9 MiB)
[2024-11-11T08:21:23.976+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4456ca8d9d4c:39349 in memory (size: 106.1 KiB, free: 434.0 MiB)
[2024-11-11T08:21:23.981+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.0.8:45299 in memory (size: 106.1 KiB, free: 434.0 MiB)
[2024-11-11T08:21:24.015+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 4456ca8d9d4c:39349 in memory (size: 106.1 KiB, free: 434.1 MiB)
[2024-11-11T08:21:24.038+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.0.8:45299 in memory (size: 106.1 KiB, free: 434.1 MiB)
[2024-11-11T08:21:24.055+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 4456ca8d9d4c:39349 in memory (size: 106.1 KiB, free: 434.2 MiB)
[2024-11-11T08:21:24.477+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.19.0.8:45299 in memory (size: 106.1 KiB, free: 434.2 MiB)
[2024-11-11T08:21:27.220+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 3563 ms on 172.19.0.8 (executor 0) (1/1)
[2024-11-11T08:21:27.272+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-11-11T08:21:27.273+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 3.689 s
[2024-11-11T08:21:27.273+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-11T08:21:27.273+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-11-11T08:21:27.273+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 3.709440 s
[2024-11-11T08:21:27.273+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO FileFormatWriter: Start to commit write Job 781569b1-d87a-4e94-9ffc-51356f7318f3.
[2024-11-11T08:21:27.630+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:27 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/raw_data/_spark_metadata/6 using temp file hdfs://namenode:9000/raw_data/_spark_metadata/.6.ea30dcc1-47e5-446c-959b-7f6c1cea8279.tmp
[2024-11-11T08:21:28.471+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:28 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/raw_data/_spark_metadata/.6.ea30dcc1-47e5-446c-959b-7f6c1cea8279.tmp to hdfs://namenode:9000/raw_data/_spark_metadata/6
[2024-11-11T08:21:28.519+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:28 INFO ManifestFileCommitProtocol: Committed batch 6
[2024-11-11T08:21:28.519+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:28 INFO FileFormatWriter: Write Job 781569b1-d87a-4e94-9ffc-51356f7318f3 committed. Elapsed time: 1250 ms.
[2024-11-11T08:21:28.519+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:28 INFO FileFormatWriter: Finished processing stats for write job 781569b1-d87a-4e94-9ffc-51356f7318f3.
[2024-11-11T08:21:29.142+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:29 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/commits/6 using temp file hdfs://namenode:9000/spark_checkpoint/commits/.6.9937a6f7-5c5f-4da7-a2ba-7e9e0487dcd9.tmp
[2024-11-11T08:21:31.021+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:31 INFO CheckpointFileManager: Renamed temp file hdfs://namenode:9000/spark_checkpoint/commits/.6.9937a6f7-5c5f-4da7-a2ba-7e9e0487dcd9.tmp to hdfs://namenode:9000/spark_checkpoint/commits/6
[2024-11-11T08:21:31.123+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:31 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-11T08:21:31.172+0000] {spark_submit.py:495} INFO - "id" : "78874c98-4dee-4b92-96e3-5cfffbc9705a",
[2024-11-11T08:21:31.175+0000] {spark_submit.py:495} INFO - "runId" : "ce66f919-ccb7-4244-8460-a0d82da7d70b",
[2024-11-11T08:21:31.175+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-11T08:21:31.176+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-11T08:21:23.042Z",
[2024-11-11T08:21:31.176+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-11T08:21:31.176+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-11T08:21:31.176+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8976660682226211,
[2024-11-11T08:21:31.176+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.12501562695336918,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "addBatch" : 5192,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "commitOffsets" : 2503,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "latestOffset" : 40,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "queryPlanning" : 76,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7996,
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "walCommit" : 157
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-11T08:21:31.177+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-11T08:21:31.178+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-11T08:21:31.178+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-11T08:21:31.179+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:31.180+0000] {spark_submit.py:495} INFO - "0" : 556
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - "0" : 557
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - "0" : 557
[2024-11-11T08:21:31.181+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - },
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8976660682226211,
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.12501562695336918,
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-11T08:21:31.182+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:31.183+0000] {spark_submit.py:495} INFO - } ],
[2024-11-11T08:21:31.183+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-11T08:21:31.185+0000] {spark_submit.py:495} INFO - "description" : "FileSink[hdfs://namenode:9000/raw_data]",
[2024-11-11T08:21:31.186+0000] {spark_submit.py:495} INFO - "numOutputRows" : -1
[2024-11-11T08:21:31.187+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:31.188+0000] {spark_submit.py:495} INFO - }
[2024-11-11T08:21:31.952+0000] {spark_submit.py:495} INFO - 24/11/11 08:21:31 INFO CheckpointFileManager: Writing atomically to hdfs://namenode:9000/spark_checkpoint/offsets/7 using temp file hdfs://namenode:9000/spark_checkpoint/offsets/.7.65fa00c3-e2d1-4ca3-af92-0d3a962f84e0.tmp
[2024-11-11T08:33:25.519+0000] {spark_submit.py:495} INFO - 24/11/11 08:30:34 ERROR DataStreamer: No ack received, took 498212ms (threshold=495000ms). File being written: /spark_checkpoint/offsets/.7.65fa00c3-e2d1-4ca3-af92-0d3a962f84e0.tmp, block: BP-1471775586-172.19.0.3-1731304032696:blk_1073741860_1036, Write pipeline datanodes: [DatanodeInfoWithStorage[172.19.0.5:9866,DS-75a528a4-0070-4f2d-805a-5652ffb80fc7,DISK], DatanodeInfoWithStorage[172.19.0.7:9866,DS-4ffd5421-a7e5-4f32-9d92-30f4f48c2937,DISK]].
[2024-11-11T08:33:25.589+0000] {spark_submit.py:495} INFO - 24/11/11 08:23:09 WARN DataStreamer: Exception for BP-1471775586-172.19.0.3-1731304032696:blk_1073741860_1036
[2024-11-11T08:33:25.629+0000] {spark_submit.py:495} INFO - java.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:44908 remote=/172.19.0.5:9866]
[2024-11-11T08:33:25.630+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
[2024-11-11T08:33:25.685+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
[2024-11-11T08:33:25.702+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
[2024-11-11T08:33:25.720+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
[2024-11-11T08:33:25.720+0000] {spark_submit.py:495} INFO - at java.base/java.io.FilterInputStream.read(FilterInputStream.java:82)
[2024-11-11T08:33:25.720+0000] {spark_submit.py:495} INFO - at java.base/java.io.FilterInputStream.read(FilterInputStream.java:82)
[2024-11-11T08:33:25.721+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:519)
[2024-11-11T08:33:25.739+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
[2024-11-11T08:33:25.787+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1137)
[2024-11-11T08:33:25.787+0000] {spark_submit.py:495} INFO - 24/11/11 08:33:25 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@61efd2e4)) by listener AppStatusListener took 1.208819861s.
[2024-11-11T08:33:25.787+0000] {spark_submit.py:495} INFO - 24/11/11 08:33:25 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 302028 ms exceeds timeout 120000 ms
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - 24/11/11 08:33:25 ERROR MicroBatchExecution: Query [id = 78874c98-4dee-4b92-96e3-5cfffbc9705a, runId = ce66f919-ccb7-4244-8460-a0d82da7d70b] terminated with error
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - java.io.InterruptedIOException: No ack received after 498s and a timeout of 495s
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:931)
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:778)
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:888)
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:847)
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
[2024-11-11T08:33:25.788+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:152)
[2024-11-11T08:33:25.797+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204)
[2024-11-11T08:33:25.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)
[2024-11-11T08:33:25.867+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)
[2024-11-11T08:33:25.914+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:761)
[2024-11-11T08:33:25.954+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:534)
[2024-11-11T08:33:25.955+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-11-11T08:33:25.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)
[2024-11-11T08:33:25.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)
[2024-11-11T08:33:25.956+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2024-11-11T08:27:04.356+0000] {job.py:218} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/job.py", line 192, in heartbeat
    self._merge_from(Job._fetch_from_db(self, session))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/retries.py", line 89, in wrapped_function
    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
  File "/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/retries.py", line 98, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/job.py", line 316, in _fetch_from_db
    session.merge(job)
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
             ^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
           ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-11-11T08:33:25.985+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:533)
[2024-11-11T08:33:25.989+0000] {job.py:226} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2024-11-11T08:33:25.990+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2024-11-11T08:33:25.999+0000] {local_task_job_runner.py:214} ERROR - Heartbeat time limit exceeded!
[2024-11-11T08:33:26.000+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)
[2024-11-11T08:33:26.006+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-11-11T08:33:26.007+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:473)
[2024-11-11T08:33:26.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:266)
[2024-11-11T08:33:26.022+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-11-11T08:33:26.047+0000] {process_utils.py:132} INFO - Sending 15 to group 569. PIDs of all processes in the group: [570, 1356, 569]
[2024-11-11T08:33:26.049+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 569
[2024-11-11T08:33:26.172+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-11-11T08:33:26.173+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2024-11-11T08:33:26.195+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-11-11T08:33:26.521+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 495, in _process_spark_submit_log
    self.log.info(line)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1489, in info
    self._log(INFO, msg, args, **kwargs)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1634, in _log
    self.handle(record)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1644, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1706, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 978, in handle
    self.emit(record)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/log/file_task_handler.py", line 248, in emit
    self.handler.emit(record)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1230, in emit
    StreamHandler.emit(self, record)
  File "/usr/local/lib/python3.11/logging/__init__.py", line 1110, in emit
    msg = self.format(record)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/logging/__init__.py", line 953, in format
    return fmt.format(record)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/logging/__init__.py", line 689, in format
    record.asctime = self.formatTime(record, self.datefmt)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/log/timezone_aware.py", line 42, in formatTime
    dt = timezone.from_timestamp(record.created, tz="local")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/timezone.py", line 316, in from_timestamp
    result = result.in_timezone(tz)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pendulum/datetime.py", line 357, in in_timezone
    return tz.convert(dt)
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pendulum/tz/timezone.py", line 136, in convert
    return cast(_DT, dt.astimezone(self))
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pendulum/datetime.py", line 1278, in astimezone
    dt = super().astimezone(tz)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pendulum/datetime.py", line 1239, in __add__
    def __add__(self, other: datetime.timedelta) -> Self:
    
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2613, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal
[2024-11-11T08:33:26.629+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=sensor_data_consumer, task_id=sensor_data_consumer, run_id=manual__2024-11-11T08:02:11.099781+00:00, execution_date=20241111T080211, start_date=20241111T080213, end_date=20241111T083326
[2024-11-11T08:33:26.755+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=570, status='terminated', started='08:02:13') (570) terminated with exit code None
[2024-11-11T08:33:26.756+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=1356, status='terminated', started='08:20:45') (1356) terminated with exit code None
[2024-11-11T08:33:26.757+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=569, status='terminated', exitcode=2, started='08:02:13') (569) terminated with exit code 2
