[2024-11-02T07:49:21.438+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-11-02T07:49:21.804+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-02T07:49:05.474366+00:00 [queued]>
[2024-11-02T07:49:21.895+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-02T07:49:05.474366+00:00 [queued]>
[2024-11-02T07:49:21.896+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-11-02T07:49:22.019+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): sensor_data_consumer> on 2024-11-02 07:49:05.474366+00:00
[2024-11-02T07:49:22.037+0000] {standard_task_runner.py:64} INFO - Started process 2249 to run task
[2024-11-02T07:49:22.046+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sensor_data_consumer', 'sensor_data_consumer', 'manual__2024-11-02T07:49:05.474366+00:00', '--job-id', '678', '--raw', '--subdir', 'DAGS_FOLDER/***_consumer.py', '--cfg-path', '/tmp/tmpsq9vz7q4']
[2024-11-02T07:49:22.052+0000] {standard_task_runner.py:91} INFO - Job 678: Subtask sensor_data_consumer
[2024-11-02T07:49:22.407+0000] {task_command.py:426} INFO - Running <TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-02T07:49:05.474366+00:00 [running]> on host 7d12808d40db
[2024-11-02T07:49:22.846+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Bich Ly' AIRFLOW_CTX_DAG_ID='sensor_data_consumer' AIRFLOW_CTX_TASK_ID='sensor_data_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-11-02T07:49:05.474366+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-02T07:49:05.474366+00:00'
[2024-11-02T07:49:22.853+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-11-02T07:49:22.957+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2024-11-02T07:49:22.976+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2 --name KafkaSparkHDFS /opt/***/dags/spark_streaming_job.py
[2024-11-02T07:50:12.790+0000] {spark_submit.py:495} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-02T07:50:14.508+0000] {spark_submit.py:495} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-02T07:50:14.511+0000] {spark_submit.py:495} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-02T07:50:14.545+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency
[2024-11-02T07:50:14.548+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-11-02T07:50:14.560+0000] {spark_submit.py:495} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-d1792193-9611-4f17-9faf-7fc03cd12b3c;1.0
[2024-11-02T07:50:14.578+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-02T07:50:16.484+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 in central
[2024-11-02T07:50:17.213+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 in central
[2024-11-02T07:50:17.480+0000] {spark_submit.py:495} INFO - found org.apache.kafka#kafka-clients;3.3.2 in central
[2024-11-02T07:50:17.664+0000] {spark_submit.py:495} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-11-02T07:50:18.058+0000] {spark_submit.py:495} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-11-02T07:50:18.371+0000] {spark_submit.py:495} INFO - found org.slf4j#slf4j-api;2.0.6 in central
[2024-11-02T07:50:18.621+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-11-02T07:50:18.776+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-11-02T07:50:18.877+0000] {spark_submit.py:495} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-11-02T07:50:19.095+0000] {spark_submit.py:495} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-11-02T07:50:19.423+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 in central
[2024-11-02T07:50:19.615+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-11-02T07:50:19.855+0000] {spark_submit.py:495} INFO - :: resolution report :: resolve 5097ms :: artifacts dl 202ms
[2024-11-02T07:50:19.856+0000] {spark_submit.py:495} INFO - :: modules in use:
[2024-11-02T07:50:19.857+0000] {spark_submit.py:495} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2024-11-02T07:50:19.857+0000] {spark_submit.py:495} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2024-11-02T07:50:19.857+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-11-02T07:50:19.857+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-11-02T07:50:19.858+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-11-02T07:50:19.858+0000] {spark_submit.py:495} INFO - org.apache.kafka#kafka-clients;3.3.2 from central in [default]
[2024-11-02T07:50:19.858+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-02T07:50:19.858+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-02T07:50:19.866+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-02T07:50:19.867+0000] {spark_submit.py:495} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-11-02T07:50:19.868+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;2.0.6 from central in [default]
[2024-11-02T07:50:19.869+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-11-02T07:50:19.869+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-02T07:50:19.869+0000] {spark_submit.py:495} INFO - |                  |            modules            ||   artifacts   |
[2024-11-02T07:50:19.869+0000] {spark_submit.py:495} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-02T07:50:19.870+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-02T07:50:19.870+0000] {spark_submit.py:495} INFO - |      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
[2024-11-02T07:50:19.870+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-02T07:50:19.938+0000] {spark_submit.py:495} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-d1792193-9611-4f17-9faf-7fc03cd12b3c
[2024-11-02T07:50:19.938+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-02T07:50:20.115+0000] {spark_submit.py:495} INFO - 0 artifacts copied, 12 already retrieved (0kB/176ms)
[2024-11-02T07:50:25.249+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-02T07:50:40.514+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO SparkContext: Running Spark version 3.4.2
[2024-11-02T07:50:40.714+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO ResourceUtils: ==============================================================
[2024-11-02T07:50:40.759+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-02T07:50:40.760+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO ResourceUtils: ==============================================================
[2024-11-02T07:50:40.760+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO SparkContext: Submitted application: KafkaSparkStreaming
[2024-11-02T07:50:40.943+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-02T07:50:41.003+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO ResourceProfile: Limiting resource is cpu
[2024-11-02T07:50:41.005+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-02T07:50:41.470+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:41 INFO SecurityManager: Changing view acls to: ***
[2024-11-02T07:50:41.486+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:41 INFO SecurityManager: Changing modify acls to: ***
[2024-11-02T07:50:41.495+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:41 INFO SecurityManager: Changing view acls groups to:
[2024-11-02T07:50:41.497+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:41 INFO SecurityManager: Changing modify acls groups to:
[2024-11-02T07:50:41.499+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-02T07:50:43.923+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:43 INFO Utils: Successfully started service 'sparkDriver' on port 33043.
[2024-11-02T07:50:44.378+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:44 INFO SparkEnv: Registering MapOutputTracker
[2024-11-02T07:50:45.321+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:45 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-02T07:50:45.611+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-02T07:50:45.617+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-02T07:50:45.674+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-02T07:50:45.972+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-63f988db-a349-470d-b2af-d814b272edcd
[2024-11-02T07:50:46.148+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-02T07:50:46.286+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-02T07:50:47.652+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-02T07:50:48.636+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-02T07:50:49.380+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:33043/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.387+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:33043/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.414+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:33043/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.415+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://7d12808d40db:33043/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.415+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://7d12808d40db:33043/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1730533840483
[2024-11-02T07:50:49.416+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://7d12808d40db:33043/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1730533840483
[2024-11-02T07:50:49.416+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://7d12808d40db:33043/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1730533840483
[2024-11-02T07:50:49.416+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://7d12808d40db:33043/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1730533840483
[2024-11-02T07:50:49.417+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://7d12808d40db:33043/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1730533840483
[2024-11-02T07:50:49.417+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://7d12808d40db:33043/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1730533840483
[2024-11-02T07:50:49.417+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://7d12808d40db:33043/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1730533840483
[2024-11-02T07:50:49.418+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://7d12808d40db:33043/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1730533840483
[2024-11-02T07:50:49.418+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:33043/files/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.418+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.spark_spark-streaming-kafka-0-10_2.12-3.4.2.jar
[2024-11-02T07:50:49.632+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:33043/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.640+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.2.jar
[2024-11-02T07:50:49.735+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar at spark://7d12808d40db:33043/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.738+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.2.jar
[2024-11-02T07:50:49.851+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://7d12808d40db:33043/files/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1730533840483
[2024-11-02T07:50:49.860+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.kafka_kafka-clients-3.3.2.jar
[2024-11-02T07:50:50.008+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:50 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://7d12808d40db:33043/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1730533840483
[2024-11-02T07:50:50.013+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:50 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-11-02T07:50:53.053+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://7d12808d40db:33043/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1730533840483
[2024-11-02T07:50:53.056+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.lz4_lz4-java-1.8.0.jar
[2024-11-02T07:50:53.262+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://7d12808d40db:33043/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1730533840483
[2024-11-02T07:50:53.270+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-11-02T07:50:53.385+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://7d12808d40db:33043/files/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1730533840483
[2024-11-02T07:50:53.391+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.slf4j_slf4j-api-2.0.6.jar
[2024-11-02T07:50:53.566+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://7d12808d40db:33043/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1730533840483
[2024-11-02T07:50:53.569+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:53 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-11-02T07:50:56.473+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:56 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://7d12808d40db:33043/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1730533840483
[2024-11-02T07:50:56.473+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:56 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/commons-logging_commons-logging-1.1.3.jar
[2024-11-02T07:50:56.704+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:56 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://7d12808d40db:33043/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1730533840483
[2024-11-02T07:50:56.772+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:56 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-11-02T07:50:57.165+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:57 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://7d12808d40db:33043/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1730533840483
[2024-11-02T07:50:57.167+0000] {spark_submit.py:495} INFO - 24/11/02 07:50:57 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-8f26f989-5e7e-4a17-a8a4-92d6826d9898/userFiles-33aefc9f-a16e-4eeb-adac-9b8163efe2ca/org.apache.commons_commons-pool2-2.11.1.jar
[2024-11-02T07:51:02.402+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:02 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-02T07:51:03.135+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.22.0.2:7077 after 332 ms (0 ms spent in bootstraps)
[2024-11-02T07:51:10.964+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241102075109-0000
[2024-11-02T07:51:11.141+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43395.
[2024-11-02T07:51:11.142+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO NettyBlockTransferService: Server created on 7d12808d40db:43395
[2024-11-02T07:51:11.148+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-02T07:51:11.222+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7d12808d40db, 43395, None)
[2024-11-02T07:51:11.306+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO BlockManagerMasterEndpoint: Registering block manager 7d12808d40db:43395 with 434.4 MiB RAM, BlockManagerId(driver, 7d12808d40db, 43395, None)
[2024-11-02T07:51:11.307+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7d12808d40db, 43395, None)
[2024-11-02T07:51:11.330+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7d12808d40db, 43395, None)
[2024-11-02T07:51:11.964+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241102075109-0000/0 on worker-20241102064420-172.22.0.4-45219 (172.22.0.4:45219) with 1 core(s)
[2024-11-02T07:51:11.987+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20241102075109-0000/0 on hostPort 172.22.0.4:45219 with 1 core(s), 1024.0 MiB RAM
[2024-11-02T07:51:13.772+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-02T07:51:16.377+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-02T07:51:16.437+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-02T07:51:24.161+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241102075109-0000/0 is now RUNNING
[2024-11-02T07:51:41.142+0000] {spark_submit.py:495} INFO - done
[2024-11-02T07:51:41.923+0000] {spark_submit.py:495} INFO - root
[2024-11-02T07:51:41.924+0000] {spark_submit.py:495} INFO - |-- sensor_id: integer (nullable = true)
[2024-11-02T07:51:41.928+0000] {spark_submit.py:495} INFO - |-- temperature: float (nullable = true)
[2024-11-02T07:51:41.929+0000] {spark_submit.py:495} INFO - |-- humidity: float (nullable = true)
[2024-11-02T07:51:41.933+0000] {spark_submit.py:495} INFO - |-- timestamp: string (nullable = true)
[2024-11-02T07:51:41.948+0000] {spark_submit.py:495} INFO - 
[2024-11-02T07:51:42.630+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:42 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-11-02T07:51:43.165+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-11-02T07:51:43.424+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:43 INFO ResolveWriteToStream: Checkpoint root /tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c resolved to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c.
[2024-11-02T07:51:43.426+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-11-02T07:51:44.580+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/metadata using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/.metadata.11ac23c6-3baa-49ce-aa1a-8ef71450cc7e.tmp
[2024-11-02T07:51:46.040+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/.metadata.11ac23c6-3baa-49ce-aa1a-8ef71450cc7e.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/metadata
[2024-11-02T07:51:46.292+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO MicroBatchExecution: Starting [id = 1fd99432-c7e5-4ad2-abdf-c2441802f585, runId = 79fe2973-457e-4920-95f8-9a2fd4db54de]. Use file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c to store the query checkpoint.
[2024-11-02T07:51:46.372+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1f568160] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@218f353a]
[2024-11-02T07:51:46.541+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-02T07:51:46.551+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO OffsetSeqLog: BatchIds found from listing:
[2024-11-02T07:51:46.552+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO MicroBatchExecution: Starting new streaming query.
[2024-11-02T07:51:46.602+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:46 INFO MicroBatchExecution: Stream started from {}
[2024-11-02T07:51:51.238+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:51 INFO AdminClientConfig: AdminClientConfig values:
[2024-11-02T07:51:51.238+0000] {spark_submit.py:495} INFO - bootstrap.servers = [10.0.2.15:9092]
[2024-11-02T07:51:51.239+0000] {spark_submit.py:495} INFO - client.dns.lookup = use_all_dns_ips
[2024-11-02T07:51:51.239+0000] {spark_submit.py:495} INFO - client.id =
[2024-11-02T07:51:51.239+0000] {spark_submit.py:495} INFO - connections.max.idle.ms = 300000
[2024-11-02T07:51:51.239+0000] {spark_submit.py:495} INFO - default.api.timeout.ms = 60000
[2024-11-02T07:51:51.240+0000] {spark_submit.py:495} INFO - metadata.max.age.ms = 300000
[2024-11-02T07:51:51.240+0000] {spark_submit.py:495} INFO - metric.reporters = []
[2024-11-02T07:51:51.240+0000] {spark_submit.py:495} INFO - metrics.num.samples = 2
[2024-11-02T07:51:51.240+0000] {spark_submit.py:495} INFO - metrics.recording.level = INFO
[2024-11-02T07:51:51.241+0000] {spark_submit.py:495} INFO - metrics.sample.window.ms = 30000
[2024-11-02T07:51:51.241+0000] {spark_submit.py:495} INFO - receive.buffer.bytes = 65536
[2024-11-02T07:51:51.252+0000] {spark_submit.py:495} INFO - reconnect.backoff.max.ms = 1000
[2024-11-02T07:51:51.264+0000] {spark_submit.py:495} INFO - reconnect.backoff.ms = 50
[2024-11-02T07:51:51.266+0000] {spark_submit.py:495} INFO - request.timeout.ms = 30000
[2024-11-02T07:51:51.267+0000] {spark_submit.py:495} INFO - retries = 2147483647
[2024-11-02T07:51:51.273+0000] {spark_submit.py:495} INFO - retry.backoff.ms = 100
[2024-11-02T07:51:51.295+0000] {spark_submit.py:495} INFO - sasl.client.callback.handler.class = null
[2024-11-02T07:51:51.308+0000] {spark_submit.py:495} INFO - sasl.jaas.config = null
[2024-11-02T07:51:51.325+0000] {spark_submit.py:495} INFO - sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2024-11-02T07:51:51.346+0000] {spark_submit.py:495} INFO - sasl.kerberos.min.time.before.relogin = 60000
[2024-11-02T07:51:51.348+0000] {spark_submit.py:495} INFO - sasl.kerberos.service.name = null
[2024-11-02T07:51:51.375+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.jitter = 0.05
[2024-11-02T07:51:51.376+0000] {spark_submit.py:495} INFO - sasl.kerberos.ticket.renew.window.factor = 0.8
[2024-11-02T07:51:51.376+0000] {spark_submit.py:495} INFO - sasl.login.callback.handler.class = null
[2024-11-02T07:51:51.376+0000] {spark_submit.py:495} INFO - sasl.login.class = null
[2024-11-02T07:51:51.426+0000] {spark_submit.py:495} INFO - sasl.login.connect.timeout.ms = null
[2024-11-02T07:51:51.427+0000] {spark_submit.py:495} INFO - sasl.login.read.timeout.ms = null
[2024-11-02T07:51:51.444+0000] {spark_submit.py:495} INFO - sasl.login.refresh.buffer.seconds = 300
[2024-11-02T07:51:51.444+0000] {spark_submit.py:495} INFO - sasl.login.refresh.min.period.seconds = 60
[2024-11-02T07:51:51.445+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.factor = 0.8
[2024-11-02T07:51:51.445+0000] {spark_submit.py:495} INFO - sasl.login.refresh.window.jitter = 0.05
[2024-11-02T07:51:51.445+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.max.ms = 10000
[2024-11-02T07:51:51.446+0000] {spark_submit.py:495} INFO - sasl.login.retry.backoff.ms = 100
[2024-11-02T07:51:51.446+0000] {spark_submit.py:495} INFO - sasl.mechanism = GSSAPI
[2024-11-02T07:51:51.447+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.clock.skew.seconds = 30
[2024-11-02T07:51:51.448+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.audience = null
[2024-11-02T07:51:51.448+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.expected.issuer = null
[2024-11-02T07:51:51.448+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2024-11-02T07:51:51.449+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2024-11-02T07:51:51.449+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2024-11-02T07:51:51.449+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.jwks.endpoint.url = null
[2024-11-02T07:51:51.450+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.scope.claim.name = scope
[2024-11-02T07:51:51.450+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.sub.claim.name = sub
[2024-11-02T07:51:51.450+0000] {spark_submit.py:495} INFO - sasl.oauthbearer.token.endpoint.url = null
[2024-11-02T07:51:51.451+0000] {spark_submit.py:495} INFO - security.protocol = PLAINTEXT
[2024-11-02T07:51:51.451+0000] {spark_submit.py:495} INFO - security.providers = null
[2024-11-02T07:51:51.452+0000] {spark_submit.py:495} INFO - send.buffer.bytes = 131072
[2024-11-02T07:51:51.452+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.max.ms = 30000
[2024-11-02T07:51:51.452+0000] {spark_submit.py:495} INFO - socket.connection.setup.timeout.ms = 10000
[2024-11-02T07:51:51.452+0000] {spark_submit.py:495} INFO - ssl.cipher.suites = null
[2024-11-02T07:51:51.453+0000] {spark_submit.py:495} INFO - ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2024-11-02T07:51:51.453+0000] {spark_submit.py:495} INFO - ssl.endpoint.identification.algorithm = https
[2024-11-02T07:51:51.453+0000] {spark_submit.py:495} INFO - ssl.engine.factory.class = null
[2024-11-02T07:51:51.454+0000] {spark_submit.py:495} INFO - ssl.key.password = null
[2024-11-02T07:51:51.454+0000] {spark_submit.py:495} INFO - ssl.keymanager.algorithm = SunX509
[2024-11-02T07:51:51.454+0000] {spark_submit.py:495} INFO - ssl.keystore.certificate.chain = null
[2024-11-02T07:51:51.454+0000] {spark_submit.py:495} INFO - ssl.keystore.key = null
[2024-11-02T07:51:51.455+0000] {spark_submit.py:495} INFO - ssl.keystore.location = null
[2024-11-02T07:51:51.455+0000] {spark_submit.py:495} INFO - ssl.keystore.password = null
[2024-11-02T07:51:51.455+0000] {spark_submit.py:495} INFO - ssl.keystore.type = JKS
[2024-11-02T07:51:51.456+0000] {spark_submit.py:495} INFO - ssl.protocol = TLSv1.3
[2024-11-02T07:51:51.456+0000] {spark_submit.py:495} INFO - ssl.provider = null
[2024-11-02T07:51:51.456+0000] {spark_submit.py:495} INFO - ssl.secure.random.implementation = null
[2024-11-02T07:51:51.457+0000] {spark_submit.py:495} INFO - ssl.trustmanager.algorithm = PKIX
[2024-11-02T07:51:51.457+0000] {spark_submit.py:495} INFO - ssl.truststore.certificates = null
[2024-11-02T07:51:51.457+0000] {spark_submit.py:495} INFO - ssl.truststore.location = null
[2024-11-02T07:51:51.457+0000] {spark_submit.py:495} INFO - ssl.truststore.password = null
[2024-11-02T07:51:51.458+0000] {spark_submit.py:495} INFO - ssl.truststore.type = JKS
[2024-11-02T07:51:51.458+0000] {spark_submit.py:495} INFO - 
[2024-11-02T07:51:52.011+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:51 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-11-02T07:51:52.032+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:51 INFO AppInfoParser: Kafka version: 3.3.2
[2024-11-02T07:51:52.033+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:52 INFO AppInfoParser: Kafka commitId: b66af662e61082cb
[2024-11-02T07:51:52.033+0000] {spark_submit.py:495} INFO - 24/11/02 07:51:52 INFO AppInfoParser: Kafka startTimeMs: 1730533911996
[2024-11-02T07:52:02.282+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/sources/0/0 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/sources/0/.0.7da0e6d3-5de4-4455-bf6a-c15f117e4871.tmp
[2024-11-02T07:52:03.413+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/sources/0/.0.7da0e6d3-5de4-4455-bf6a-c15f117e4871.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/sources/0/0
[2024-11-02T07:52:03.419+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:03 INFO KafkaMicroBatchStream: Initial offsets: {"raw_data":{"0":6024}}
[2024-11-02T07:52:03.662+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/0 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.0.4571d8b2-52c6-41c8-b524-ad2aa47d678a.tmp
[2024-11-02T07:52:04.510+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.0.4571d8b2-52c6-41c8-b524-ad2aa47d678a.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/0
[2024-11-02T07:52:04.523+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:04 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1730533923513,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T07:52:09.139+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T07:52:09.593+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:09 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.4:49244) with ID 0,  ResourceProfileId 0
[2024-11-02T07:52:09.856+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T07:52:10.079+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T07:52:10.094+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T07:52:10.234+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.22.0.4:37367 with 434.4 MiB RAM, BlockManagerId(0, 172.22.0.4, 37367, None)
[2024-11-02T07:52:10.531+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T07:52:10.583+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T07:52:13.708+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:13 INFO CodeGenerator: Code generated in 1921.751041 ms
[2024-11-02T07:52:17.103+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T07:52:17.272+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T07:52:18.154+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:18 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T07:52:18.203+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:18 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T07:52:18.225+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:18 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T07:52:18.249+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:18 INFO DAGScheduler: Missing parents: List()
[2024-11-02T07:52:18.480+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:18 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T07:52:21.088+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
[2024-11-02T07:52:22.499+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2036.0 B, free 434.4 MiB)
[2024-11-02T07:52:22.513+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7d12808d40db:43395 (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T07:52:22.640+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2024-11-02T07:52:22.797+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T07:52:23.039+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-02T07:52:27.735+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 7388 bytes)
[2024-11-02T07:52:32.751+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.4:37367 (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T07:52:35.785+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8139 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T07:52:35.798+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-02T07:52:35.822+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:35 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 17.277 s
[2024-11-02T07:52:35.980+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T07:52:35.999+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-02T07:52:36.246+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:36 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 18.757644 s
[2024-11-02T07:52:36.288+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T07:52:36.288+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T07:52:36.289+0000] {spark_submit.py:495} INFO - Batch: 0
[2024-11-02T07:52:36.289+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T07:52:37.310+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+---------+
[2024-11-02T07:52:37.311+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp|
[2024-11-02T07:52:37.324+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+---------+
[2024-11-02T07:52:37.324+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+---------+
[2024-11-02T07:52:37.344+0000] {spark_submit.py:495} INFO - 
[2024-11-02T07:52:37.578+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T07:52:38.999+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/0 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.0.c6a32de3-76ae-495f-8fb5-7cff87a99e5a.tmp
[2024-11-02T07:52:39.786+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.0.c6a32de3-76ae-495f-8fb5-7cff87a99e5a.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/0
[2024-11-02T07:52:40.090+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:52:40.097+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:52:40.099+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:52:40.100+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:52:40.106+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:51:46.511Z",
[2024-11-02T07:52:40.113+0000] {spark_submit.py:495} INFO - "batchId" : 0,
[2024-11-02T07:52:40.114+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:52:40.115+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:52:40.115+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:52:40.116+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:52:40.116+0000] {spark_submit.py:495} INFO - "addBatch" : 27264,
[2024-11-02T07:52:40.116+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1965,
[2024-11-02T07:52:40.117+0000] {spark_submit.py:495} INFO - "getBatch" : 110,
[2024-11-02T07:52:40.122+0000] {spark_submit.py:495} INFO - "latestOffset" : 16868,
[2024-11-02T07:52:40.123+0000] {spark_submit.py:495} INFO - "queryPlanning" : 5263,
[2024-11-02T07:52:40.127+0000] {spark_submit.py:495} INFO - "triggerExecution" : 53272,
[2024-11-02T07:52:40.127+0000] {spark_submit.py:495} INFO - "walCommit" : 992
[2024-11-02T07:52:40.128+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:40.128+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:52:40.128+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:52:40.129+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:52:40.129+0000] {spark_submit.py:495} INFO - "startOffset" : null,
[2024-11-02T07:52:40.129+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:52:40.129+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:52:40.129+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:52:40.130+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:40.130+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:40.130+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:52:40.130+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:52:40.131+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:52:40.131+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:40.131+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:40.131+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:52:40.132+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:52:40.132+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:52:40.132+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:52:40.132+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:52:40.133+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:52:40.133+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:52:40.133+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:40.133+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:52:40.133+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:52:40.133+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:52:40.134+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:52:40.134+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:40.134+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:50.256+0000] {spark_submit.py:495} INFO - 24/11/02 07:52:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:52:50.275+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:52:50.276+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:52:50.277+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:52:50.277+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:52:49.690Z",
[2024-11-02T07:52:50.278+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:52:50.279+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:52:50.301+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:52:50.302+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:52:50.303+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:52:50.303+0000] {spark_submit.py:495} INFO - "latestOffset" : 310,
[2024-11-02T07:52:50.303+0000] {spark_submit.py:495} INFO - "triggerExecution" : 311
[2024-11-02T07:52:50.304+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:50.304+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:52:50.304+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:52:50.305+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:52:50.305+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:52:50.325+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:52:50.335+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:52:50.346+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:50.348+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:50.348+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:52:50.349+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:52:50.349+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:52:50.357+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:50.377+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:50.383+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:52:50.388+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:52:50.389+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:52:50.401+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:50.402+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:52:50.402+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:52:50.411+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:52:50.415+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:52:50.416+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:52:50.416+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:52:50.417+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:52:50.417+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:52:50.417+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:50.418+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:52:50.418+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:52:50.419+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:52:50.419+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:52:50.428+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:52:50.437+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:00.425+0000] {spark_submit.py:495} INFO - 24/11/02 07:53:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:53:00.427+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:53:00.429+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:53:00.430+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:53:00.430+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:53:00.197Z",
[2024-11-02T07:53:00.431+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:53:00.431+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:00.431+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:00.431+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:00.432+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:53:00.432+0000] {spark_submit.py:495} INFO - "latestOffset" : 205,
[2024-11-02T07:53:00.432+0000] {spark_submit.py:495} INFO - "triggerExecution" : 215
[2024-11-02T07:53:00.432+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:00.433+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:53:00.438+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:53:00.443+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:53:00.443+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:53:00.450+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:00.451+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:00.451+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:00.451+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:00.456+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:53:00.457+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:00.459+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:00.459+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:00.460+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:00.460+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:53:00.467+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:00.467+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:00.470+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:00.471+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:00.471+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:00.472+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:00.472+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:00.478+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:53:00.482+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:53:00.482+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:53:00.487+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:53:00.488+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:00.489+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:53:00.489+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:53:00.489+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:53:00.490+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:53:00.493+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:00.493+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:10.716+0000] {spark_submit.py:495} INFO - 24/11/02 07:53:10 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:53:10.724+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:53:10.728+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:53:10.728+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:53:10.729+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:53:10.367Z",
[2024-11-02T07:53:10.729+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:53:10.729+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:10.729+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:10.738+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:10.739+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:53:10.739+0000] {spark_submit.py:495} INFO - "latestOffset" : 334,
[2024-11-02T07:53:10.739+0000] {spark_submit.py:495} INFO - "triggerExecution" : 334
[2024-11-02T07:53:10.739+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:10.740+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:53:10.740+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:53:10.740+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:53:10.740+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:53:10.741+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:10.741+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:10.741+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:10.741+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:10.742+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:53:10.742+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:10.742+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:10.742+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:10.743+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:10.743+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:53:10.743+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:10.743+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:10.744+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:10.744+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:10.744+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:10.744+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:10.745+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:10.745+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:53:10.745+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:53:10.745+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:53:10.746+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:53:10.746+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:10.746+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:53:10.746+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:53:10.746+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:53:10.747+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:53:10.747+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:10.747+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:20.769+0000] {spark_submit.py:495} INFO - 24/11/02 07:53:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:53:20.774+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:53:20.782+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:53:20.783+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:53:20.783+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:53:20.672Z",
[2024-11-02T07:53:20.784+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:53:20.784+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:20.803+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:20.804+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:20.804+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:53:20.804+0000] {spark_submit.py:495} INFO - "latestOffset" : 73,
[2024-11-02T07:53:20.804+0000] {spark_submit.py:495} INFO - "triggerExecution" : 75
[2024-11-02T07:53:20.804+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:20.805+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:53:20.805+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:53:20.805+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:53:20.805+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:53:20.805+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:20.805+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:20.806+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:20.806+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:20.806+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:53:20.806+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:20.806+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:20.807+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:20.807+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:20.807+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:53:20.807+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:20.807+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:20.808+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:20.808+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:20.808+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:20.808+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:20.808+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:20.808+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:53:20.809+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:53:20.809+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:53:20.809+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:53:20.809+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:20.809+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:53:20.809+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:53:20.810+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:53:20.810+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:53:20.810+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:20.813+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:30.793+0000] {spark_submit.py:495} INFO - 24/11/02 07:53:30 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:53:30.797+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:53:30.798+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:53:30.798+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:53:30.799+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:53:30.736Z",
[2024-11-02T07:53:30.799+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:53:30.799+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:30.799+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:30.800+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:30.805+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:53:30.814+0000] {spark_submit.py:495} INFO - "latestOffset" : 46,
[2024-11-02T07:53:30.814+0000] {spark_submit.py:495} INFO - "triggerExecution" : 47
[2024-11-02T07:53:30.815+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:30.815+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:53:30.815+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:53:30.821+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:53:30.822+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:53:30.822+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:30.827+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:30.827+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:30.827+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:30.828+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:53:30.828+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:30.828+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:30.828+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:30.828+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:30.829+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:53:30.829+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:30.829+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:30.829+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:30.829+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:30.829+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:30.830+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:30.830+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:30.830+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:53:30.830+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:53:30.830+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:53:30.830+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:53:30.831+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:30.831+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:53:30.831+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:53:30.831+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:53:30.831+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:53:30.832+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:30.832+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:40.808+0000] {spark_submit.py:495} INFO - 24/11/02 07:53:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:53:40.809+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:53:40.809+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:53:40.813+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:53:40.814+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:53:40.785Z",
[2024-11-02T07:53:40.822+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:53:40.824+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:40.825+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:40.825+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:40.825+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:53:40.826+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-02T07:53:40.826+0000] {spark_submit.py:495} INFO - "triggerExecution" : 16
[2024-11-02T07:53:40.826+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:40.827+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:53:40.827+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:53:40.827+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:53:40.827+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:53:40.828+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:40.828+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:40.828+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:40.828+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:40.829+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:53:40.829+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:40.829+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:40.829+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:40.830+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:40.830+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:53:40.830+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:40.830+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:40.831+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:40.831+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:40.831+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:40.841+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:40.841+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:40.842+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:53:40.842+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:53:40.843+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:53:40.843+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:53:40.843+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:40.844+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:53:40.848+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:53:40.849+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:53:40.849+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:53:40.851+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:40.851+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:50.863+0000] {spark_submit.py:495} INFO - 24/11/02 07:53:50 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:53:50.864+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:53:50.864+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:53:50.864+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:53:50.865+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:53:50.764Z",
[2024-11-02T07:53:50.865+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:53:50.865+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:50.865+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:50.866+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:50.866+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:53:50.866+0000] {spark_submit.py:495} INFO - "latestOffset" : 76,
[2024-11-02T07:53:50.866+0000] {spark_submit.py:495} INFO - "triggerExecution" : 78
[2024-11-02T07:53:50.867+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:50.867+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:53:50.867+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:53:50.867+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:53:50.868+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:53:50.868+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:50.868+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:50.868+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:50.868+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:50.869+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:53:50.869+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:50.869+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:50.869+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:50.870+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:50.870+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:53:50.870+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:53:50.870+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:53:50.871+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:50.871+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:53:50.871+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:53:50.871+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:53:50.872+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:53:50.872+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:53:50.872+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:53:50.872+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:53:50.873+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:53:50.873+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:50.873+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:53:50.873+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:53:50.874+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:53:50.874+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:53:50.874+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:53:50.874+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:00.937+0000] {spark_submit.py:495} INFO - 24/11/02 07:54:00 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:54:00.998+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:54:00.999+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:54:00.999+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:54:00.999+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:54:00.651Z",
[2024-11-02T07:54:00.999+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:54:00.999+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:01.000+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:01.000+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:01.000+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:54:01.000+0000] {spark_submit.py:495} INFO - "latestOffset" : 215,
[2024-11-02T07:54:01.001+0000] {spark_submit.py:495} INFO - "triggerExecution" : 216
[2024-11-02T07:54:01.001+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:01.001+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:54:01.001+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:54:01.002+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:54:01.002+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:54:01.002+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:01.002+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:01.002+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:01.002+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:01.003+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:54:01.003+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:01.003+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:01.003+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:01.003+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:01.003+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:54:01.004+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:01.004+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:01.004+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:01.004+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:01.004+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:01.004+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:01.005+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:01.005+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:54:01.005+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:54:01.005+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:54:01.005+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:54:01.005+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:01.006+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:54:01.006+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:54:01.006+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:54:01.006+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:54:01.006+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:01.006+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:10.898+0000] {spark_submit.py:495} INFO - 24/11/02 07:54:10 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:54:10.914+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:54:10.914+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:54:10.918+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:54:10.922+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:54:10.837Z",
[2024-11-02T07:54:10.924+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:54:10.927+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:10.928+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:10.928+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:10.929+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:54:10.929+0000] {spark_submit.py:495} INFO - "latestOffset" : 48,
[2024-11-02T07:54:10.929+0000] {spark_submit.py:495} INFO - "triggerExecution" : 57
[2024-11-02T07:54:10.929+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:10.929+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:54:10.930+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:54:10.930+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:54:10.937+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:54:10.937+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:10.937+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:10.938+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:10.938+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:10.938+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:54:10.938+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:10.938+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:10.939+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:10.939+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:10.939+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:54:10.939+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:10.940+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:10.940+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:10.940+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:10.940+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:10.940+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:10.941+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:10.949+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:54:10.949+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:54:10.950+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:54:10.950+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:54:10.950+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:10.950+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:54:10.951+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:54:10.951+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:54:10.955+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:54:10.955+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:10.957+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:20.966+0000] {spark_submit.py:495} INFO - 24/11/02 07:54:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:54:20.978+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:54:20.979+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:54:20.989+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:54:21.019+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:54:20.906Z",
[2024-11-02T07:54:21.029+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:54:21.041+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:21.049+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:21.049+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:21.049+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:54:21.050+0000] {spark_submit.py:495} INFO - "latestOffset" : 43,
[2024-11-02T07:54:21.050+0000] {spark_submit.py:495} INFO - "triggerExecution" : 43
[2024-11-02T07:54:21.050+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:21.050+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:54:21.063+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:54:21.071+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:54:21.072+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:54:21.072+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:21.073+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:21.073+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:21.073+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:21.073+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:54:21.074+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:21.074+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:21.074+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:21.075+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:21.075+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:54:21.075+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:21.075+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:21.076+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:21.076+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:21.076+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:21.092+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:21.102+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:21.103+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:54:21.103+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:54:21.103+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:54:21.103+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:54:21.103+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:21.104+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:54:21.104+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:54:21.104+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:54:21.122+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:54:21.125+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:21.125+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:31.131+0000] {spark_submit.py:495} INFO - 24/11/02 07:54:31 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:54:31.132+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:54:31.132+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:54:31.132+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:54:31.132+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:54:30.946Z",
[2024-11-02T07:54:31.133+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:54:31.133+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:31.133+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:31.133+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:31.134+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:54:31.134+0000] {spark_submit.py:495} INFO - "latestOffset" : 170,
[2024-11-02T07:54:31.134+0000] {spark_submit.py:495} INFO - "triggerExecution" : 171
[2024-11-02T07:54:31.135+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:31.135+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:54:31.135+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:54:31.135+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:54:31.136+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:54:31.136+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:31.136+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:31.136+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:31.136+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:31.137+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:54:31.137+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:31.137+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:31.137+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:31.138+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:31.138+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:54:31.138+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:31.138+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:31.138+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:31.139+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:31.139+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:31.139+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:31.139+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:31.140+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:54:31.140+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:54:31.140+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:54:31.141+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:54:31.141+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:31.141+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:54:31.141+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:54:31.142+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:54:31.142+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:54:31.142+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:31.142+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:42.762+0000] {spark_submit.py:495} INFO - 24/11/02 07:54:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:54:42.767+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:54:42.767+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:54:42.767+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:54:42.768+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:54:41.009Z",
[2024-11-02T07:54:42.768+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:54:42.768+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:42.769+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:42.769+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:42.769+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:54:42.770+0000] {spark_submit.py:495} INFO - "latestOffset" : 1612,
[2024-11-02T07:54:42.770+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1612
[2024-11-02T07:54:42.771+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:42.771+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:54:42.772+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:54:43.008+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:54:43.017+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:54:43.018+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:43.018+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:43.019+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:43.119+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:43.120+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:54:43.121+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:43.121+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:43.175+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:43.175+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:43.413+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:54:43.414+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:43.414+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:43.415+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:43.415+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:43.416+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:43.416+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:43.416+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:43.417+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:54:43.417+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:54:43.417+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:54:43.418+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:54:43.418+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:43.418+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:54:43.419+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:54:43.419+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:54:43.419+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:54:43.420+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:43.420+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:52.875+0000] {spark_submit.py:495} INFO - 24/11/02 07:54:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:54:52.912+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:54:52.913+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:54:52.913+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:54:52.914+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:54:52.550Z",
[2024-11-02T07:54:52.914+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:54:52.914+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:52.915+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:52.915+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:52.915+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:54:52.916+0000] {spark_submit.py:495} INFO - "latestOffset" : 261,
[2024-11-02T07:54:52.916+0000] {spark_submit.py:495} INFO - "triggerExecution" : 263
[2024-11-02T07:54:52.932+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:52.945+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:54:52.946+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:54:52.946+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:54:52.947+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:54:52.947+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:52.947+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:52.948+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:52.948+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:52.948+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:54:52.948+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:52.948+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:52.949+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:52.949+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:52.949+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:54:52.949+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:54:53.002+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:54:53.003+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:53.003+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:54:53.003+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:54:53.004+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:54:53.004+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:54:53.004+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:54:53.004+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:54:53.005+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:54:53.005+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:54:53.005+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:53.005+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:54:53.006+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:54:53.011+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:54:53.012+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:54:53.012+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:54:53.012+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:02.889+0000] {spark_submit.py:495} INFO - 24/11/02 07:55:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:55:02.896+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:55:02.896+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:55:02.896+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:55:02.897+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:55:02.807Z",
[2024-11-02T07:55:02.897+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:55:02.898+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:02.898+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:02.899+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:02.899+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:55:02.900+0000] {spark_submit.py:495} INFO - "latestOffset" : 62,
[2024-11-02T07:55:02.900+0000] {spark_submit.py:495} INFO - "triggerExecution" : 63
[2024-11-02T07:55:02.901+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:02.901+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:55:02.902+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:55:02.902+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:55:02.902+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:55:02.903+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:02.903+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:02.903+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:02.904+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:02.904+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:55:02.905+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:02.905+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:02.906+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:02.906+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:02.906+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:55:02.907+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:02.907+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:02.907+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:02.908+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:02.908+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:02.908+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:02.908+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:02.909+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:55:02.909+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:55:02.910+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:55:02.910+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:55:02.910+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:02.911+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:55:02.911+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:55:02.912+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:55:02.912+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:55:02.913+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:02.914+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:12.899+0000] {spark_submit.py:495} INFO - 24/11/02 07:55:12 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:55:12.900+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:55:12.901+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:55:12.901+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:55:12.901+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:55:12.817Z",
[2024-11-02T07:55:12.902+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:55:12.902+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:12.902+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:12.902+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:12.902+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:55:12.902+0000] {spark_submit.py:495} INFO - "latestOffset" : 70,
[2024-11-02T07:55:12.903+0000] {spark_submit.py:495} INFO - "triggerExecution" : 70
[2024-11-02T07:55:12.903+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:12.903+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:55:12.903+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:55:12.903+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:55:12.904+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:55:12.904+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:12.904+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:12.904+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:12.921+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:12.921+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:55:12.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:12.939+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:12.939+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:12.940+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:12.940+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:55:12.940+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:12.941+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:12.941+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:13.003+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:13.018+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:13.021+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:13.021+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:13.022+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:55:13.022+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:55:13.023+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:55:13.049+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:55:13.053+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:13.053+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:55:13.054+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:55:13.054+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:55:13.054+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:55:13.054+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:13.054+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:22.984+0000] {spark_submit.py:495} INFO - 24/11/02 07:55:22 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:55:22.987+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:55:22.987+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:55:22.988+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:55:22.988+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:55:22.754Z",
[2024-11-02T07:55:22.988+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:55:22.988+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:22.988+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:22.989+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:22.989+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:55:22.989+0000] {spark_submit.py:495} INFO - "latestOffset" : 227,
[2024-11-02T07:55:22.989+0000] {spark_submit.py:495} INFO - "triggerExecution" : 227
[2024-11-02T07:55:22.989+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:22.990+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:55:22.990+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:55:22.990+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:55:22.990+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:55:22.990+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:22.991+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:22.991+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:22.991+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:22.991+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:55:22.991+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:22.991+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:22.992+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:22.992+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:22.992+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:55:22.992+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:22.992+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:22.993+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:22.993+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:22.993+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:22.993+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:22.993+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:22.993+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:55:22.994+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:55:22.994+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:55:22.994+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:55:22.994+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:22.994+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:55:22.995+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:55:22.995+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:55:22.995+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:55:22.995+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:22.995+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:32.996+0000] {spark_submit.py:495} INFO - 24/11/02 07:55:32 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:55:32.998+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:55:32.998+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:55:33.000+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:55:33.004+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:55:32.934Z",
[2024-11-02T07:55:33.007+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:55:33.015+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:33.016+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:33.016+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:33.016+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:55:33.016+0000] {spark_submit.py:495} INFO - "latestOffset" : 48,
[2024-11-02T07:55:33.016+0000] {spark_submit.py:495} INFO - "triggerExecution" : 49
[2024-11-02T07:55:33.017+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:33.017+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:55:33.017+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:55:33.017+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:55:33.018+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:55:33.018+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:33.018+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:33.018+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:33.018+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:33.019+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:55:33.019+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:33.019+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:33.019+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:33.019+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:33.020+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:55:33.020+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:33.020+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:33.020+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:33.020+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:33.021+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:33.021+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:33.021+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:33.021+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:55:33.021+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:55:33.022+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:55:33.022+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:55:33.022+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:33.022+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:55:33.023+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:55:33.023+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:55:33.023+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:55:33.023+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:33.023+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:42.999+0000] {spark_submit.py:495} INFO - 24/11/02 07:55:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:55:42.999+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:55:43.002+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:55:43.003+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:55:43.004+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:55:42.977Z",
[2024-11-02T07:55:43.005+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:55:43.006+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:43.006+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:43.006+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:43.006+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:55:43.006+0000] {spark_submit.py:495} INFO - "latestOffset" : 13,
[2024-11-02T07:55:43.007+0000] {spark_submit.py:495} INFO - "triggerExecution" : 13
[2024-11-02T07:55:43.007+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:43.007+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:55:43.007+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:55:43.007+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:55:43.008+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:55:43.008+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:43.008+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:43.008+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:43.008+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:43.009+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:55:43.009+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:43.009+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:43.009+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:43.009+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:43.010+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:55:43.010+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:43.010+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:43.010+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:43.010+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:43.011+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:43.011+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:43.011+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:43.011+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:55:43.011+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:55:43.012+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:55:43.012+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:55:43.012+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:43.012+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:55:43.012+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:55:43.013+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:55:43.013+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:55:43.013+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:43.013+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:53.013+0000] {spark_submit.py:495} INFO - 24/11/02 07:55:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:55:53.023+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:55:53.024+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:55:53.024+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:55:53.024+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:55:52.971Z",
[2024-11-02T07:55:53.035+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:55:53.035+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:53.035+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:53.035+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:53.036+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:55:53.036+0000] {spark_submit.py:495} INFO - "latestOffset" : 28,
[2024-11-02T07:55:53.036+0000] {spark_submit.py:495} INFO - "triggerExecution" : 28
[2024-11-02T07:55:53.036+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:53.036+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:55:53.036+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:55:53.037+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:55:53.037+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:55:53.037+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:53.037+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:53.037+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:53.037+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:53.038+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:55:53.038+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:53.038+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:53.038+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:53.038+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:53.038+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:55:53.039+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:55:53.039+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:55:53.039+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:53.039+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:55:53.039+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:55:53.039+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:55:53.040+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:55:53.040+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:55:53.040+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:55:53.040+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:55:53.040+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:55:53.041+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:53.041+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:55:53.041+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:55:53.041+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:55:53.041+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:55:53.042+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:55:53.042+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:03.056+0000] {spark_submit.py:495} INFO - 24/11/02 07:56:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:56:03.062+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:56:03.063+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:56:03.076+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:56:03.076+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:56:03.006Z",
[2024-11-02T07:56:03.077+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:56:03.077+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:03.077+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:03.077+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:03.077+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:56:03.078+0000] {spark_submit.py:495} INFO - "latestOffset" : 24,
[2024-11-02T07:56:03.078+0000] {spark_submit.py:495} INFO - "triggerExecution" : 24
[2024-11-02T07:56:03.078+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:03.078+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:56:03.107+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:56:03.108+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:56:03.108+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:56:03.108+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:03.108+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:03.108+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:03.115+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:03.125+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:56:03.125+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:03.125+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:03.126+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:03.126+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:03.126+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:56:03.126+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:03.126+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:03.127+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:03.127+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:03.127+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:03.127+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:03.127+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:03.128+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:56:03.128+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:56:03.128+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:56:03.128+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:56:03.128+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:03.129+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:56:03.129+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:56:03.129+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:56:03.129+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:56:03.130+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:03.130+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:13.051+0000] {spark_submit.py:495} INFO - 24/11/02 07:56:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:56:13.053+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:56:13.057+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:56:13.057+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:56:13.057+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:56:12.962Z",
[2024-11-02T07:56:13.058+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:56:13.061+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:13.061+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:13.062+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:13.064+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:56:13.065+0000] {spark_submit.py:495} INFO - "latestOffset" : 85,
[2024-11-02T07:56:13.065+0000] {spark_submit.py:495} INFO - "triggerExecution" : 85
[2024-11-02T07:56:13.065+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:13.066+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:56:13.068+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:56:13.068+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:56:13.069+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:56:13.070+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:13.070+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:13.070+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:13.070+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:13.071+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:56:13.071+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:13.071+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:13.071+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:13.075+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:13.075+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:56:13.078+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:13.079+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:13.079+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:13.079+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:13.080+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:13.082+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:13.083+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:13.085+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:56:13.085+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:56:13.086+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:56:13.087+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:56:13.087+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:13.087+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:56:13.087+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:56:13.088+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:56:13.092+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:56:13.092+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:13.092+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:23.259+0000] {spark_submit.py:495} INFO - 24/11/02 07:56:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:56:23.259+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:56:23.260+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:56:23.260+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:56:23.260+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:56:23.063Z",
[2024-11-02T07:56:23.260+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:56:23.261+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:23.261+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:23.261+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:23.261+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:56:23.261+0000] {spark_submit.py:495} INFO - "latestOffset" : 65,
[2024-11-02T07:56:23.262+0000] {spark_submit.py:495} INFO - "triggerExecution" : 65
[2024-11-02T07:56:23.286+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:23.287+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:56:23.287+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:56:23.287+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:56:23.287+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:56:23.287+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:23.288+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:23.322+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:23.325+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:23.327+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:56:23.328+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:23.328+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:23.328+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:23.328+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:23.328+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:56:23.329+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:23.329+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:23.329+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:23.329+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:23.329+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:23.329+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:23.330+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:56:23.331+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:56:23.331+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:56:23.331+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:56:23.331+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:23.331+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:33.198+0000] {spark_submit.py:495} INFO - 24/11/02 07:56:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:56:33.199+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:56:33.199+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:56:33.200+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:56:33.200+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:56:33.117Z",
[2024-11-02T07:56:33.200+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:56:33.201+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:33.201+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:33.202+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:33.202+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:56:33.203+0000] {spark_submit.py:495} INFO - "latestOffset" : 60,
[2024-11-02T07:56:33.203+0000] {spark_submit.py:495} INFO - "triggerExecution" : 60
[2024-11-02T07:56:33.204+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:33.204+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:56:33.204+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:56:33.205+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:56:33.205+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:56:33.205+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:33.205+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:33.206+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:33.206+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:33.206+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:56:33.207+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:33.207+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:33.207+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:33.208+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:33.208+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:56:33.208+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:33.209+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:33.209+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:33.209+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:33.210+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:33.210+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:33.211+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:33.211+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:56:33.211+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:56:33.212+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:56:33.212+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:56:33.212+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:33.213+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:56:33.213+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:56:33.213+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:56:33.213+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:56:33.214+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:33.214+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:43.351+0000] {spark_submit.py:495} INFO - 24/11/02 07:56:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:56:43.352+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:56:43.352+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:56:43.352+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:56:43.353+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:56:43.144Z",
[2024-11-02T07:56:43.353+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:56:43.353+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:43.354+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:43.354+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:43.354+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:56:43.355+0000] {spark_submit.py:495} INFO - "latestOffset" : 188,
[2024-11-02T07:56:43.355+0000] {spark_submit.py:495} INFO - "triggerExecution" : 191
[2024-11-02T07:56:43.355+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:43.355+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:56:43.356+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:56:43.356+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:56:43.356+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:56:43.356+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:43.356+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:43.357+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:43.357+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:43.357+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:56:43.357+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:43.358+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:43.358+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:43.358+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:43.358+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:56:43.359+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:43.359+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:43.359+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:43.359+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:43.360+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:43.360+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:43.360+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:43.360+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:56:43.361+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:56:43.361+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:56:43.361+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:56:43.361+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:43.361+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:56:43.400+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:56:43.403+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:56:43.426+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:56:43.432+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:43.433+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:53.853+0000] {spark_submit.py:495} INFO - 24/11/02 07:56:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:56:53.854+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:56:53.854+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:56:53.854+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:56:53.854+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:56:53.148Z",
[2024-11-02T07:56:53.862+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:56:53.862+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:53.863+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:53.863+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:53.863+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:56:53.863+0000] {spark_submit.py:495} INFO - "latestOffset" : 697,
[2024-11-02T07:56:53.864+0000] {spark_submit.py:495} INFO - "triggerExecution" : 698
[2024-11-02T07:56:53.864+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:53.865+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:56:53.865+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:56:53.865+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:56:53.865+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:56:53.866+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:53.867+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:53.883+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:53.883+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:53.884+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:56:53.884+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:53.884+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:53.884+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:53.885+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:53.885+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:56:53.885+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:56:53.885+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:56:53.885+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:53.886+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:56:53.886+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:56:53.886+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:56:53.886+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:56:53.886+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:56:53.886+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:56:53.887+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:56:53.887+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:56:53.887+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:53.887+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:56:53.888+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:56:53.888+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:56:53.888+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:56:53.889+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:56:53.889+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:00.961+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:00 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2024-11-02T07:57:04.082+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:57:04.082+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:57:04.083+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:57:04.083+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:57:04.084+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:57:03.519Z",
[2024-11-02T07:57:04.084+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:57:04.084+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:04.085+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:04.085+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:04.085+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:57:04.086+0000] {spark_submit.py:495} INFO - "latestOffset" : 559,
[2024-11-02T07:57:04.086+0000] {spark_submit.py:495} INFO - "triggerExecution" : 560
[2024-11-02T07:57:04.086+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:04.087+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:57:04.088+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:57:04.089+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:57:04.089+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:57:04.089+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:04.090+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:04.090+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:04.091+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:04.091+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:57:04.092+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:04.092+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:04.093+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:04.094+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:04.095+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:57:04.096+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:04.096+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:04.097+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:04.097+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:04.097+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:04.098+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:04.098+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:04.099+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:57:04.099+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:57:04.100+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:57:04.100+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:57:04.101+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:04.102+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:57:04.102+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:57:04.103+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:57:04.103+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:57:04.104+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:04.105+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:14.109+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:57:14.110+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:57:14.110+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:57:14.111+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:57:14.111+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:57:14.059Z",
[2024-11-02T07:57:14.111+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:57:14.112+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:14.112+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:14.112+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:14.112+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:57:14.113+0000] {spark_submit.py:495} INFO - "latestOffset" : 22,
[2024-11-02T07:57:14.113+0000] {spark_submit.py:495} INFO - "triggerExecution" : 23
[2024-11-02T07:57:14.113+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:14.113+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:57:14.114+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:57:14.114+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:57:14.114+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:57:14.114+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:14.115+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:14.115+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:14.115+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:14.115+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:57:14.116+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:14.116+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:14.116+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:14.117+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:14.197+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:57:14.209+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:14.209+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:14.209+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:14.255+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:14.255+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:14.256+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:14.256+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:14.256+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:57:14.258+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:57:14.263+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:57:14.263+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:57:14.264+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:14.264+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:57:14.264+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:57:14.264+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:57:14.265+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:57:14.265+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:14.265+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:24.119+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:24 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:57:24.124+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:57:24.128+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:57:24.129+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:57:24.129+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:57:24.074Z",
[2024-11-02T07:57:24.129+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:57:24.129+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:24.129+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:24.130+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:24.130+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:57:24.130+0000] {spark_submit.py:495} INFO - "latestOffset" : 37,
[2024-11-02T07:57:24.130+0000] {spark_submit.py:495} INFO - "triggerExecution" : 37
[2024-11-02T07:57:24.130+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:24.131+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:57:24.131+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:57:24.146+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:57:24.146+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:57:24.146+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:24.149+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:24.151+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:24.152+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:24.153+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:57:24.153+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:24.154+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:24.154+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:24.154+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:24.154+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:57:24.155+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:24.155+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:24.155+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:24.155+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:24.155+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:24.156+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:24.156+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:24.156+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:57:24.156+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:57:24.156+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:57:24.157+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:57:24.157+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:24.157+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:57:24.157+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:57:24.157+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:57:24.158+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:57:24.158+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:24.158+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:34.147+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:57:34.150+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:57:34.152+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:57:34.153+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:57:34.164+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:57:34.023Z",
[2024-11-02T07:57:34.166+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:57:34.167+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:34.167+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:34.168+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:34.168+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:57:34.168+0000] {spark_submit.py:495} INFO - "latestOffset" : 100,
[2024-11-02T07:57:34.168+0000] {spark_submit.py:495} INFO - "triggerExecution" : 100
[2024-11-02T07:57:34.169+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:34.169+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:57:34.169+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:57:34.172+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:57:34.172+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:57:34.173+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:34.175+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:34.175+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:34.175+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:34.176+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:57:34.176+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:34.176+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:34.176+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:34.176+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:34.177+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:57:34.177+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:34.177+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:34.177+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:34.178+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:34.178+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:34.178+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:34.178+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:34.179+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:57:34.179+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:57:34.179+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:57:34.179+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:57:34.180+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:34.180+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:57:34.180+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:57:34.180+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:57:34.181+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:57:34.181+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:34.181+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:44.154+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:57:44.155+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:57:44.156+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:57:44.156+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:57:44.156+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:57:44.131Z",
[2024-11-02T07:57:44.156+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:57:44.156+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:44.157+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:44.157+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:44.157+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:57:44.157+0000] {spark_submit.py:495} INFO - "latestOffset" : 19,
[2024-11-02T07:57:44.157+0000] {spark_submit.py:495} INFO - "triggerExecution" : 19
[2024-11-02T07:57:44.157+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:44.158+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:57:44.158+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:57:44.158+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:57:44.167+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:57:44.168+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:44.169+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:44.170+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:44.170+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:44.170+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:57:44.170+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:44.170+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:44.170+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:44.171+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:44.173+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:57:44.174+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:44.175+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:44.176+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:44.176+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:44.177+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:44.179+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:44.179+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:44.181+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:57:44.183+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:57:44.187+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:57:44.189+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:57:44.190+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:44.193+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:57:44.194+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:57:44.195+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:57:44.197+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:57:44.198+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:44.198+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:54.194+0000] {spark_submit.py:495} INFO - 24/11/02 07:57:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:57:54.200+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:57:54.201+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:57:54.204+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:57:54.205+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:57:54.136Z",
[2024-11-02T07:57:54.209+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:57:54.209+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:54.210+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:54.211+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:54.211+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:57:54.212+0000] {spark_submit.py:495} INFO - "latestOffset" : 34,
[2024-11-02T07:57:54.212+0000] {spark_submit.py:495} INFO - "triggerExecution" : 34
[2024-11-02T07:57:54.212+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:54.212+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:57:54.212+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:57:54.217+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:57:54.221+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:57:54.225+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:54.225+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:54.226+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:54.226+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:54.226+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:57:54.226+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:54.226+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:54.227+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:54.227+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:54.227+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:57:54.227+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:57:54.227+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:57:54.227+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:54.228+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:57:54.234+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:57:54.234+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:57:54.235+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:57:54.240+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:57:54.243+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:57:54.245+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:57:54.248+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:57:54.251+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:54.251+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:57:54.253+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:57:54.256+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:57:54.258+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:57:54.259+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:57:54.260+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:04.220+0000] {spark_submit.py:495} INFO - 24/11/02 07:58:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:58:04.220+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:58:04.225+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:58:04.227+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:58:04.227+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:58:04.147Z",
[2024-11-02T07:58:04.227+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:58:04.227+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:04.228+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:04.228+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:04.228+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:58:04.228+0000] {spark_submit.py:495} INFO - "latestOffset" : 66,
[2024-11-02T07:58:04.228+0000] {spark_submit.py:495} INFO - "triggerExecution" : 66
[2024-11-02T07:58:04.228+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:04.229+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:04.230+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:04.230+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:58:04.230+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:04.230+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:04.230+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:04.230+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:04.231+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:58:04.231+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:04.231+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:04.231+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:04.231+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:04.231+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:04.232+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:04.232+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:04.232+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:58:04.232+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:58:04.232+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:58:04.232+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:58:04.233+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:04.233+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:58:04.233+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:58:04.233+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:58:04.233+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:58:04.233+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:04.234+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:14.289+0000] {spark_submit.py:495} INFO - 24/11/02 07:58:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:58:14.296+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:58:14.307+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:58:14.308+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:58:14.308+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:58:14.085Z",
[2024-11-02T07:58:14.308+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:58:14.308+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:14.309+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:14.309+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:14.309+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:58:14.360+0000] {spark_submit.py:495} INFO - "latestOffset" : 135,
[2024-11-02T07:58:14.360+0000] {spark_submit.py:495} INFO - "triggerExecution" : 135
[2024-11-02T07:58:14.361+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:14.361+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:58:14.362+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:58:14.362+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:58:14.363+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:58:14.363+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:14.363+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:14.363+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:14.364+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:14.364+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:58:14.364+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:14.364+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:14.365+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:14.365+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:14.365+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:58:14.365+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:14.366+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:14.366+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:14.367+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:14.367+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:14.367+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:14.368+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:14.368+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:58:14.369+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:58:14.369+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:58:14.369+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:58:14.369+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:14.370+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:58:14.370+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:58:14.370+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:58:14.371+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:58:14.371+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:14.371+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:24.289+0000] {spark_submit.py:495} INFO - 24/11/02 07:58:24 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:58:24.293+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:58:24.294+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:58:24.295+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:58:24.295+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:58:24.268Z",
[2024-11-02T07:58:24.296+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:58:24.296+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:24.296+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:24.296+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:24.297+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:58:24.297+0000] {spark_submit.py:495} INFO - "latestOffset" : 19,
[2024-11-02T07:58:24.297+0000] {spark_submit.py:495} INFO - "triggerExecution" : 19
[2024-11-02T07:58:24.297+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:24.297+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:58:24.298+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:58:24.298+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:58:24.298+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:58:24.298+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:24.298+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:24.299+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:24.299+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:24.299+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:58:24.299+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:24.299+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:24.299+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:24.300+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:24.300+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:58:24.300+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:24.300+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:24.300+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:24.301+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:24.301+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:24.301+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:24.301+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:24.301+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:58:24.302+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:58:24.302+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:58:24.302+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:58:24.302+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:24.302+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:58:24.303+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:58:24.303+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:58:24.303+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:58:24.303+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:24.303+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:34.588+0000] {spark_submit.py:495} INFO - 24/11/02 07:58:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:58:34.590+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:58:34.592+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:58:34.594+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:58:34.594+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:58:34.470Z",
[2024-11-02T07:58:34.595+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:58:34.595+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:34.596+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:34.596+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:34.596+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:58:34.596+0000] {spark_submit.py:495} INFO - "latestOffset" : 110,
[2024-11-02T07:58:34.597+0000] {spark_submit.py:495} INFO - "triggerExecution" : 111
[2024-11-02T07:58:34.601+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:34.602+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:58:34.603+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:58:34.604+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:58:34.605+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:58:34.607+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:34.607+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:34.609+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:34.611+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:34.612+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:58:34.612+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:34.613+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:34.614+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:34.615+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:34.616+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:58:34.618+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:34.618+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:34.618+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:34.618+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:34.618+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:34.619+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:34.619+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:34.619+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:58:34.619+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:58:34.620+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:58:34.620+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:58:34.622+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:34.622+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:58:34.622+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:58:34.622+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:58:34.623+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:58:34.623+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:34.623+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:45.152+0000] {spark_submit.py:495} INFO - 24/11/02 07:58:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:58:45.172+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:58:45.172+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:58:45.173+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:58:45.173+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:58:44.503Z",
[2024-11-02T07:58:45.179+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:58:45.185+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:45.239+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:45.258+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:45.259+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:58:45.263+0000] {spark_submit.py:495} INFO - "latestOffset" : 525,
[2024-11-02T07:58:45.272+0000] {spark_submit.py:495} INFO - "triggerExecution" : 525
[2024-11-02T07:58:45.272+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:45.273+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:58:45.273+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:58:45.273+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:58:45.273+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:58:45.292+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:45.293+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:45.294+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:45.294+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:45.294+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:58:45.295+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:45.295+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:45.296+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:45.296+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:45.296+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:58:45.297+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:45.297+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:45.297+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:45.298+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:45.298+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:45.298+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:45.299+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:45.299+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:58:45.299+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:58:45.300+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:58:45.300+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:58:45.300+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:45.301+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:58:45.301+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:58:45.302+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:58:45.302+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:58:45.302+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:45.303+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:55.115+0000] {spark_submit.py:495} INFO - 24/11/02 07:58:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:58:55.116+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:58:55.126+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:58:55.127+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:58:55.128+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:58:54.986Z",
[2024-11-02T07:58:55.129+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:58:55.129+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:55.130+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:55.131+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:55.132+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:58:55.146+0000] {spark_submit.py:495} INFO - "latestOffset" : 116,
[2024-11-02T07:58:55.147+0000] {spark_submit.py:495} INFO - "triggerExecution" : 116
[2024-11-02T07:58:55.156+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:55.157+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:58:55.162+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:58:55.176+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:58:55.183+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:58:55.184+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:55.185+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:55.185+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:55.186+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:55.186+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:58:55.187+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:55.198+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:55.201+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:55.202+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:55.205+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:58:55.209+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:58:55.216+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:58:55.234+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:55.237+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:58:55.238+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:58:55.239+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:58:55.239+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:58:55.240+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:58:55.245+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:58:55.251+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:58:55.252+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:58:55.266+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:55.266+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:58:55.267+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:58:55.268+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:58:55.269+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:58:55.269+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:58:55.270+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:05.313+0000] {spark_submit.py:495} INFO - 24/11/02 07:59:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:59:05.322+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:59:05.336+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:59:05.337+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:59:05.337+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:59:05.111Z",
[2024-11-02T07:59:05.338+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:59:05.338+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:05.339+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:05.340+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:05.364+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:59:05.365+0000] {spark_submit.py:495} INFO - "latestOffset" : 140,
[2024-11-02T07:59:05.366+0000] {spark_submit.py:495} INFO - "triggerExecution" : 141
[2024-11-02T07:59:05.371+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:05.371+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:59:05.402+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:59:05.403+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:59:05.404+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:59:05.404+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:05.405+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:05.406+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:05.427+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:05.428+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:59:05.429+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:05.430+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:05.430+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:05.431+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:05.431+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:59:05.454+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:05.455+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:05.455+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:05.456+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:05.457+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:05.457+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:05.495+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:05.537+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:59:05.538+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:59:05.539+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:59:05.540+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:59:05.621+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:05.622+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:59:05.623+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:59:05.624+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:59:05.625+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:59:05.626+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:05.627+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:15.350+0000] {spark_submit.py:495} INFO - 24/11/02 07:59:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:59:15.352+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:59:15.462+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:59:15.463+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:59:15.558+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:59:15.070Z",
[2024-11-02T07:59:15.559+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:59:15.560+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:15.561+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:15.562+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:15.563+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:59:15.564+0000] {spark_submit.py:495} INFO - "latestOffset" : 169,
[2024-11-02T07:59:15.565+0000] {spark_submit.py:495} INFO - "triggerExecution" : 170
[2024-11-02T07:59:15.566+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:15.804+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:59:15.839+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:59:15.901+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:59:15.902+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:59:15.908+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:15.909+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:15.909+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:15.910+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:15.910+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:59:15.911+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:15.911+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:15.912+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:15.912+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:15.913+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:59:15.914+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:15.914+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:15.915+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:15.915+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:15.916+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:15.917+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:15.917+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:15.918+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:59:15.919+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:59:15.920+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:59:15.920+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:59:15.921+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:15.921+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:59:15.922+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:59:15.922+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:59:15.923+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:59:15.923+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:15.924+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:25.343+0000] {spark_submit.py:495} INFO - 24/11/02 07:59:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:59:25.345+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:59:25.346+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:59:25.348+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:59:25.349+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:59:25.224Z",
[2024-11-02T07:59:25.350+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:59:25.360+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:25.361+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:25.366+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:25.367+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:59:25.368+0000] {spark_submit.py:495} INFO - "latestOffset" : 82,
[2024-11-02T07:59:25.369+0000] {spark_submit.py:495} INFO - "triggerExecution" : 83
[2024-11-02T07:59:25.370+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:25.371+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:59:25.372+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:59:25.372+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:59:25.380+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:59:25.382+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:25.392+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:25.394+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:25.397+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:25.401+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:59:25.402+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:25.403+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:25.404+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:25.404+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:25.405+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:59:25.406+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:25.407+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:25.407+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:25.408+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:25.408+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:25.409+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:25.409+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:25.410+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:59:25.410+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:59:25.411+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:59:25.411+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:59:25.411+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:25.412+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:59:25.412+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:59:25.413+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:59:25.413+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:59:25.414+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:25.414+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:35.383+0000] {spark_submit.py:495} INFO - 24/11/02 07:59:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:59:35.385+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:59:35.388+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:59:35.388+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:59:35.388+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:59:35.239Z",
[2024-11-02T07:59:35.389+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:59:35.389+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:35.389+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:35.389+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:35.390+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:59:35.390+0000] {spark_submit.py:495} INFO - "latestOffset" : 124,
[2024-11-02T07:59:35.390+0000] {spark_submit.py:495} INFO - "triggerExecution" : 125
[2024-11-02T07:59:35.390+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:35.391+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:59:35.391+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:59:35.391+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:59:35.391+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:59:35.391+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:35.392+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:35.392+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:35.392+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:35.393+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:59:35.393+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:35.394+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:35.394+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:35.395+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:35.395+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:59:35.396+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:35.396+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:35.397+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:35.397+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:35.398+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:35.398+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:35.398+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:35.399+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:59:35.399+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:59:35.399+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:59:35.400+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:59:35.400+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:35.400+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:59:35.401+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:59:35.401+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:59:35.401+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:59:35.401+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:35.402+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:45.402+0000] {spark_submit.py:495} INFO - 24/11/02 07:59:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:59:45.403+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:59:45.419+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:59:45.419+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:59:45.419+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:59:45.354Z",
[2024-11-02T07:59:45.420+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:59:45.420+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:45.420+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:45.420+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:45.421+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:59:45.421+0000] {spark_submit.py:495} INFO - "latestOffset" : 26,
[2024-11-02T07:59:45.421+0000] {spark_submit.py:495} INFO - "triggerExecution" : 26
[2024-11-02T07:59:45.421+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:45.422+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:59:45.422+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:59:45.422+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:59:45.422+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:59:45.422+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:45.423+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:45.423+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:45.423+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:45.423+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:59:45.423+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:45.424+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:45.424+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:45.424+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:45.424+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:59:45.425+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:45.425+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:45.425+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:45.425+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:45.425+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:45.425+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:45.426+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:45.426+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:59:45.426+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:59:45.426+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:59:45.426+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:45.427+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:55.426+0000] {spark_submit.py:495} INFO - 24/11/02 07:59:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T07:59:55.435+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T07:59:55.436+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T07:59:55.437+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T07:59:55.437+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T07:59:55.326Z",
[2024-11-02T07:59:55.437+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T07:59:55.438+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:55.438+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:55.438+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:55.438+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T07:59:55.439+0000] {spark_submit.py:495} INFO - "latestOffset" : 82,
[2024-11-02T07:59:55.439+0000] {spark_submit.py:495} INFO - "triggerExecution" : 82
[2024-11-02T07:59:55.439+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:55.439+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T07:59:55.440+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T07:59:55.440+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T07:59:55.440+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T07:59:55.441+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:55.441+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:55.441+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:55.441+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:55.442+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T07:59:55.442+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:55.442+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:55.442+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:55.442+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:55.443+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T07:59:55.443+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T07:59:55.443+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T07:59:55.443+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:55.443+0000] {spark_submit.py:495} INFO - },
[2024-11-02T07:59:55.444+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T07:59:55.444+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T07:59:55.444+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T07:59:55.444+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T07:59:55.444+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T07:59:55.445+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T07:59:55.445+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T07:59:55.445+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:55.445+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T07:59:55.446+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T07:59:55.446+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T07:59:55.446+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T07:59:55.446+0000] {spark_submit.py:495} INFO - }
[2024-11-02T07:59:55.447+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:05.481+0000] {spark_submit.py:495} INFO - 24/11/02 08:00:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:00:05.501+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:00:05.502+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:00:05.502+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:00:05.502+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:00:05.310Z",
[2024-11-02T08:00:05.503+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:00:05.503+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:05.503+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:05.503+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:05.504+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:00:05.504+0000] {spark_submit.py:495} INFO - "latestOffset" : 136,
[2024-11-02T08:00:05.504+0000] {spark_submit.py:495} INFO - "triggerExecution" : 136
[2024-11-02T08:00:05.504+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:05.505+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:00:05.505+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:00:05.505+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:00:05.505+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:00:05.506+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:05.506+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:05.506+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:05.506+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:05.507+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:00:05.507+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:05.507+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:05.507+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:05.508+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:05.508+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:00:05.508+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:05.508+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:05.536+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:05.540+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:05.541+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:05.541+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:05.541+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:05.541+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:00:05.542+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:00:05.542+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:00:05.542+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:00:05.542+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:05.542+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:00:05.543+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:00:05.543+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:00:05.543+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:00:05.543+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:05.543+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:15.506+0000] {spark_submit.py:495} INFO - 24/11/02 08:00:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:00:15.507+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:00:15.508+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:00:15.512+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:00:15.513+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:00:15.310Z",
[2024-11-02T08:00:15.513+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:00:15.513+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:15.513+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:15.514+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:15.514+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:00:15.514+0000] {spark_submit.py:495} INFO - "latestOffset" : 158,
[2024-11-02T08:00:15.514+0000] {spark_submit.py:495} INFO - "triggerExecution" : 158
[2024-11-02T08:00:15.514+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:15.515+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:00:15.515+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:00:15.515+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:00:15.515+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:00:15.515+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:15.516+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:15.516+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:15.516+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:15.516+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:00:15.516+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:15.517+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:15.517+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:15.517+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:15.517+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:00:15.518+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:15.518+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:15.518+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:15.518+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:15.522+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:15.523+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:15.524+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:15.525+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:00:15.525+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:00:15.525+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:00:15.525+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:00:15.526+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:15.526+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:00:15.526+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:00:15.526+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:00:15.526+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:00:15.527+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:15.527+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:25.477+0000] {spark_submit.py:495} INFO - 24/11/02 08:00:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:00:25.477+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:00:25.478+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:00:25.478+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:00:25.478+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:00:25.462Z",
[2024-11-02T08:00:25.478+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:00:25.479+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:25.479+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:25.479+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:25.479+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:00:25.479+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-02T08:00:25.480+0000] {spark_submit.py:495} INFO - "triggerExecution" : 12
[2024-11-02T08:00:25.480+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:25.480+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:00:25.480+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:00:25.480+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:00:25.481+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:00:25.481+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:25.481+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:25.481+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:25.481+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:25.482+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:00:25.482+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:25.482+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:25.482+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:25.482+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:25.483+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:00:25.483+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:25.483+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:25.483+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:25.484+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:25.484+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:25.484+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:25.484+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:25.484+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:00:25.484+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:00:25.485+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:00:25.485+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:00:25.485+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:25.485+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:00:25.485+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:00:25.486+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:00:25.486+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:00:25.486+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:25.486+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:35.495+0000] {spark_submit.py:495} INFO - 24/11/02 08:00:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:00:35.496+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:00:35.497+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:00:35.497+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:00:35.498+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:00:35.470Z",
[2024-11-02T08:00:35.498+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:00:35.498+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:35.499+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:35.499+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:35.499+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:00:35.500+0000] {spark_submit.py:495} INFO - "latestOffset" : 20,
[2024-11-02T08:00:35.500+0000] {spark_submit.py:495} INFO - "triggerExecution" : 20
[2024-11-02T08:00:35.500+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:35.500+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:00:35.501+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:00:35.501+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:00:35.501+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:00:35.501+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:35.502+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:35.502+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:35.502+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:35.502+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:00:35.504+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:35.508+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:35.511+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:35.518+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:35.518+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:00:35.519+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:35.519+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:35.519+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:35.519+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:35.520+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:35.520+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:35.520+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:35.521+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:00:35.521+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:00:35.527+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:00:35.531+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:00:35.532+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:35.536+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:00:35.537+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:00:35.538+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:00:35.539+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:00:35.540+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:35.540+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:45.624+0000] {spark_submit.py:495} INFO - 24/11/02 08:00:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:00:45.625+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:00:45.625+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:00:45.626+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:00:45.626+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:00:45.503Z",
[2024-11-02T08:00:45.644+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:00:45.645+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:45.645+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:45.645+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:45.645+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:00:45.645+0000] {spark_submit.py:495} INFO - "latestOffset" : 62,
[2024-11-02T08:00:45.675+0000] {spark_submit.py:495} INFO - "triggerExecution" : 62
[2024-11-02T08:00:45.675+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:45.691+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:00:45.695+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:00:45.696+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:00:45.696+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:00:45.696+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:45.696+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:45.696+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:45.697+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:45.697+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:00:45.697+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:45.697+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:45.697+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:45.697+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:45.698+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:00:45.698+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:45.698+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:45.698+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:45.699+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:45.699+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:45.699+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:45.699+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:45.699+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:00:45.699+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:00:45.700+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:00:45.700+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:00:45.700+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:45.700+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:00:45.701+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:00:45.701+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:00:45.701+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:00:45.702+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:45.702+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:55.617+0000] {spark_submit.py:495} INFO - 24/11/02 08:00:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:00:55.622+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:00:55.652+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:00:55.654+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:00:55.654+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:00:55.507Z",
[2024-11-02T08:00:55.655+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:00:55.655+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:55.655+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:55.655+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:55.655+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:00:55.656+0000] {spark_submit.py:495} INFO - "latestOffset" : 86,
[2024-11-02T08:00:55.656+0000] {spark_submit.py:495} INFO - "triggerExecution" : 86
[2024-11-02T08:00:55.656+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:55.656+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:00:55.656+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:00:55.657+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:00:55.657+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:00:55.695+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:55.699+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:55.700+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:55.700+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:55.700+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:00:55.700+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:55.700+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:55.700+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:55.701+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:55.701+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:00:55.753+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:00:55.754+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:00:55.770+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:55.776+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:00:55.803+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:00:55.803+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:00:55.814+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:00:55.815+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:00:55.815+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:00:55.815+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:00:55.816+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:00:55.816+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:55.816+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:00:55.836+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:00:55.837+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:00:55.837+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:00:55.837+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:00:55.848+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:05.716+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:01:05.721+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:01:05.722+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:01:05.722+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:01:05.722+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:01:05.568Z",
[2024-11-02T08:01:05.722+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:01:05.723+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:01:05.723+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:01:05.737+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:01:05.737+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:01:05.737+0000] {spark_submit.py:495} INFO - "latestOffset" : 86,
[2024-11-02T08:01:05.738+0000] {spark_submit.py:495} INFO - "triggerExecution" : 86
[2024-11-02T08:01:05.738+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:05.738+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:01:05.738+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:01:05.738+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:01:05.739+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:01:05.739+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:05.755+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:05.755+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:05.756+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:05.756+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:01:05.758+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:05.759+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:05.759+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:05.760+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:05.760+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:01:05.761+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:05.761+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:05.761+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:05.761+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:05.762+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:01:05.762+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:01:05.762+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:01:05.762+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:01:05.762+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:01:05.763+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:01:05.763+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:01:05.763+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:05.763+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:01:05.763+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:01:05.764+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:01:05.764+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:01:05.764+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:05.764+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:16.969+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:01:17.034+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:01:17.034+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:01:17.034+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:01:17.035+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:01:15.415Z",
[2024-11-02T08:01:17.035+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:01:17.035+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:01:17.036+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:01:17.036+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:01:17.036+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:01:17.110+0000] {spark_submit.py:495} INFO - "latestOffset" : 1285,
[2024-11-02T08:01:17.141+0000] {spark_submit.py:495} INFO - "triggerExecution" : 1285
[2024-11-02T08:01:17.142+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:17.142+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:01:17.142+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:01:17.142+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:01:17.143+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:01:17.143+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:17.143+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:17.143+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:17.143+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:17.144+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:01:17.144+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:17.144+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:17.144+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:17.144+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:17.145+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:01:17.145+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:17.145+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:17.145+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:17.145+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:17.146+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:01:17.146+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:01:17.146+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:01:17.146+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:01:17.147+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:01:17.147+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:01:17.147+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:01:17.147+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:17.148+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:01:17.148+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:01:17.148+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:01:17.148+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:01:17.148+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:17.149+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:19.716+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/1 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.1.fd5c29fa-7082-4a13-80d6-92da212ba5ff.tmp
[2024-11-02T08:01:21.630+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.1.fd5c29fa-7082-4a13-80d6-92da212ba5ff.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/1
[2024-11-02T08:01:21.640+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:21 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1730534479462,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:01:22.291+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:22.341+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:22.418+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:22.497+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:23.763+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:23.825+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:24.398+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:01:24.456+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:01:24.478+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:01:24.516+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:01:24.533+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:01:24.534+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:01:24.581+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:01:24.783+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.3 KiB, free 434.4 MiB)
[2024-11-02T08:01:25.012+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:01:25.030+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:01:25.041+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:01:25.051+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:01:25.058+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-11-02T08:01:25.062+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:01:25.605+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:01:52.202+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 27141 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:01:52.211+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 27.666 s
[2024-11-02T08:01:52.212+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:01:52.213+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-11-02T08:01:52.214+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-11-02T08:01:52.239+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 27.762448 s
[2024-11-02T08:01:52.258+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:01:52.298+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:01:52.299+0000] {spark_submit.py:495} INFO - Batch: 1
[2024-11-02T08:01:52.299+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:01:53.178+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:01:53.223+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:01:53.364+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:53 INFO CodeGenerator: Code generated in 28.69511 ms
[2024-11-02T08:01:54.837+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:54 INFO CodeGenerator: Code generated in 953.977216 ms
[2024-11-02T08:01:54.969+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:01:55.038+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:01:55.038+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:01:55.038+0000] {spark_submit.py:495} INFO - |1        |29.86      |50.64   |2024-11-02 08:01:18|
[2024-11-02T08:01:55.038+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:01:55.039+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:01:55.039+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:01:55.954+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/1 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.1.7db49d53-1a76-4f66-8392-883803027ab7.tmp
[2024-11-02T08:01:56.934+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.1.7db49d53-1a76-4f66-8392-883803027ab7.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/1
[2024-11-02T08:01:56.988+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:01:56.992+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:01:56.992+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:01:56.993+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:01:56.993+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:01:19.323Z",
[2024-11-02T08:01:56.993+0000] {spark_submit.py:495} INFO - "batchId" : 1,
[2024-11-02T08:01:56.993+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-02T08:01:56.993+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.976190476190476,
[2024-11-02T08:01:56.994+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.02658443215652914,
[2024-11-02T08:01:56.994+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:01:57.013+0000] {spark_submit.py:495} INFO - "addBatch" : 32647,
[2024-11-02T08:01:57.014+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1926,
[2024-11-02T08:01:57.014+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:01:57.014+0000] {spark_submit.py:495} INFO - "latestOffset" : 139,
[2024-11-02T08:01:57.015+0000] {spark_submit.py:495} INFO - "queryPlanning" : 690,
[2024-11-02T08:01:57.015+0000] {spark_submit.py:495} INFO - "triggerExecution" : 37616,
[2024-11-02T08:01:57.015+0000] {spark_submit.py:495} INFO - "walCommit" : 2172
[2024-11-02T08:01:57.015+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:57.016+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:01:57.016+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:01:57.016+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:01:57.016+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:01:57.016+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:57.017+0000] {spark_submit.py:495} INFO - "0" : 6024
[2024-11-02T08:01:57.017+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:57.017+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:57.018+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:01:57.018+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:57.018+0000] {spark_submit.py:495} INFO - "0" : 6025
[2024-11-02T08:01:57.018+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:57.018+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:57.019+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:01:57.019+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:01:57.019+0000] {spark_submit.py:495} INFO - "0" : 6025
[2024-11-02T08:01:57.019+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:57.019+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:01:57.020+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-02T08:01:57.020+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 2.976190476190476,
[2024-11-02T08:01:57.020+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.02658443215652914,
[2024-11-02T08:01:57.020+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:01:57.021+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:01:57.021+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:01:57.021+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:01:57.021+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:57.021+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:01:57.022+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:01:57.022+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:01:57.022+0000] {spark_submit.py:495} INFO - "numOutputRows" : 1
[2024-11-02T08:01:57.022+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:57.022+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:01:57.376+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/2 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.2.3adbd73e-f9b0-4f6b-914d-a8c71894ebee.tmp
[2024-11-02T08:01:58.070+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.2.3adbd73e-f9b0-4f6b-914d-a8c71894ebee.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/2
[2024-11-02T08:01:58.089+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1730534517211,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:01:58.236+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:58.275+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:58.357+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:58.382+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:58.477+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:58.604+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:01:58.783+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:01:58.787+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:01:58.789+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:01:58.794+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:01:58.795+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:01:58.805+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:01:58.816+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:01:58.824+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 23.3 KiB, free 434.4 MiB)
[2024-11-02T08:01:58.894+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:01:58.917+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:01:58.957+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:01:58.957+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:01:58.958+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-11-02T08:01:58.970+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:01:59.360+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7d12808d40db:43395 in memory (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T08:01:59.443+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:01:59.574+0000] {spark_submit.py:495} INFO - 24/11/02 08:01:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.22.0.4:37367 in memory (size: 2036.0 B, free: 434.4 MiB)
[2024-11-02T08:02:00.549+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1532 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:02:00.560+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-11-02T08:02:00.564+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.762 s
[2024-11-02T08:02:00.635+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:02:00.644+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-11-02T08:02:00.647+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.859183 s
[2024-11-02T08:02:00.667+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:02:00.669+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:00.670+0000] {spark_submit.py:495} INFO - Batch: 2
[2024-11-02T08:02:00.670+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:01.557+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:01.561+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:02:01.561+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:01.562+0000] {spark_submit.py:495} INFO - |1        |29.22      |50.53   |2024-11-02 08:01:19|
[2024-11-02T08:02:01.562+0000] {spark_submit.py:495} INFO - |2        |29.96      |50.87   |2024-11-02 08:01:20|
[2024-11-02T08:02:01.562+0000] {spark_submit.py:495} INFO - |2        |29.35      |50.2    |2024-11-02 08:01:20|
[2024-11-02T08:02:01.563+0000] {spark_submit.py:495} INFO - |3        |29.19      |50.95   |2024-11-02 08:01:21|
[2024-11-02T08:02:01.563+0000] {spark_submit.py:495} INFO - |3        |29.85      |50.12   |2024-11-02 08:01:22|
[2024-11-02T08:02:01.563+0000] {spark_submit.py:495} INFO - |4        |29.92      |50.17   |2024-11-02 08:01:22|
[2024-11-02T08:02:01.565+0000] {spark_submit.py:495} INFO - |4        |29.3       |50.42   |2024-11-02 08:01:23|
[2024-11-02T08:02:01.565+0000] {spark_submit.py:495} INFO - |5        |29.04      |50.71   |2024-11-02 08:01:24|
[2024-11-02T08:02:01.566+0000] {spark_submit.py:495} INFO - |5        |29.82      |50.47   |2024-11-02 08:01:24|
[2024-11-02T08:02:01.566+0000] {spark_submit.py:495} INFO - |1        |29.11      |50.6    |2024-11-02 08:01:25|
[2024-11-02T08:02:01.566+0000] {spark_submit.py:495} INFO - |1        |29.7       |50.42   |2024-11-02 08:01:25|
[2024-11-02T08:02:01.567+0000] {spark_submit.py:495} INFO - |2        |29.71      |50.9    |2024-11-02 08:01:26|
[2024-11-02T08:02:01.567+0000] {spark_submit.py:495} INFO - |2        |29.08      |50.03   |2024-11-02 08:01:26|
[2024-11-02T08:02:01.571+0000] {spark_submit.py:495} INFO - |3        |29.1       |50.5    |2024-11-02 08:01:27|
[2024-11-02T08:02:01.571+0000] {spark_submit.py:495} INFO - |3        |29.6       |50.14   |2024-11-02 08:01:27|
[2024-11-02T08:02:01.571+0000] {spark_submit.py:495} INFO - |4        |29.13      |50.9    |2024-11-02 08:01:29|
[2024-11-02T08:02:01.572+0000] {spark_submit.py:495} INFO - |4        |29.27      |50.11   |2024-11-02 08:01:29|
[2024-11-02T08:02:01.572+0000] {spark_submit.py:495} INFO - |5        |29.59      |50.39   |2024-11-02 08:01:30|
[2024-11-02T08:02:01.572+0000] {spark_submit.py:495} INFO - |5        |29.32      |50.18   |2024-11-02 08:01:30|
[2024-11-02T08:02:01.572+0000] {spark_submit.py:495} INFO - |1        |29.67      |50.35   |2024-11-02 08:01:31|
[2024-11-02T08:02:01.573+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:01.573+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2024-11-02T08:02:01.573+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:02:01.605+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:02:01.926+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/2 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.2.78036cc9-8f99-49bd-ab1b-cae78267a252.tmp
[2024-11-02T08:02:02.824+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.2.78036cc9-8f99-49bd-ab1b-cae78267a252.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/2
[2024-11-02T08:02:02.824+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:02:02.825+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:02:02.825+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:02:02.825+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:02:02.826+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:01:56.977Z",
[2024-11-02T08:02:02.826+0000] {spark_submit.py:495} INFO - "batchId" : 2,
[2024-11-02T08:02:02.826+0000] {spark_submit.py:495} INFO - "numInputRows" : 65,
[2024-11-02T08:02:02.827+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7262442237212512,
[2024-11-02T08:02:02.827+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 11.143493913937938,
[2024-11-02T08:02:02.827+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:02:02.827+0000] {spark_submit.py:495} INFO - "addBatch" : 3273,
[2024-11-02T08:02:02.827+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1233,
[2024-11-02T08:02:02.828+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:02:02.828+0000] {spark_submit.py:495} INFO - "latestOffset" : 232,
[2024-11-02T08:02:02.828+0000] {spark_submit.py:495} INFO - "queryPlanning" : 196,
[2024-11-02T08:02:02.828+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5833,
[2024-11-02T08:02:02.829+0000] {spark_submit.py:495} INFO - "walCommit" : 887
[2024-11-02T08:02:02.831+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:02.832+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:02:02.832+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:02:02.839+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:02:02.839+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:02:02.844+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:02.844+0000] {spark_submit.py:495} INFO - "0" : 6025
[2024-11-02T08:02:02.845+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:02.846+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:02.847+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:02:02.848+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:02.849+0000] {spark_submit.py:495} INFO - "0" : 6090
[2024-11-02T08:02:02.849+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:02.849+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:02.850+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:02:02.850+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:02.850+0000] {spark_submit.py:495} INFO - "0" : 6090
[2024-11-02T08:02:02.850+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:02.850+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:02.851+0000] {spark_submit.py:495} INFO - "numInputRows" : 65,
[2024-11-02T08:02:02.851+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7262442237212512,
[2024-11-02T08:02:02.851+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 11.143493913937938,
[2024-11-02T08:02:02.851+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:02:02.852+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:02:02.852+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:02:02.852+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:02:02.852+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:02.852+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:02:02.853+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:02:02.853+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:02:02.853+0000] {spark_submit.py:495} INFO - "numOutputRows" : 65
[2024-11-02T08:02:02.853+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:02.853+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:03.171+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/3 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.3.78bf64fe-fc85-4ad5-9c2a-5d7f33087d18.tmp
[2024-11-02T08:02:03.801+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.3.78bf64fe-fc85-4ad5-9c2a-5d7f33087d18.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/3
[2024-11-02T08:02:03.809+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:03 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1730534522871,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:02:03.809+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:03.944+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:04.027+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:04.092+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:04.229+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:04.264+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:04.390+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:02:04.399+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:02:04.400+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:02:04.400+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:02:04.400+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:02:04.400+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:02:04.401+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:02:04.401+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:02:04.452+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:02:04.456+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:04.473+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:02:04.489+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:02:04.490+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-11-02T08:02:04.491+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:02:04.791+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:04.863+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:05.084+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:06.352+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1865 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:02:06.360+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-11-02T08:02:06.383+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.009 s
[2024-11-02T08:02:06.390+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:02:06.403+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-11-02T08:02:06.407+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.069035 s
[2024-11-02T08:02:06.414+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:02:06.420+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:06.428+0000] {spark_submit.py:495} INFO - Batch: 3
[2024-11-02T08:02:06.429+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:06.800+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:06.800+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:02:06.800+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:06.800+0000] {spark_submit.py:495} INFO - |4        |29.98      |50.03   |2024-11-02 08:01:57|
[2024-11-02T08:02:06.801+0000] {spark_submit.py:495} INFO - |4        |29.08      |50.9    |2024-11-02 08:01:58|
[2024-11-02T08:02:06.801+0000] {spark_submit.py:495} INFO - |5        |29.05      |50.99   |2024-11-02 08:01:59|
[2024-11-02T08:02:06.801+0000] {spark_submit.py:495} INFO - |5        |29.57      |50.61   |2024-11-02 08:01:59|
[2024-11-02T08:02:06.801+0000] {spark_submit.py:495} INFO - |1        |29.38      |50.68   |2024-11-02 08:02:00|
[2024-11-02T08:02:06.801+0000] {spark_submit.py:495} INFO - |1        |29.7       |50.93   |2024-11-02 08:02:00|
[2024-11-02T08:02:06.801+0000] {spark_submit.py:495} INFO - |2        |29.08      |50.76   |2024-11-02 08:02:01|
[2024-11-02T08:02:06.802+0000] {spark_submit.py:495} INFO - |2        |29.04      |50.44   |2024-11-02 08:02:01|
[2024-11-02T08:02:06.802+0000] {spark_submit.py:495} INFO - |3        |29.97      |50.58   |2024-11-02 08:02:02|
[2024-11-02T08:02:06.802+0000] {spark_submit.py:495} INFO - |3        |29.13      |50.84   |2024-11-02 08:02:02|
[2024-11-02T08:02:06.802+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:06.802+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:02:06.802+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:02:07.003+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/3 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.3.9925ae24-2b5c-434a-ad99-187031564b65.tmp
[2024-11-02T08:02:07.900+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.3.9925ae24-2b5c-434a-ad99-187031564b65.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/3
[2024-11-02T08:02:07.937+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:07 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:02:07.938+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:02:07.938+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:02:07.939+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:02:07.949+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:02:02.823Z",
[2024-11-02T08:02:07.950+0000] {spark_submit.py:495} INFO - "batchId" : 3,
[2024-11-02T08:02:07.962+0000] {spark_submit.py:495} INFO - "numInputRows" : 10,
[2024-11-02T08:02:07.962+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7105713308244954,
[2024-11-02T08:02:07.963+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.9704433497536946,
[2024-11-02T08:02:07.963+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:02:07.963+0000] {spark_submit.py:495} INFO - "addBatch" : 2854,
[2024-11-02T08:02:07.963+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1095,
[2024-11-02T08:02:07.963+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:02:07.964+0000] {spark_submit.py:495} INFO - "latestOffset" : 48,
[2024-11-02T08:02:07.964+0000] {spark_submit.py:495} INFO - "queryPlanning" : 194,
[2024-11-02T08:02:07.964+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5075,
[2024-11-02T08:02:07.964+0000] {spark_submit.py:495} INFO - "walCommit" : 881
[2024-11-02T08:02:07.965+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:07.965+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:02:07.965+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:02:07.965+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:02:07.965+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:02:07.986+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:07.986+0000] {spark_submit.py:495} INFO - "0" : 6090
[2024-11-02T08:02:07.986+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:07.987+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:07.987+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:02:07.987+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:07.987+0000] {spark_submit.py:495} INFO - "0" : 6100
[2024-11-02T08:02:07.987+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:07.988+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:07.988+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:02:07.988+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:07.988+0000] {spark_submit.py:495} INFO - "0" : 6100
[2024-11-02T08:02:07.988+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:07.989+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:07.989+0000] {spark_submit.py:495} INFO - "numInputRows" : 10,
[2024-11-02T08:02:07.989+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.7105713308244954,
[2024-11-02T08:02:07.989+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.9704433497536946,
[2024-11-02T08:02:07.989+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:02:07.990+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:02:07.990+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:02:08.028+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:02:08.028+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:08.028+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:02:08.029+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:02:08.029+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:02:08.029+0000] {spark_submit.py:495} INFO - "numOutputRows" : 10
[2024-11-02T08:02:08.029+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:08.029+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:08.336+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/4 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.4.8720178a-3a46-461e-8ac9-d997a47e3609.tmp
[2024-11-02T08:02:09.680+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.4.8720178a-3a46-461e-8ac9-d997a47e3609.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/4
[2024-11-02T08:02:09.681+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:09 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1730534527982,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:02:10.112+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:10.115+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:10.480+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:10.664+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:10.866+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:10.890+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:11.159+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:02:11.185+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:02:11.228+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:02:11.263+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:02:11.264+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:02:11.264+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:02:11.305+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:02:11.455+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:02:11.785+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:02:11.788+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:11.870+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:11.955+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:11.966+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:02:12.059+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:02:12.126+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-11-02T08:02:12.244+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:02:13.330+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:15.906+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3695 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:02:15.908+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 4.597 s
[2024-11-02T08:02:15.909+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:02:15.943+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-11-02T08:02:15.958+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-11-02T08:02:16.016+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 4.787867 s
[2024-11-02T08:02:16.024+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:02:16.031+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:16.032+0000] {spark_submit.py:495} INFO - Batch: 4
[2024-11-02T08:02:16.032+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:17.501+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:17.503+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:02:17.503+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:17.505+0000] {spark_submit.py:495} INFO - |4        |29.45      |50.82   |2024-11-02 08:02:03|
[2024-11-02T08:02:17.529+0000] {spark_submit.py:495} INFO - |4        |29.7       |50.31   |2024-11-02 08:02:03|
[2024-11-02T08:02:17.530+0000] {spark_submit.py:495} INFO - |5        |29.57      |50.83   |2024-11-02 08:02:04|
[2024-11-02T08:02:17.531+0000] {spark_submit.py:495} INFO - |5        |29.94      |50.88   |2024-11-02 08:02:04|
[2024-11-02T08:02:17.532+0000] {spark_submit.py:495} INFO - |1        |29.05      |50.54   |2024-11-02 08:02:05|
[2024-11-02T08:02:17.533+0000] {spark_submit.py:495} INFO - |1        |29.48      |50.19   |2024-11-02 08:02:05|
[2024-11-02T08:02:17.564+0000] {spark_submit.py:495} INFO - |2        |29.64      |50.92   |2024-11-02 08:02:07|
[2024-11-02T08:02:17.565+0000] {spark_submit.py:495} INFO - |2        |29.94      |50.31   |2024-11-02 08:02:07|
[2024-11-02T08:02:17.566+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:17.566+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:02:17.567+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:02:18.063+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/4 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.4.6101f683-fbed-4a1e-9cbd-f3f23617b310.tmp
[2024-11-02T08:02:18.949+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.4.6101f683-fbed-4a1e-9cbd-f3f23617b310.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/4
[2024-11-02T08:02:18.958+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:02:18.961+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:02:18.961+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:02:18.962+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:02:18.975+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:02:07.925Z",
[2024-11-02T08:02:18.976+0000] {spark_submit.py:495} INFO - "batchId" : 4,
[2024-11-02T08:02:18.976+0000] {spark_submit.py:495} INFO - "numInputRows" : 8,
[2024-11-02T08:02:18.976+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.5680125441003527,
[2024-11-02T08:02:18.976+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7263482840021791,
[2024-11-02T08:02:18.977+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:02:18.977+0000] {spark_submit.py:495} INFO - "addBatch" : 7380,
[2024-11-02T08:02:18.977+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1439,
[2024-11-02T08:02:18.977+0000] {spark_submit.py:495} INFO - "getBatch" : 16,
[2024-11-02T08:02:18.977+0000] {spark_submit.py:495} INFO - "latestOffset" : 54,
[2024-11-02T08:02:18.977+0000] {spark_submit.py:495} INFO - "queryPlanning" : 409,
[2024-11-02T08:02:18.978+0000] {spark_submit.py:495} INFO - "triggerExecution" : 11014,
[2024-11-02T08:02:18.978+0000] {spark_submit.py:495} INFO - "walCommit" : 1711
[2024-11-02T08:02:18.978+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:18.978+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:02:18.978+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:02:18.978+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:02:18.979+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:02:18.979+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:18.979+0000] {spark_submit.py:495} INFO - "0" : 6100
[2024-11-02T08:02:18.979+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:18.979+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:18.980+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:02:18.980+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:18.980+0000] {spark_submit.py:495} INFO - "0" : 6108
[2024-11-02T08:02:18.980+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:18.980+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:18.981+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:02:18.981+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:18.981+0000] {spark_submit.py:495} INFO - "0" : 6108
[2024-11-02T08:02:18.982+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:18.982+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:18.982+0000] {spark_submit.py:495} INFO - "numInputRows" : 8,
[2024-11-02T08:02:18.982+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.5680125441003527,
[2024-11-02T08:02:18.982+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7263482840021791,
[2024-11-02T08:02:18.983+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:02:18.983+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:02:18.983+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:02:18.983+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:02:18.983+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:18.984+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:02:18.984+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:02:18.984+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:02:18.984+0000] {spark_submit.py:495} INFO - "numOutputRows" : 8
[2024-11-02T08:02:18.984+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:18.985+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:19.257+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/5 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.5.84ee1a73-5d99-4f5f-bcaf-013866c9062d.tmp
[2024-11-02T08:02:21.719+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.5.84ee1a73-5d99-4f5f-bcaf-013866c9062d.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/5
[2024-11-02T08:02:21.720+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:21 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1730534538975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:02:22.090+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:22.109+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:22.271+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:22.375+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:22.502+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:22.506+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:02:22.833+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:02:22.839+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:02:22.884+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:02:22.885+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:02:22.916+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:02:23.010+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:02:23.011+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:02:23.011+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:02:23.693+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:02:23.701+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:23.944+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:23.946+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:24.066+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:02:24.099+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:02:24.101+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:24 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-11-02T08:02:24.169+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:24 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:02:27.543+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:02:29.385+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 5094 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:02:29.386+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-11-02T08:02:29.387+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 6.484 s
[2024-11-02T08:02:29.391+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:02:29.399+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-11-02T08:02:29.519+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 6.636355 s
[2024-11-02T08:02:29.562+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:02:29.581+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:29.582+0000] {spark_submit.py:495} INFO - Batch: 5
[2024-11-02T08:02:29.582+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:02:29.814+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:29.815+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:02:29.816+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:29.816+0000] {spark_submit.py:495} INFO - |3        |29.98      |50.56   |2024-11-02 08:02:08|
[2024-11-02T08:02:29.817+0000] {spark_submit.py:495} INFO - |3        |29.06      |50.18   |2024-11-02 08:02:08|
[2024-11-02T08:02:29.817+0000] {spark_submit.py:495} INFO - |4        |29.28      |50.08   |2024-11-02 08:02:09|
[2024-11-02T08:02:29.817+0000] {spark_submit.py:495} INFO - |4        |29.55      |50.68   |2024-11-02 08:02:09|
[2024-11-02T08:02:29.817+0000] {spark_submit.py:495} INFO - |5        |29.26      |50.44   |2024-11-02 08:02:10|
[2024-11-02T08:02:29.818+0000] {spark_submit.py:495} INFO - |5        |29.63      |50.62   |2024-11-02 08:02:10|
[2024-11-02T08:02:29.818+0000] {spark_submit.py:495} INFO - |1        |29.71      |50.89   |2024-11-02 08:02:11|
[2024-11-02T08:02:29.819+0000] {spark_submit.py:495} INFO - |1        |29.64      |50.03   |2024-11-02 08:02:11|
[2024-11-02T08:02:29.819+0000] {spark_submit.py:495} INFO - |2        |29.04      |50.44   |2024-11-02 08:02:12|
[2024-11-02T08:02:29.819+0000] {spark_submit.py:495} INFO - |2        |29.68      |50.83   |2024-11-02 08:02:12|
[2024-11-02T08:02:29.819+0000] {spark_submit.py:495} INFO - |3        |29.67      |50.35   |2024-11-02 08:02:14|
[2024-11-02T08:02:29.820+0000] {spark_submit.py:495} INFO - |3        |29.01      |50.85   |2024-11-02 08:02:14|
[2024-11-02T08:02:29.820+0000] {spark_submit.py:495} INFO - |4        |29.39      |50.35   |2024-11-02 08:02:15|
[2024-11-02T08:02:29.820+0000] {spark_submit.py:495} INFO - |4        |29.92      |50.04   |2024-11-02 08:02:15|
[2024-11-02T08:02:29.820+0000] {spark_submit.py:495} INFO - |5        |29.52      |50.34   |2024-11-02 08:02:16|
[2024-11-02T08:02:29.821+0000] {spark_submit.py:495} INFO - |5        |29.9       |50.24   |2024-11-02 08:02:17|
[2024-11-02T08:02:30.036+0000] {spark_submit.py:495} INFO - |1        |29.9       |50.6    |2024-11-02 08:02:18|
[2024-11-02T08:02:30.098+0000] {spark_submit.py:495} INFO - |1        |29.31      |50.58   |2024-11-02 08:02:18|
[2024-11-02T08:02:30.099+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:02:30.099+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:02:30.099+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:02:31.627+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/5 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.5.2e4ec7dc-04ec-4335-b8ee-aaf4e1744941.tmp
[2024-11-02T08:02:33.351+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.5.2e4ec7dc-04ec-4335-b8ee-aaf4e1744941.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/5
[2024-11-02T08:02:33.373+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:02:33.374+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:02:33.374+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:02:33.375+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:02:33.375+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:02:18.942Z",
[2024-11-02T08:02:33.375+0000] {spark_submit.py:495} INFO - "batchId" : 5,
[2024-11-02T08:02:33.386+0000] {spark_submit.py:495} INFO - "numInputRows" : 18,
[2024-11-02T08:02:33.389+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.6338386130525553,
[2024-11-02T08:02:33.390+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2488725456185388,
[2024-11-02T08:02:33.396+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:02:33.396+0000] {spark_submit.py:495} INFO - "addBatch" : 7893,
[2024-11-02T08:02:33.400+0000] {spark_submit.py:495} INFO - "commitOffsets" : 3485,
[2024-11-02T08:02:33.404+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:02:33.405+0000] {spark_submit.py:495} INFO - "latestOffset" : 32,
[2024-11-02T08:02:33.416+0000] {spark_submit.py:495} INFO - "queryPlanning" : 233,
[2024-11-02T08:02:33.416+0000] {spark_submit.py:495} INFO - "triggerExecution" : 14413,
[2024-11-02T08:02:33.416+0000] {spark_submit.py:495} INFO - "walCommit" : 2767
[2024-11-02T08:02:33.417+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:33.417+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:02:33.417+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:02:33.417+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:02:33.417+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:02:33.418+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:33.418+0000] {spark_submit.py:495} INFO - "0" : 6108
[2024-11-02T08:02:33.418+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:33.418+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:33.418+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:02:33.418+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:33.419+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:33.419+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:33.419+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:33.420+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:02:33.420+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:33.421+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:33.421+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:33.450+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:33.450+0000] {spark_submit.py:495} INFO - "numInputRows" : 18,
[2024-11-02T08:02:33.451+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.6338386130525553,
[2024-11-02T08:02:33.451+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2488725456185388,
[2024-11-02T08:02:33.451+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:02:33.451+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:02:33.451+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:02:33.452+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:02:33.452+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:33.452+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:02:33.483+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:02:33.484+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:02:33.484+0000] {spark_submit.py:495} INFO - "numOutputRows" : 18
[2024-11-02T08:02:33.484+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:33.484+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:43.471+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:02:43.472+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:02:43.472+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:02:43.472+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:02:43.473+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:02:43.360Z",
[2024-11-02T08:02:43.473+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-02T08:02:43.473+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:02:43.473+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:02:43.474+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:02:43.474+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:02:43.474+0000] {spark_submit.py:495} INFO - "latestOffset" : 105,
[2024-11-02T08:02:43.474+0000] {spark_submit.py:495} INFO - "triggerExecution" : 105
[2024-11-02T08:02:43.474+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:43.475+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:02:43.475+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:02:43.475+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:02:43.476+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:02:43.476+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:43.476+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:43.477+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:43.477+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:43.477+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:02:43.478+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:43.478+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:43.478+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:43.478+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:43.479+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:02:43.479+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:43.479+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:43.479+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:43.479+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:43.480+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:02:43.480+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:02:43.480+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:02:43.480+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:02:43.481+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:02:43.481+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:02:43.481+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:02:43.481+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:43.482+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:02:43.482+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:02:43.482+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:02:43.482+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:02:43.482+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:43.483+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:53.699+0000] {spark_submit.py:495} INFO - 24/11/02 08:02:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:02:53.726+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:02:53.765+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:02:53.765+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:02:53.765+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:02:53.410Z",
[2024-11-02T08:02:53.765+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-02T08:02:53.766+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:02:53.766+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:02:53.766+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:02:53.766+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:02:53.767+0000] {spark_submit.py:495} INFO - "latestOffset" : 261,
[2024-11-02T08:02:53.767+0000] {spark_submit.py:495} INFO - "triggerExecution" : 261
[2024-11-02T08:02:53.767+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:53.767+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:02:53.767+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:02:53.768+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:02:53.768+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:02:53.768+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:53.768+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:53.769+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:53.769+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:53.769+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:02:53.769+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:53.770+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:53.770+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:53.770+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:53.770+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:02:53.771+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:02:53.771+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:02:53.771+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:53.771+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:02:53.772+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:02:53.772+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:02:53.772+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:02:53.772+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:02:53.772+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:02:53.773+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:02:53.773+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:02:53.773+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:53.773+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:02:53.774+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:02:53.774+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:02:53.774+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:02:53.774+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:02:53.774+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:03.778+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:03.778+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:03.779+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:03.779+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:03.779+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:03.687Z",
[2024-11-02T08:03:03.779+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-02T08:03:03.779+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:03:03.780+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:03:03.780+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:03:03.780+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:03.780+0000] {spark_submit.py:495} INFO - "latestOffset" : 73,
[2024-11-02T08:03:03.780+0000] {spark_submit.py:495} INFO - "triggerExecution" : 73
[2024-11-02T08:03:03.793+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:03.794+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:03.794+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:03.795+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:03.795+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:03.795+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:03.795+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:03.795+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:03.796+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:03.796+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:03.796+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:03.796+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:03.796+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:03.797+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:03.797+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:03.797+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:03.798+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:03.798+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:03.798+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:03.798+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:03:03.798+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:03:03.799+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:03:03.799+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:03.799+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:03.799+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:03.799+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:03.800+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:03.800+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:03.800+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:03.800+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:03.801+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:03:03.801+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:03.801+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:13.888+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:13.893+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:13.894+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:13.894+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:13.895+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:13.748Z",
[2024-11-02T08:03:13.900+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-02T08:03:13.901+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:03:13.901+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:03:13.902+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:03:13.902+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:13.902+0000] {spark_submit.py:495} INFO - "latestOffset" : 100,
[2024-11-02T08:03:13.902+0000] {spark_submit.py:495} INFO - "triggerExecution" : 100
[2024-11-02T08:03:13.902+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:13.902+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:13.903+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:13.903+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:13.926+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:13.929+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:13.929+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:13.929+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:13.930+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:13.930+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:13.930+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:13.930+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:13.930+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:13.930+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:13.931+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:13.931+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:13.931+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:13.931+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:13.931+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:13.931+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:03:13.932+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:03:13.932+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:03:13.932+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:13.932+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:13.963+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:13.963+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:13.964+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:14.011+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:14.011+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:14.064+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:14.064+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:03:14.065+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:14.065+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:16.954+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/6 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.6.69ca662b-8767-4cf1-a3ae-1ab77343d4e4.tmp
[2024-11-02T08:03:17.311+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.6.69ca662b-8767-4cf1-a3ae-1ab77343d4e4.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/6
[2024-11-02T08:03:17.365+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1730534596807,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:17.564+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:17.565+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:17.647+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:17.669+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:17.777+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:17.812+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:17.914+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:18.191+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:18.404+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:18.414+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:18.433+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:18.435+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:18.436+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:18.466+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:18.475+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[28] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:18.488+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 23.3 KiB, free 434.4 MiB)
[2024-11-02T08:03:18.549+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:03:18.594+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:18.608+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:18.617+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:18.618+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-11-02T08:03:18.630+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:18.911+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:19.773+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1139 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:19.782+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-11-02T08:03:19.787+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 1.326 s
[2024-11-02T08:03:19.793+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:19.806+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-11-02T08:03:19.812+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 1.379082 s
[2024-11-02T08:03:19.813+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:19.813+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:19.813+0000] {spark_submit.py:495} INFO - Batch: 6
[2024-11-02T08:03:19.813+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:19.945+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:19.951+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:19.952+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:19.953+0000] {spark_submit.py:495} INFO - |1        |29.75      |50.47   |2024-11-02 08:03:16|
[2024-11-02T08:03:19.956+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:19.956+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:19.956+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:20.047+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/6 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.6.bfab7ace-f1d7-45b4-b654-8ea2c378f220.tmp
[2024-11-02T08:03:20.774+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.6.bfab7ace-f1d7-45b4-b654-8ea2c378f220.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/6
[2024-11-02T08:03:20.788+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:20 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:20.797+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:20.798+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:20.798+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:20.798+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:16.789Z",
[2024-11-02T08:03:20.798+0000] {spark_submit.py:495} INFO - "batchId" : 6,
[2024-11-02T08:03:20.798+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-02T08:03:20.798+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 43.47826086956522,
[2024-11-02T08:03:20.799+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.25043826696719257,
[2024-11-02T08:03:20.799+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:20.799+0000] {spark_submit.py:495} INFO - "addBatch" : 2389,
[2024-11-02T08:03:20.799+0000] {spark_submit.py:495} INFO - "commitOffsets" : 834,
[2024-11-02T08:03:20.799+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:03:20.799+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-02T08:03:20.800+0000] {spark_submit.py:495} INFO - "queryPlanning" : 186,
[2024-11-02T08:03:20.800+0000] {spark_submit.py:495} INFO - "triggerExecution" : 3993,
[2024-11-02T08:03:20.800+0000] {spark_submit.py:495} INFO - "walCommit" : 559
[2024-11-02T08:03:20.800+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:20.800+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:20.800+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:20.801+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:20.801+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:20.801+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:20.802+0000] {spark_submit.py:495} INFO - "0" : 6126
[2024-11-02T08:03:20.802+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:20.802+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:20.802+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:20.802+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:20.803+0000] {spark_submit.py:495} INFO - "0" : 6127
[2024-11-02T08:03:20.803+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:20.803+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:20.803+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:20.803+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:20.804+0000] {spark_submit.py:495} INFO - "0" : 6127
[2024-11-02T08:03:20.804+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:20.804+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:20.840+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-02T08:03:20.841+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 43.47826086956522,
[2024-11-02T08:03:20.850+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.25043826696719257,
[2024-11-02T08:03:20.854+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:20.857+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:20.860+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:20.861+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:20.861+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:20.862+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:20.862+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:20.862+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:20.862+0000] {spark_submit.py:495} INFO - "numOutputRows" : 1
[2024-11-02T08:03:20.863+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:20.863+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:20.968+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/7 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.7.7dee5dc7-97a2-41e1-b5fc-9f95475f0405.tmp
[2024-11-02T08:03:21.399+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.7.7dee5dc7-97a2-41e1-b5fc-9f95475f0405.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/7
[2024-11-02T08:03:21.399+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:21 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1730534600863,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:21.521+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:21.573+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:21.774+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:21.885+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:22.057+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:22.089+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:22.224+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:22.255+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:22.267+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:22.286+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:22.288+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:22.289+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:22.327+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:22.330+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:22.476+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:03:22.510+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:22.527+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:22.530+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:22.532+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:22.533+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:22.538+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-11-02T08:03:22.562+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:22.984+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:24.265+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1689 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:24.280+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-11-02T08:03:24.280+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 1.980 s
[2024-11-02T08:03:24.281+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:24.302+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-11-02T08:03:24.363+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 2.108758 s
[2024-11-02T08:03:24.374+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:24.375+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:24.421+0000] {spark_submit.py:495} INFO - Batch: 7
[2024-11-02T08:03:24.518+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:24.851+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:24.852+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:24.876+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:24.876+0000] {spark_submit.py:495} INFO - |2        |29.35      |50.47   |2024-11-02 08:03:17|
[2024-11-02T08:03:24.876+0000] {spark_submit.py:495} INFO - |3        |29.2       |50.49   |2024-11-02 08:03:18|
[2024-11-02T08:03:24.877+0000] {spark_submit.py:495} INFO - |4        |29.38      |50.15   |2024-11-02 08:03:19|
[2024-11-02T08:03:24.877+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:24.877+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:24.877+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:25.315+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/7 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.7.3eb6bff0-bd6b-4a18-bbf7-0324440582c3.tmp
[2024-11-02T08:03:26.310+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.7.3eb6bff0-bd6b-4a18-bbf7-0324440582c3.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/7
[2024-11-02T08:03:26.382+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:26.391+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:26.393+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:26.393+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:26.394+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:20.793Z",
[2024-11-02T08:03:26.394+0000] {spark_submit.py:495} INFO - "batchId" : 7,
[2024-11-02T08:03:26.394+0000] {spark_submit.py:495} INFO - "numInputRows" : 3,
[2024-11-02T08:03:26.413+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7492507492507493,
[2024-11-02T08:03:26.414+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.543773790103317,
[2024-11-02T08:03:26.414+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:26.414+0000] {spark_submit.py:495} INFO - "addBatch" : 3239,
[2024-11-02T08:03:26.415+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1456,
[2024-11-02T08:03:26.423+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:03:26.424+0000] {spark_submit.py:495} INFO - "latestOffset" : 70,
[2024-11-02T08:03:26.424+0000] {spark_submit.py:495} INFO - "queryPlanning" : 212,
[2024-11-02T08:03:26.425+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5517,
[2024-11-02T08:03:26.426+0000] {spark_submit.py:495} INFO - "walCommit" : 539
[2024-11-02T08:03:26.426+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:26.427+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:26.427+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:26.428+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:26.428+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:26.428+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:26.429+0000] {spark_submit.py:495} INFO - "0" : 6127
[2024-11-02T08:03:26.429+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:26.430+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:26.430+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:26.431+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:26.431+0000] {spark_submit.py:495} INFO - "0" : 6130
[2024-11-02T08:03:26.431+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:26.432+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:26.432+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:26.433+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:26.433+0000] {spark_submit.py:495} INFO - "0" : 6130
[2024-11-02T08:03:26.434+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:26.434+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:26.435+0000] {spark_submit.py:495} INFO - "numInputRows" : 3,
[2024-11-02T08:03:26.435+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7492507492507493,
[2024-11-02T08:03:26.435+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.543773790103317,
[2024-11-02T08:03:26.436+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:26.436+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:26.437+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:26.437+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:26.437+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:26.438+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:26.438+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:26.438+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:26.439+0000] {spark_submit.py:495} INFO - "numOutputRows" : 3
[2024-11-02T08:03:26.439+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:26.439+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:26.650+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/8 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.8.738f2c05-37ff-45ae-a193-93ee2f234af8.tmp
[2024-11-02T08:03:27.434+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.8.738f2c05-37ff-45ae-a193-93ee2f234af8.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/8
[2024-11-02T08:03:27.436+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1730534606431,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:27.551+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:27.568+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:27.771+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:27.787+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:27.948+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:27.955+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:28.096+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:28.107+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:28.126+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:28.138+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:28.141+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:28.178+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:28.178+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[36] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:28.217+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:28.278+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:03:28.297+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:28.315+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:28.395+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[36] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:28.396+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-11-02T08:03:28.409+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:28.455+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:28.457+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:28 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:29.139+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:30.482+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2070 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:30.516+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-11-02T08:03:30.616+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 2.348 s
[2024-11-02T08:03:30.617+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:30.618+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-11-02T08:03:30.716+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 2.581474 s
[2024-11-02T08:03:30.718+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:30.875+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:30.876+0000] {spark_submit.py:495} INFO - Batch: 8
[2024-11-02T08:03:30.876+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:31.210+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:31.210+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:31.211+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:31.211+0000] {spark_submit.py:495} INFO - |5        |29.62      |50.13   |2024-11-02 08:03:21|
[2024-11-02T08:03:31.213+0000] {spark_submit.py:495} INFO - |1        |29.78      |50.61   |2024-11-02 08:03:22|
[2024-11-02T08:03:31.217+0000] {spark_submit.py:495} INFO - |2        |29.96      |50.02   |2024-11-02 08:03:23|
[2024-11-02T08:03:31.261+0000] {spark_submit.py:495} INFO - |3        |29.92      |50.67   |2024-11-02 08:03:24|
[2024-11-02T08:03:31.261+0000] {spark_submit.py:495} INFO - |4        |29.75      |50.4    |2024-11-02 08:03:25|
[2024-11-02T08:03:31.262+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:31.262+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:31.262+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:32.011+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/8 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.8.603a69f0-8a05-44e6-b0f5-20bfaa3bfa3f.tmp
[2024-11-02T08:03:33.089+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.8.603a69f0-8a05-44e6-b0f5-20bfaa3bfa3f.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/8
[2024-11-02T08:03:33.108+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:33.111+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:33.127+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:33.133+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:33.133+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:26.338Z",
[2024-11-02T08:03:33.133+0000] {spark_submit.py:495} INFO - "batchId" : 8,
[2024-11-02T08:03:33.134+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:03:33.134+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9017132551848512,
[2024-11-02T08:03:33.134+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7412898443291327,
[2024-11-02T08:03:33.134+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:33.134+0000] {spark_submit.py:495} INFO - "addBatch" : 3589,
[2024-11-02T08:03:33.134+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1911,
[2024-11-02T08:03:33.135+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:03:33.135+0000] {spark_submit.py:495} INFO - "latestOffset" : 93,
[2024-11-02T08:03:33.135+0000] {spark_submit.py:495} INFO - "queryPlanning" : 147,
[2024-11-02T08:03:33.135+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6745,
[2024-11-02T08:03:33.135+0000] {spark_submit.py:495} INFO - "walCommit" : 1003
[2024-11-02T08:03:33.135+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:33.136+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:33.136+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:33.136+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:33.136+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:33.136+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:33.136+0000] {spark_submit.py:495} INFO - "0" : 6130
[2024-11-02T08:03:33.137+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:33.137+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:33.137+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:33.138+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:33.138+0000] {spark_submit.py:495} INFO - "0" : 6135
[2024-11-02T08:03:33.138+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:33.138+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:33.138+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:33.138+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:33.139+0000] {spark_submit.py:495} INFO - "0" : 6135
[2024-11-02T08:03:33.139+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:33.139+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:33.139+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:03:33.139+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9017132551848512,
[2024-11-02T08:03:33.139+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7412898443291327,
[2024-11-02T08:03:33.140+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:33.140+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:33.140+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:33.140+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:33.173+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:33.173+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:33.174+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:33.174+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:33.174+0000] {spark_submit.py:495} INFO - "numOutputRows" : 5
[2024-11-02T08:03:33.174+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:33.174+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:34.105+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/9 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.9.b37f9891-58e4-453d-baf4-fa8d54604a14.tmp
[2024-11-02T08:03:35.702+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.9.b37f9891-58e4-453d-baf4-fa8d54604a14.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/9
[2024-11-02T08:03:35.703+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:35 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1730534613403,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:35.819+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:35.942+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:36.081+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:36.266+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:36.806+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:36.846+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:37.199+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:37.201+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:37.201+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:37.202+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:37.202+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:37.202+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:37.232+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[40] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:37.353+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:37.440+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:03:37.523+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:37.684+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:37.746+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:37.815+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[40] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:37.851+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:37.902+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-11-02T08:03:37.903+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:37 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:38.219+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:39.361+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1482 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:39.380+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-11-02T08:03:39.385+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 2.106 s
[2024-11-02T08:03:39.404+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:39.404+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-11-02T08:03:39.455+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 2.248355 s
[2024-11-02T08:03:39.461+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:39.461+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:39.462+0000] {spark_submit.py:495} INFO - Batch: 9
[2024-11-02T08:03:39.462+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:39.846+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:39.852+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:39.853+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:39.853+0000] {spark_submit.py:495} INFO - |5        |29.18      |50.5    |2024-11-02 08:03:26|
[2024-11-02T08:03:39.854+0000] {spark_submit.py:495} INFO - |1        |29.86      |50.64   |2024-11-02 08:03:27|
[2024-11-02T08:03:39.854+0000] {spark_submit.py:495} INFO - |2        |29.58      |50.87   |2024-11-02 08:03:28|
[2024-11-02T08:03:39.854+0000] {spark_submit.py:495} INFO - |3        |29.45      |50.88   |2024-11-02 08:03:29|
[2024-11-02T08:03:39.854+0000] {spark_submit.py:495} INFO - |4        |29.29      |50.99   |2024-11-02 08:03:30|
[2024-11-02T08:03:39.854+0000] {spark_submit.py:495} INFO - |5        |29.68      |50.51   |2024-11-02 08:03:32|
[2024-11-02T08:03:39.854+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:39.855+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:39.855+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:40.016+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/9 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.9.63c52ac4-c604-4a2d-b8d1-7a20da0ab826.tmp
[2024-11-02T08:03:40.543+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.9.63c52ac4-c604-4a2d-b8d1-7a20da0ab826.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/9
[2024-11-02T08:03:40.543+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:40 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:40.544+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:40.544+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:40.544+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:40.544+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:33.108Z",
[2024-11-02T08:03:40.544+0000] {spark_submit.py:495} INFO - "batchId" : 9,
[2024-11-02T08:03:40.545+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:03:40.545+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8862629246676514,
[2024-11-02T08:03:40.545+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8086253369272237,
[2024-11-02T08:03:40.545+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:40.545+0000] {spark_submit.py:495} INFO - "addBatch" : 3901,
[2024-11-02T08:03:40.546+0000] {spark_submit.py:495} INFO - "commitOffsets" : 669,
[2024-11-02T08:03:40.546+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:03:40.546+0000] {spark_submit.py:495} INFO - "latestOffset" : 295,
[2024-11-02T08:03:40.546+0000] {spark_submit.py:495} INFO - "queryPlanning" : 239,
[2024-11-02T08:03:40.547+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7420,
[2024-11-02T08:03:40.547+0000] {spark_submit.py:495} INFO - "walCommit" : 2296
[2024-11-02T08:03:40.547+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:40.547+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:40.547+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:40.548+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:40.551+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:40.551+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:40.551+0000] {spark_submit.py:495} INFO - "0" : 6135
[2024-11-02T08:03:40.551+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:40.552+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:40.552+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:40.552+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:40.552+0000] {spark_submit.py:495} INFO - "0" : 6141
[2024-11-02T08:03:40.552+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:40.553+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:40.553+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:40.553+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:40.553+0000] {spark_submit.py:495} INFO - "0" : 6141
[2024-11-02T08:03:40.553+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:40.554+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:40.554+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:03:40.554+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8862629246676514,
[2024-11-02T08:03:40.554+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8086253369272237,
[2024-11-02T08:03:40.554+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:40.555+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:40.555+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:40.555+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:40.555+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:40.555+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:40.556+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:40.556+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:40.556+0000] {spark_submit.py:495} INFO - "numOutputRows" : 6
[2024-11-02T08:03:40.556+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:40.556+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:41.106+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/10 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.10.63a04bb5-8923-4e32-9b9a-bdbe5d6c1426.tmp
[2024-11-02T08:03:41.747+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.10.63a04bb5-8923-4e32-9b9a-bdbe5d6c1426.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/10
[2024-11-02T08:03:41.748+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:41 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1730534620684,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:41.921+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:41.945+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:42.001+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:42.015+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:42.212+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:42.296+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:42.417+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:42.417+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:42.480+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:42.499+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:42.528+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:42.618+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:42.620+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:42.742+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:42 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:43.173+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:03:43.242+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:43.243+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:43.301+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:43.301+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:43.302+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-11-02T08:03:43.315+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:43.713+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:43.887+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:43 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:44.580+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 887 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:44.603+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-11-02T08:03:44.609+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 1.982 s
[2024-11-02T08:03:44.631+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:44.633+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-11-02T08:03:44.634+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 2.208035 s
[2024-11-02T08:03:44.634+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:44.634+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:44.635+0000] {spark_submit.py:495} INFO - Batch: 10
[2024-11-02T08:03:44.635+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:45.040+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:45.056+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:45.056+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:45.056+0000] {spark_submit.py:495} INFO - |1        |29.31      |50.55   |2024-11-02 08:03:33|
[2024-11-02T08:03:45.074+0000] {spark_submit.py:495} INFO - |2        |29.02      |50.28   |2024-11-02 08:03:34|
[2024-11-02T08:03:45.109+0000] {spark_submit.py:495} INFO - |3        |29.89      |50.95   |2024-11-02 08:03:35|
[2024-11-02T08:03:45.137+0000] {spark_submit.py:495} INFO - |4        |29.67      |50.12   |2024-11-02 08:03:36|
[2024-11-02T08:03:45.200+0000] {spark_submit.py:495} INFO - |5        |29.14      |50.48   |2024-11-02 08:03:37|
[2024-11-02T08:03:45.201+0000] {spark_submit.py:495} INFO - |1        |29.04      |50.83   |2024-11-02 08:03:38|
[2024-11-02T08:03:45.210+0000] {spark_submit.py:495} INFO - |2        |29.04      |50.45   |2024-11-02 08:03:39|
[2024-11-02T08:03:45.211+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:45.211+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:45.212+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:45.261+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/10 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.10.2261a8f1-67ed-4ac2-a1b9-404c76b1bfc6.tmp
[2024-11-02T08:03:46.587+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.10.2261a8f1-67ed-4ac2-a1b9-404c76b1bfc6.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/10
[2024-11-02T08:03:46.598+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:46.609+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:46.630+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:46.650+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:46.650+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:40.540Z",
[2024-11-02T08:03:46.651+0000] {spark_submit.py:495} INFO - "batchId" : 10,
[2024-11-02T08:03:46.651+0000] {spark_submit.py:495} INFO - "numInputRows" : 7,
[2024-11-02T08:03:46.651+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9418729817007534,
[2024-11-02T08:03:46.651+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1577902745616937,
[2024-11-02T08:03:46.651+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:46.652+0000] {spark_submit.py:495} INFO - "addBatch" : 3079,
[2024-11-02T08:03:46.652+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1551,
[2024-11-02T08:03:46.652+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:03:46.652+0000] {spark_submit.py:495} INFO - "latestOffset" : 144,
[2024-11-02T08:03:46.653+0000] {spark_submit.py:495} INFO - "queryPlanning" : 263,
[2024-11-02T08:03:46.653+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6046,
[2024-11-02T08:03:46.653+0000] {spark_submit.py:495} INFO - "walCommit" : 1009
[2024-11-02T08:03:46.654+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:46.654+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:46.673+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:46.674+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:46.674+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:46.675+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:46.676+0000] {spark_submit.py:495} INFO - "0" : 6141
[2024-11-02T08:03:46.676+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:46.676+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:46.677+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:46.677+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:46.677+0000] {spark_submit.py:495} INFO - "0" : 6148
[2024-11-02T08:03:46.677+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:46.677+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:46.678+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:46.678+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:46.678+0000] {spark_submit.py:495} INFO - "0" : 6148
[2024-11-02T08:03:46.678+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:46.679+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:46.679+0000] {spark_submit.py:495} INFO - "numInputRows" : 7,
[2024-11-02T08:03:46.680+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9418729817007534,
[2024-11-02T08:03:46.680+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1577902745616937,
[2024-11-02T08:03:46.680+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:46.680+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:46.681+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:46.681+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:46.681+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:46.682+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:46.682+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:46.683+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:46.684+0000] {spark_submit.py:495} INFO - "numOutputRows" : 7
[2024-11-02T08:03:46.685+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:46.686+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:46.823+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/11 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.11.42cd2564-1211-455f-a577-70a1d97e6665.tmp
[2024-11-02T08:03:47.851+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.11.42cd2564-1211-455f-a577-70a1d97e6665.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/11
[2024-11-02T08:03:47.859+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:47 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1730534626696,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:47.938+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:47.950+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:47.994+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:48.021+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:48.071+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:48.090+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:48.150+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:48.151+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:48.156+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:48.156+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:48.156+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:48.165+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:48.182+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[48] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:48.233+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:48.512+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:03:48.562+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:48.648+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:48.683+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:48.688+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[48] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:48.708+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-11-02T08:03:48.709+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:48.709+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:48 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:49.245+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:49 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:50.614+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1918 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:50.636+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-11-02T08:03:50.649+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 2.457 s
[2024-11-02T08:03:50.663+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:50.693+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-11-02T08:03:50.704+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 2.523802 s
[2024-11-02T08:03:50.714+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:50.715+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:50.715+0000] {spark_submit.py:495} INFO - Batch: 11
[2024-11-02T08:03:50.715+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:50.874+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:50.901+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:50.903+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:50.905+0000] {spark_submit.py:495} INFO - |3        |29.02      |50.14   |2024-11-02 08:03:41|
[2024-11-02T08:03:50.911+0000] {spark_submit.py:495} INFO - |4        |29.98      |50.35   |2024-11-02 08:03:42|
[2024-11-02T08:03:50.912+0000] {spark_submit.py:495} INFO - |5        |29.12      |50.71   |2024-11-02 08:03:43|
[2024-11-02T08:03:50.922+0000] {spark_submit.py:495} INFO - |1        |29.7       |50.16   |2024-11-02 08:03:44|
[2024-11-02T08:03:50.922+0000] {spark_submit.py:495} INFO - |2        |29.65      |50.88   |2024-11-02 08:03:45|
[2024-11-02T08:03:50.922+0000] {spark_submit.py:495} INFO - |3        |29.2       |50.79   |2024-11-02 08:03:46|
[2024-11-02T08:03:50.923+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:50.923+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:50.923+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:51.094+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/11 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.11.c36a407f-221f-4f18-b8e3-dec1d7de00fc.tmp
[2024-11-02T08:03:51.868+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.11.c36a407f-221f-4f18-b8e3-dec1d7de00fc.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/11
[2024-11-02T08:03:51.942+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:51 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:51.947+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:51.948+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:51.948+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:51.948+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:46.598Z",
[2024-11-02T08:03:51.948+0000] {spark_submit.py:495} INFO - "batchId" : 11,
[2024-11-02T08:03:51.949+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:03:51.949+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9904258831297458,
[2024-11-02T08:03:51.949+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1378721790252229,
[2024-11-02T08:03:51.949+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:51.949+0000] {spark_submit.py:495} INFO - "addBatch" : 2909,
[2024-11-02T08:03:51.949+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1006,
[2024-11-02T08:03:51.950+0000] {spark_submit.py:495} INFO - "getBatch" : 2,
[2024-11-02T08:03:51.950+0000] {spark_submit.py:495} INFO - "latestOffset" : 97,
[2024-11-02T08:03:51.985+0000] {spark_submit.py:495} INFO - "queryPlanning" : 84,
[2024-11-02T08:03:51.985+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5273,
[2024-11-02T08:03:51.991+0000] {spark_submit.py:495} INFO - "walCommit" : 1153
[2024-11-02T08:03:51.993+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:52.005+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:52.005+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:52.005+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:52.005+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:52.006+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:52.006+0000] {spark_submit.py:495} INFO - "0" : 6148
[2024-11-02T08:03:52.006+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:52.006+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:52.006+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:52.007+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:52.007+0000] {spark_submit.py:495} INFO - "0" : 6154
[2024-11-02T08:03:52.007+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:52.007+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:52.007+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:52.008+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:52.008+0000] {spark_submit.py:495} INFO - "0" : 6154
[2024-11-02T08:03:52.008+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:52.008+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:52.009+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:03:52.009+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9904258831297458,
[2024-11-02T08:03:52.009+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1378721790252229,
[2024-11-02T08:03:52.009+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:52.009+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:52.020+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:52.026+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:52.026+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:52.026+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:52.027+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:52.027+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:52.027+0000] {spark_submit.py:495} INFO - "numOutputRows" : 6
[2024-11-02T08:03:52.027+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:52.027+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:52.470+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/12 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.12.6db651b7-147f-4baa-b7e2-b2124d0eebe7.tmp
[2024-11-02T08:03:53.161+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.12.6db651b7-147f-4baa-b7e2-b2124d0eebe7.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/12
[2024-11-02T08:03:53.161+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1730534632084,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:53.212+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:53.216+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:53.371+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:53.385+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:53.564+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:53.584+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:53.711+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:53.717+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:53.721+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:53.728+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:53.731+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:53.737+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:53.743+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[52] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:53.767+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:53.845+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:03:53.975+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:53.983+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:53.989+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:53.998+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:53.999+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[52] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:53.999+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-11-02T08:03:54.013+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:53 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:54.291+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:55.793+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 1790 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:03:55.811+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-11-02T08:03:55.839+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 2.088 s
[2024-11-02T08:03:55.840+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:03:55.841+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-11-02T08:03:55.851+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 2.129920 s
[2024-11-02T08:03:55.852+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:03:55.856+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:55.857+0000] {spark_submit.py:495} INFO - Batch: 12
[2024-11-02T08:03:55.858+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:03:56.163+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:56.170+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:03:56.178+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:56.180+0000] {spark_submit.py:495} INFO - |4        |29.55      |50.39   |2024-11-02 08:03:47|
[2024-11-02T08:03:56.180+0000] {spark_submit.py:495} INFO - |5        |29.19      |50.66   |2024-11-02 08:03:48|
[2024-11-02T08:03:56.180+0000] {spark_submit.py:495} INFO - |1        |29.25      |50.52   |2024-11-02 08:03:50|
[2024-11-02T08:03:56.195+0000] {spark_submit.py:495} INFO - |2        |29.33      |50.84   |2024-11-02 08:03:51|
[2024-11-02T08:03:56.197+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:03:56.212+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:03:56.227+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:03:56.412+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/12 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.12.2dc7b494-6735-459e-b3e8-14f5f29cd988.tmp
[2024-11-02T08:03:57.204+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.12.2dc7b494-6735-459e-b3e8-14f5f29cd988.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/12
[2024-11-02T08:03:57.220+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:03:57.228+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:03:57.229+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:03:57.229+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:03:57.234+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:51.938Z",
[2024-11-02T08:03:57.247+0000] {spark_submit.py:495} INFO - "batchId" : 12,
[2024-11-02T08:03:57.247+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-02T08:03:57.248+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7490636704119851,
[2024-11-02T08:03:57.248+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.759589821496392,
[2024-11-02T08:03:57.248+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:03:57.248+0000] {spark_submit.py:495} INFO - "addBatch" : 2886,
[2024-11-02T08:03:57.248+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1038,
[2024-11-02T08:03:57.249+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:03:57.249+0000] {spark_submit.py:495} INFO - "latestOffset" : 146,
[2024-11-02T08:03:57.249+0000] {spark_submit.py:495} INFO - "queryPlanning" : 134,
[2024-11-02T08:03:57.249+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5266,
[2024-11-02T08:03:57.249+0000] {spark_submit.py:495} INFO - "walCommit" : 1059
[2024-11-02T08:03:57.249+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:57.250+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:03:57.250+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:03:57.250+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:03:57.250+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:03:57.250+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:57.251+0000] {spark_submit.py:495} INFO - "0" : 6154
[2024-11-02T08:03:57.251+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:57.251+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:57.251+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:03:57.251+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:57.251+0000] {spark_submit.py:495} INFO - "0" : 6158
[2024-11-02T08:03:57.252+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:57.252+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:57.252+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:03:57.252+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:03:57.252+0000] {spark_submit.py:495} INFO - "0" : 6158
[2024-11-02T08:03:57.253+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:57.253+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:03:57.253+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-02T08:03:57.253+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7490636704119851,
[2024-11-02T08:03:57.253+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.759589821496392,
[2024-11-02T08:03:57.254+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:03:57.254+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:03:57.254+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:03:57.263+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:03:57.278+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:57.282+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:03:57.295+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:03:57.296+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:03:57.296+0000] {spark_submit.py:495} INFO - "numOutputRows" : 4
[2024-11-02T08:03:57.296+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:57.296+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:03:57.361+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/13 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.13.eafeb6f2-087e-4e5c-99e7-97c72cc5ce79.tmp
[2024-11-02T08:03:57.653+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.13.eafeb6f2-087e-4e5c-99e7-97c72cc5ce79.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/13
[2024-11-02T08:03:57.654+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1730534637263,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:03:57.700+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:57.719+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:57.805+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:57.820+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:57.890+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:57.912+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:03:58.127+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:03:58.142+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:03:58.143+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:03:58.143+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:03:58.143+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:03:58.146+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:03:58.146+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:03:58.146+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:03:58.230+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:03:58.236+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:58.259+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:03:58.260+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:03:58.261+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-11-02T08:03:58.300+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:58.333+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:03:58.363+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:03:58.839+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:58 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:00.008+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:59 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 1672 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:04:00.032+0000] {spark_submit.py:495} INFO - 24/11/02 08:03:59 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-11-02T08:04:00.032+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 1.888 s
[2024-11-02T08:04:00.032+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:04:00.032+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-11-02T08:04:00.039+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 1.924653 s
[2024-11-02T08:04:00.040+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:04:00.057+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:00.076+0000] {spark_submit.py:495} INFO - Batch: 13
[2024-11-02T08:04:00.083+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:00.427+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:00.442+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:04:00.443+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:00.444+0000] {spark_submit.py:495} INFO - |3        |29.11      |50.65   |2024-11-02 08:03:52|
[2024-11-02T08:04:00.444+0000] {spark_submit.py:495} INFO - |4        |29.98      |50.87   |2024-11-02 08:03:53|
[2024-11-02T08:04:00.444+0000] {spark_submit.py:495} INFO - |5        |29.87      |50.66   |2024-11-02 08:03:54|
[2024-11-02T08:04:00.444+0000] {spark_submit.py:495} INFO - |1        |29.16      |50.79   |2024-11-02 08:03:55|
[2024-11-02T08:04:00.444+0000] {spark_submit.py:495} INFO - |2        |29.78      |50.05   |2024-11-02 08:03:56|
[2024-11-02T08:04:00.445+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:00.445+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:04:00.445+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:04:00.604+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/13 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.13.72db7aa7-a001-40e0-a8cc-fbdaa08922f2.tmp
[2024-11-02T08:04:01.579+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.13.72db7aa7-a001-40e0-a8cc-fbdaa08922f2.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/13
[2024-11-02T08:04:01.588+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:01 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:01.589+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:01.589+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:01.590+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:01.590+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:03:57.214Z",
[2024-11-02T08:04:01.590+0000] {spark_submit.py:495} INFO - "batchId" : 13,
[2024-11-02T08:04:01.591+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:04:01.607+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9476876421531464,
[2024-11-02T08:04:01.608+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1478420569329661,
[2024-11-02T08:04:01.608+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:01.608+0000] {spark_submit.py:495} INFO - "addBatch" : 2682,
[2024-11-02T08:04:01.608+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1151,
[2024-11-02T08:04:01.608+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:04:01.609+0000] {spark_submit.py:495} INFO - "latestOffset" : 49,
[2024-11-02T08:04:01.609+0000] {spark_submit.py:495} INFO - "queryPlanning" : 78,
[2024-11-02T08:04:01.609+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4356,
[2024-11-02T08:04:01.626+0000] {spark_submit.py:495} INFO - "walCommit" : 391
[2024-11-02T08:04:01.626+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:01.626+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:01.627+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:01.627+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:01.627+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:01.627+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:01.628+0000] {spark_submit.py:495} INFO - "0" : 6158
[2024-11-02T08:04:01.628+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:01.628+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:01.628+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:01.628+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:01.629+0000] {spark_submit.py:495} INFO - "0" : 6163
[2024-11-02T08:04:01.629+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:01.629+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:01.629+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:01.641+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:01.641+0000] {spark_submit.py:495} INFO - "0" : 6163
[2024-11-02T08:04:01.641+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:01.650+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:01.650+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:04:01.652+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9476876421531464,
[2024-11-02T08:04:01.652+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1478420569329661,
[2024-11-02T08:04:01.652+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:01.652+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:01.653+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:01.653+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:01.653+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:01.653+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:01.653+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:01.654+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:01.654+0000] {spark_submit.py:495} INFO - "numOutputRows" : 5
[2024-11-02T08:04:01.654+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:01.654+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:01.898+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/14 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.14.9e9f6172-ed4b-475e-88d2-d6e8df374620.tmp
[2024-11-02T08:04:02.494+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.14.9e9f6172-ed4b-475e-88d2-d6e8df374620.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/14
[2024-11-02T08:04:02.505+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:02 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1730534641621,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:04:02.762+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:02.825+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:02.968+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:02.989+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:03.308+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:03.339+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:03.622+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:04:03.626+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:04:03.630+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:04:03.635+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:04:03.650+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:04:03.669+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:04:03.687+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:04:03.766+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:04:04.016+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:03 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:04:04.056+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:04.101+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:04.135+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:04:04.136+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:04.146+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[60] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:04:04.147+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-11-02T08:04:04.201+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:04:04.453+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:05.507+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 1328 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:04:05.534+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-11-02T08:04:05.552+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 1.856 s
[2024-11-02T08:04:05.566+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:04:05.566+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-11-02T08:04:05.582+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 1.959618 s
[2024-11-02T08:04:05.583+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:04:05.595+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:05.596+0000] {spark_submit.py:495} INFO - Batch: 14
[2024-11-02T08:04:05.596+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:05.986+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:05.989+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:04:05.990+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:05.991+0000] {spark_submit.py:495} INFO - |3        |29.92      |50.8    |2024-11-02 08:03:57|
[2024-11-02T08:04:05.991+0000] {spark_submit.py:495} INFO - |4        |29.65      |50.97   |2024-11-02 08:03:58|
[2024-11-02T08:04:05.991+0000] {spark_submit.py:495} INFO - |5        |29.49      |50.59   |2024-11-02 08:03:59|
[2024-11-02T08:04:05.991+0000] {spark_submit.py:495} INFO - |1        |29.4       |50.85   |2024-11-02 08:04:00|
[2024-11-02T08:04:05.992+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:05.992+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:04:06.000+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:04:06.140+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/14 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.14.73c4cb44-8a1f-4907-90a2-91eb8572f288.tmp
[2024-11-02T08:04:06.583+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.14.73c4cb44-8a1f-4907-90a2-91eb8572f288.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/14
[2024-11-02T08:04:06.595+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:06 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:06.595+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:06.595+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:06.596+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:06.596+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:01.583Z",
[2024-11-02T08:04:06.596+0000] {spark_submit.py:495} INFO - "batchId" : 14,
[2024-11-02T08:04:06.596+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-02T08:04:06.596+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9155413138017854,
[2024-11-02T08:04:06.597+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8079175924055747,
[2024-11-02T08:04:06.597+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:06.597+0000] {spark_submit.py:495} INFO - "addBatch" : 3152,
[2024-11-02T08:04:06.597+0000] {spark_submit.py:495} INFO - "commitOffsets" : 540,
[2024-11-02T08:04:06.597+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:04:06.597+0000] {spark_submit.py:495} INFO - "latestOffset" : 38,
[2024-11-02T08:04:06.598+0000] {spark_submit.py:495} INFO - "queryPlanning" : 331,
[2024-11-02T08:04:06.598+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4951,
[2024-11-02T08:04:06.598+0000] {spark_submit.py:495} INFO - "walCommit" : 863
[2024-11-02T08:04:06.598+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:06.598+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:06.599+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:06.599+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:06.599+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:06.599+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:06.599+0000] {spark_submit.py:495} INFO - "0" : 6163
[2024-11-02T08:04:06.600+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:06.600+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:06.600+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:06.600+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:06.600+0000] {spark_submit.py:495} INFO - "0" : 6167
[2024-11-02T08:04:06.600+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:06.601+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:06.601+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:06.601+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:06.601+0000] {spark_submit.py:495} INFO - "0" : 6167
[2024-11-02T08:04:06.601+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:06.602+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:06.602+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-02T08:04:06.602+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9155413138017854,
[2024-11-02T08:04:06.602+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8079175924055747,
[2024-11-02T08:04:06.602+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:06.603+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:06.603+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:06.603+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:06.603+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:06.603+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:06.604+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:06.604+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:06.604+0000] {spark_submit.py:495} INFO - "numOutputRows" : 4
[2024-11-02T08:04:06.605+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:06.605+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:06.749+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/15 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.15.24aaf389-35c9-4e9f-9dde-9b63e9ffac55.tmp
[2024-11-02T08:04:07.197+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.15.24aaf389-35c9-4e9f-9dde-9b63e9ffac55.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/15
[2024-11-02T08:04:07.211+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1730534646597,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:04:07.370+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:07.387+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:07.479+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:07.508+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:07.587+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:07.620+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:07.621+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:04:07.621+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:04:07.631+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:04:07.632+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:04:07.632+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:04:07.632+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:04:07.646+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[64] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:04:07.683+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:04:07.779+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:07.789+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:04:07.799+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:07.841+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:04:07.846+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[64] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:04:07.846+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-11-02T08:04:07.847+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:07.864+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:07 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:04:08.250+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:08 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:09.242+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 1411 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:04:09.263+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-11-02T08:04:09.288+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 1.647 s
[2024-11-02T08:04:09.378+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:04:09.381+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-11-02T08:04:09.397+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 1.760506 s
[2024-11-02T08:04:09.397+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:04:09.398+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:09.398+0000] {spark_submit.py:495} INFO - Batch: 15
[2024-11-02T08:04:09.398+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:09.736+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:09.822+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:04:09.823+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:09.823+0000] {spark_submit.py:495} INFO - |2        |29.89      |50.29   |2024-11-02 08:04:01|
[2024-11-02T08:04:09.823+0000] {spark_submit.py:495} INFO - |3        |29.2       |50.34   |2024-11-02 08:04:02|
[2024-11-02T08:04:09.824+0000] {spark_submit.py:495} INFO - |4        |29.96      |50.44   |2024-11-02 08:04:04|
[2024-11-02T08:04:09.824+0000] {spark_submit.py:495} INFO - |5        |29.97      |50.19   |2024-11-02 08:04:05|
[2024-11-02T08:04:09.824+0000] {spark_submit.py:495} INFO - |1        |29.18      |50.99   |2024-11-02 08:04:06|
[2024-11-02T08:04:09.824+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:09.824+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:04:09.824+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:04:10.149+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/15 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.15.8c03fba9-70f7-4b45-a4bf-6274ca876704.tmp
[2024-11-02T08:04:10.836+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.15.8c03fba9-70f7-4b45-a4bf-6274ca876704.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/15
[2024-11-02T08:04:10.854+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:10 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:10.865+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:10.876+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:10.876+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:10.877+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:06.536Z",
[2024-11-02T08:04:10.877+0000] {spark_submit.py:495} INFO - "batchId" : 15,
[2024-11-02T08:04:10.877+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:04:10.877+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0094891984655763,
[2024-11-02T08:04:10.878+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1644154634373545,
[2024-11-02T08:04:10.878+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:10.878+0000] {spark_submit.py:495} INFO - "addBatch" : 2307,
[2024-11-02T08:04:10.878+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1089,
[2024-11-02T08:04:10.878+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:04:10.879+0000] {spark_submit.py:495} INFO - "latestOffset" : 60,
[2024-11-02T08:04:10.879+0000] {spark_submit.py:495} INFO - "queryPlanning" : 204,
[2024-11-02T08:04:10.879+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4294,
[2024-11-02T08:04:10.879+0000] {spark_submit.py:495} INFO - "walCommit" : 612
[2024-11-02T08:04:10.879+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:10.880+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:10.880+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:10.880+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:10.881+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:10.881+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:10.904+0000] {spark_submit.py:495} INFO - "0" : 6167
[2024-11-02T08:04:10.907+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:10.907+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:10.908+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:10.908+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:10.908+0000] {spark_submit.py:495} INFO - "0" : 6172
[2024-11-02T08:04:10.908+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:10.908+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:10.909+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:10.909+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:10.909+0000] {spark_submit.py:495} INFO - "0" : 6172
[2024-11-02T08:04:10.909+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:10.909+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:10.910+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:04:10.910+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.0094891984655763,
[2024-11-02T08:04:10.910+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.1644154634373545,
[2024-11-02T08:04:10.910+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:10.910+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:10.910+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:10.911+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:10.911+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:10.911+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:10.911+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:10.911+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:10.916+0000] {spark_submit.py:495} INFO - "numOutputRows" : 5
[2024-11-02T08:04:10.917+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:10.917+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:11.033+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/16 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.16.e4668a16-ff9f-42dd-a3e6-1c5462bbb394.tmp
[2024-11-02T08:04:11.478+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.16.e4668a16-ff9f-42dd-a3e6-1c5462bbb394.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/16
[2024-11-02T08:04:11.483+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1730534650914,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:04:11.600+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:11.628+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:11.668+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:11.673+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:11.865+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:11.881+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:11.989+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:04:11.994+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:04:11.994+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:04:11.994+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:04:12.004+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:04:12.007+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:04:12.014+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:11 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:04:12.067+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:04:12.204+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:04:12.205+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:12.206+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:04:12.227+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:04:12.227+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-11-02T08:04:12.227+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:12.228+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:12.228+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:04:12.468+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:12.933+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 720 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:04:12.935+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-11-02T08:04:12.938+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.941 s
[2024-11-02T08:04:12.939+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:04:12.940+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2024-11-02T08:04:12.941+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.954334 s
[2024-11-02T08:04:12.941+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:04:12.942+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:12.942+0000] {spark_submit.py:495} INFO - Batch: 16
[2024-11-02T08:04:12.942+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:13.046+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:13.054+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:04:13.060+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:13.070+0000] {spark_submit.py:495} INFO - |2        |29.66      |50.7    |2024-11-02 08:04:07|
[2024-11-02T08:04:13.082+0000] {spark_submit.py:495} INFO - |3        |29.02      |50.37   |2024-11-02 08:04:08|
[2024-11-02T08:04:13.085+0000] {spark_submit.py:495} INFO - |4        |29.45      |50.5    |2024-11-02 08:04:09|
[2024-11-02T08:04:13.086+0000] {spark_submit.py:495} INFO - |5        |29.75      |50.29   |2024-11-02 08:04:10|
[2024-11-02T08:04:13.086+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:13.086+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:04:13.087+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:04:13.270+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/16 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.16.a18c1f8e-eb74-4031-9e42-af1ee8dba324.tmp
[2024-11-02T08:04:13.850+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.16.a18c1f8e-eb74-4031-9e42-af1ee8dba324.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/16
[2024-11-02T08:04:13.863+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:13.875+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:13.876+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:13.876+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:13.877+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:10.854Z",
[2024-11-02T08:04:13.877+0000] {spark_submit.py:495} INFO - "batchId" : 16,
[2024-11-02T08:04:13.877+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-02T08:04:13.877+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9263547938860585,
[2024-11-02T08:04:13.877+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3368983957219251,
[2024-11-02T08:04:13.883+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:13.888+0000] {spark_submit.py:495} INFO - "addBatch" : 1438,
[2024-11-02T08:04:13.890+0000] {spark_submit.py:495} INFO - "commitOffsets" : 794,
[2024-11-02T08:04:13.893+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:04:13.894+0000] {spark_submit.py:495} INFO - "latestOffset" : 60,
[2024-11-02T08:04:13.896+0000] {spark_submit.py:495} INFO - "queryPlanning" : 179,
[2024-11-02T08:04:13.896+0000] {spark_submit.py:495} INFO - "triggerExecution" : 2992,
[2024-11-02T08:04:13.896+0000] {spark_submit.py:495} INFO - "walCommit" : 513
[2024-11-02T08:04:13.896+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:13.896+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:13.897+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:13.899+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:13.901+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:13.907+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:13.909+0000] {spark_submit.py:495} INFO - "0" : 6172
[2024-11-02T08:04:13.910+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:13.910+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:13.910+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:13.911+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:13.911+0000] {spark_submit.py:495} INFO - "0" : 6176
[2024-11-02T08:04:13.911+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:13.912+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:13.912+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:13.912+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:13.912+0000] {spark_submit.py:495} INFO - "0" : 6176
[2024-11-02T08:04:13.912+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:13.913+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:13.914+0000] {spark_submit.py:495} INFO - "numInputRows" : 4,
[2024-11-02T08:04:13.914+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9263547938860585,
[2024-11-02T08:04:13.914+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3368983957219251,
[2024-11-02T08:04:13.914+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:13.914+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:13.915+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:13.915+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:13.915+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:13.915+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:13.915+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:13.916+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:13.916+0000] {spark_submit.py:495} INFO - "numOutputRows" : 4
[2024-11-02T08:04:13.916+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:13.916+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:14.013+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/17 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.17.95c4211b-3c24-42b7-8e25-0ca2aa9aa794.tmp
[2024-11-02T08:04:14.360+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.17.95c4211b-3c24-42b7-8e25-0ca2aa9aa794.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/17
[2024-11-02T08:04:14.368+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1730534653902,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:04:14.567+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:14.603+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:14.637+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:14.640+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:14.700+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:14.740+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:14.936+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:04:14.941+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:04:14.951+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:04:14.956+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:04:14.958+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:04:14.973+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:04:14.973+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:04:14.986+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:14 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:04:15.073+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:04:15.138+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:15.167+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:15.171+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:15.188+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:04:15.189+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:04:15.189+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-11-02T08:04:15.212+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:04:15.536+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:15 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:16.338+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 1130 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:04:16.346+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-11-02T08:04:16.378+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 1.385 s
[2024-11-02T08:04:16.403+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:04:16.404+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-11-02T08:04:16.405+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 1.464221 s
[2024-11-02T08:04:16.417+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:04:16.418+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:16.419+0000] {spark_submit.py:495} INFO - Batch: 17
[2024-11-02T08:04:16.419+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:16.681+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:16.690+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:04:16.739+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:16.739+0000] {spark_submit.py:495} INFO - |1        |29.94      |50.21   |2024-11-02 08:04:11|
[2024-11-02T08:04:16.739+0000] {spark_submit.py:495} INFO - |2        |29.42      |50.71   |2024-11-02 08:04:12|
[2024-11-02T08:04:16.739+0000] {spark_submit.py:495} INFO - |3        |29.03      |50.62   |2024-11-02 08:04:13|
[2024-11-02T08:04:16.740+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:16.759+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:04:16.760+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:04:16.882+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/17 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.17.aa50ae21-5611-45a1-b1d5-6fc3d6ab8c8c.tmp
[2024-11-02T08:04:18.023+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.17.aa50ae21-5611-45a1-b1d5-6fc3d6ab8c8c.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/17
[2024-11-02T08:04:18.064+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:18 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:18.067+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:18.067+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:18.068+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:18.071+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:13.868Z",
[2024-11-02T08:04:18.073+0000] {spark_submit.py:495} INFO - "batchId" : 17,
[2024-11-02T08:04:18.074+0000] {spark_submit.py:495} INFO - "numInputRows" : 3,
[2024-11-02T08:04:18.074+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9953550099535502,
[2024-11-02T08:04:18.075+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7225433526011561,
[2024-11-02T08:04:18.077+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:18.078+0000] {spark_submit.py:495} INFO - "addBatch" : 2067,
[2024-11-02T08:04:18.078+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1342,
[2024-11-02T08:04:18.082+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:04:18.082+0000] {spark_submit.py:495} INFO - "latestOffset" : 34,
[2024-11-02T08:04:18.083+0000] {spark_submit.py:495} INFO - "queryPlanning" : 243,
[2024-11-02T08:04:18.083+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4152,
[2024-11-02T08:04:18.083+0000] {spark_submit.py:495} INFO - "walCommit" : 459
[2024-11-02T08:04:18.083+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:18.083+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:18.084+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:18.084+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:18.084+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:18.084+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:18.084+0000] {spark_submit.py:495} INFO - "0" : 6176
[2024-11-02T08:04:18.084+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:18.088+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:18.089+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:18.089+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:18.090+0000] {spark_submit.py:495} INFO - "0" : 6179
[2024-11-02T08:04:18.090+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:18.090+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:18.090+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:18.090+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:18.091+0000] {spark_submit.py:495} INFO - "0" : 6179
[2024-11-02T08:04:18.091+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:18.091+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:18.091+0000] {spark_submit.py:495} INFO - "numInputRows" : 3,
[2024-11-02T08:04:18.091+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9953550099535502,
[2024-11-02T08:04:18.092+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7225433526011561,
[2024-11-02T08:04:18.092+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:18.092+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:18.092+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:18.092+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:18.092+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:18.093+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:18.093+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:18.093+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:18.093+0000] {spark_submit.py:495} INFO - "numOutputRows" : 3
[2024-11-02T08:04:18.093+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:18.093+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:18.361+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/18 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.18.7aa3b3fc-bcc5-4f1f-82c6-3c67dadbc612.tmp
[2024-11-02T08:04:19.063+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.18.7aa3b3fc-bcc5-4f1f-82c6-3c67dadbc612.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/18
[2024-11-02T08:04:19.085+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1730534658138,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:04:19.094+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:19.130+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:19.202+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:19.206+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:19.470+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:19.553+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:04:19.828+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:04:19.836+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:04:19.957+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:04:19.972+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:04:20.042+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:04:20.044+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:04:20.044+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:19 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:04:20.193+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:04:20.384+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:04:20.392+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:20.397+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:04:20.397+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:04:20.397+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-11-02T08:04:20.456+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:20.464+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:20.464+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:20 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:04:21.527+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:22.639+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 2183 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:04:22.655+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-11-02T08:04:22.682+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 2.663 s
[2024-11-02T08:04:22.688+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:04:22.702+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-11-02T08:04:22.705+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 2.834161 s
[2024-11-02T08:04:22.706+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:04:22.706+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:22.707+0000] {spark_submit.py:495} INFO - Batch: 18
[2024-11-02T08:04:22.707+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:04:22.892+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:22.905+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:04:22.916+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:22.917+0000] {spark_submit.py:495} INFO - |4        |29.61      |50.37   |2024-11-02 08:04:14|
[2024-11-02T08:04:22.921+0000] {spark_submit.py:495} INFO - |5        |29.56      |50.75   |2024-11-02 08:04:15|
[2024-11-02T08:04:22.921+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:04:22.922+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:04:22.922+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:04:22.985+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/18 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.18.27b056a0-5c0f-46e2-8881-ee7c6211cdd0.tmp
[2024-11-02T08:04:23.227+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.18.27b056a0-5c0f-46e2-8881-ee7c6211cdd0.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/18
[2024-11-02T08:04:23.234+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:23.235+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:23.237+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:23.239+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:23.241+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:18.058Z",
[2024-11-02T08:04:23.242+0000] {spark_submit.py:495} INFO - "batchId" : 18,
[2024-11-02T08:04:23.242+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-02T08:04:23.242+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.477326968973747,
[2024-11-02T08:04:23.242+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.3868471953578337,
[2024-11-02T08:04:23.243+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:23.243+0000] {spark_submit.py:495} INFO - "addBatch" : 3769,
[2024-11-02T08:04:23.243+0000] {spark_submit.py:495} INFO - "commitOffsets" : 324,
[2024-11-02T08:04:23.243+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:04:23.243+0000] {spark_submit.py:495} INFO - "latestOffset" : 80,
[2024-11-02T08:04:23.243+0000] {spark_submit.py:495} INFO - "queryPlanning" : 82,
[2024-11-02T08:04:23.246+0000] {spark_submit.py:495} INFO - "triggerExecution" : 5170,
[2024-11-02T08:04:23.248+0000] {spark_submit.py:495} INFO - "walCommit" : 914
[2024-11-02T08:04:23.249+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:23.250+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:23.250+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:23.251+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:23.252+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:23.252+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:23.252+0000] {spark_submit.py:495} INFO - "0" : 6179
[2024-11-02T08:04:23.253+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:23.253+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:23.253+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:23.253+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:23.253+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:23.253+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:23.254+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:23.254+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:23.254+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:23.254+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:23.255+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:23.255+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:23.255+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-02T08:04:23.255+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.477326968973747,
[2024-11-02T08:04:23.256+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.3868471953578337,
[2024-11-02T08:04:23.256+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:23.256+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:23.256+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:23.256+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:23.257+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:23.257+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:23.260+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:23.261+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:23.264+0000] {spark_submit.py:495} INFO - "numOutputRows" : 2
[2024-11-02T08:04:23.265+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:23.265+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:33.250+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:33.250+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:33.252+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:33.253+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:33.253+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:33.227Z",
[2024-11-02T08:04:33.254+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:04:33.254+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:04:33.254+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:04:33.255+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:04:33.255+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:33.256+0000] {spark_submit.py:495} INFO - "latestOffset" : 20,
[2024-11-02T08:04:33.256+0000] {spark_submit.py:495} INFO - "triggerExecution" : 20
[2024-11-02T08:04:33.256+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:33.256+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:33.256+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:33.257+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:33.257+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:33.257+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:33.258+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:33.258+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:33.258+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:33.258+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:33.259+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:33.259+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:33.259+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:33.259+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:33.259+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:33.259+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:33.260+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:33.261+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:33.261+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:33.262+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:04:33.263+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:04:33.263+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:04:33.264+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:33.264+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:33.264+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:33.264+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:33.264+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:33.264+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:33.265+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:33.265+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:33.265+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:04:33.265+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:33.265+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:43.251+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:43.252+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:43.253+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:43.253+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:43.254+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:43.237Z",
[2024-11-02T08:04:43.254+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:04:43.254+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:04:43.254+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:04:43.255+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:04:43.255+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:43.255+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-02T08:04:43.255+0000] {spark_submit.py:495} INFO - "triggerExecution" : 12
[2024-11-02T08:04:43.255+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:43.256+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:43.256+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:43.256+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:43.257+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:43.257+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:43.257+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:43.257+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:43.258+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:43.258+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:43.258+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:43.258+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:43.258+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:43.259+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:43.259+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:43.259+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:43.260+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:43.261+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:43.261+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:43.263+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:04:43.265+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:04:43.265+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:04:43.265+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:43.265+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:43.266+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:43.266+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:43.266+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:43.266+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:43.266+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:43.266+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:43.267+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:04:43.267+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:43.267+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:51.337+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:51.358+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:04:53.310+0000] {spark_submit.py:495} INFO - 24/11/02 08:04:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:04:53.311+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:04:53.327+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:04:53.327+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:04:53.336+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:04:53.222Z",
[2024-11-02T08:04:53.336+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:04:53.338+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:04:53.339+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:04:53.351+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:04:53.357+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:04:53.362+0000] {spark_submit.py:495} INFO - "latestOffset" : 43,
[2024-11-02T08:04:53.363+0000] {spark_submit.py:495} INFO - "triggerExecution" : 54
[2024-11-02T08:04:53.363+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:53.373+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:04:53.373+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:04:53.388+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:04:53.389+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:04:53.395+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:53.395+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:53.404+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:53.404+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:53.404+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:04:53.404+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:53.405+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:53.405+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:53.405+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:53.405+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:04:53.405+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:04:53.423+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:04:53.424+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:53.425+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:04:53.432+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:04:53.433+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:04:53.434+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:04:53.434+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:04:53.434+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:04:53.446+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:04:53.446+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:04:53.447+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:53.463+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:04:53.464+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:04:53.464+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:04:53.464+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:04:53.464+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:04:53.464+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:03.427+0000] {spark_submit.py:495} INFO - 24/11/02 08:05:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:05:03.431+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:05:03.440+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:05:03.441+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:05:03.447+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:05:03.230Z",
[2024-11-02T08:05:03.456+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:05:03.457+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:03.457+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:03.457+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:03.457+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:05:03.464+0000] {spark_submit.py:495} INFO - "latestOffset" : 184,
[2024-11-02T08:05:03.464+0000] {spark_submit.py:495} INFO - "triggerExecution" : 184
[2024-11-02T08:05:03.464+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:03.465+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:05:03.465+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:05:03.465+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:05:03.465+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:05:03.465+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:03.466+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:03.466+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:03.466+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:03.466+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:05:03.466+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:03.467+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:03.467+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:03.467+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:03.467+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:05:03.467+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:03.468+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:03.468+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:03.468+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:03.479+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:03.483+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:03.490+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:03.491+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:05:03.492+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:05:03.493+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:05:03.497+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:05:03.498+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:03.498+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:05:03.498+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:05:03.499+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:05:03.499+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:05:03.500+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:03.501+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:13.437+0000] {spark_submit.py:495} INFO - 24/11/02 08:05:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:05:13.438+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:05:13.438+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:05:13.439+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:05:13.439+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:05:13.423Z",
[2024-11-02T08:05:13.439+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:05:13.439+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:13.439+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:13.440+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:13.440+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:05:13.440+0000] {spark_submit.py:495} INFO - "latestOffset" : 10,
[2024-11-02T08:05:13.440+0000] {spark_submit.py:495} INFO - "triggerExecution" : 10
[2024-11-02T08:05:13.441+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:13.442+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:05:13.442+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:05:13.442+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:05:13.442+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:05:13.442+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:13.443+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:13.443+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:13.443+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:13.444+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:05:13.445+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:13.445+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:13.445+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:13.446+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:13.446+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:05:13.447+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:13.447+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:13.447+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:13.447+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:13.448+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:13.448+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:13.449+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:13.450+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:05:13.450+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:05:13.450+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:05:13.450+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:05:13.450+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:13.453+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:05:13.453+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:05:13.455+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:05:13.456+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:05:13.457+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:13.457+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:23.437+0000] {spark_submit.py:495} INFO - 24/11/02 08:05:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:05:23.466+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:05:23.466+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:05:23.467+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:05:23.476+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:05:23.422Z",
[2024-11-02T08:05:23.476+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:05:23.477+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:23.482+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:23.482+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:23.483+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:05:23.483+0000] {spark_submit.py:495} INFO - "latestOffset" : 12,
[2024-11-02T08:05:23.483+0000] {spark_submit.py:495} INFO - "triggerExecution" : 12
[2024-11-02T08:05:23.484+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:23.496+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:05:23.497+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:05:23.497+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:05:23.498+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:05:23.498+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:23.498+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:23.499+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:23.499+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:23.499+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:05:23.499+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:23.500+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:23.500+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:23.500+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:23.501+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:05:23.501+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:23.502+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:23.502+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:23.502+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:23.503+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:23.503+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:23.504+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:23.504+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:05:23.504+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:05:23.505+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:05:23.505+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:05:23.505+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:23.506+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:05:23.506+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:05:23.506+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:05:23.507+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:05:23.507+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:23.507+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:33.669+0000] {spark_submit.py:495} INFO - 24/11/02 08:05:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:05:33.677+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:05:33.679+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:05:33.681+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:05:33.681+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:05:33.489Z",
[2024-11-02T08:05:33.681+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:05:33.681+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:33.681+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:33.682+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:33.682+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:05:33.682+0000] {spark_submit.py:495} INFO - "latestOffset" : 161,
[2024-11-02T08:05:33.682+0000] {spark_submit.py:495} INFO - "triggerExecution" : 161
[2024-11-02T08:05:33.682+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:33.682+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:05:33.683+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:05:33.683+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:05:33.683+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:05:33.683+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:33.683+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:33.684+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:33.684+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:33.684+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:05:33.684+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:33.684+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:33.684+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:33.685+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:33.685+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:05:33.685+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:33.685+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:33.685+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:33.686+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:33.686+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:33.686+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:33.686+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:33.686+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:05:33.686+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:05:33.687+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:05:33.687+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:05:33.687+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:33.687+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:05:33.687+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:05:33.688+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:05:33.688+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:05:33.688+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:33.688+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:43.727+0000] {spark_submit.py:495} INFO - 24/11/02 08:05:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:05:43.728+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:05:43.728+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:05:43.730+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:05:43.732+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:05:43.675Z",
[2024-11-02T08:05:43.733+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:05:43.738+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:43.741+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:43.741+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:43.743+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:05:43.744+0000] {spark_submit.py:495} INFO - "latestOffset" : 49,
[2024-11-02T08:05:43.746+0000] {spark_submit.py:495} INFO - "triggerExecution" : 49
[2024-11-02T08:05:43.748+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:43.752+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:05:43.754+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:05:43.755+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:05:43.756+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:05:43.756+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:43.756+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:43.756+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:43.757+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:43.757+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:05:43.757+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:43.757+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:43.757+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:43.758+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:43.758+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:05:43.758+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:43.758+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:43.759+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:43.759+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:43.759+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:43.759+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:43.759+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:43.760+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:05:43.760+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:05:43.764+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:05:43.769+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:05:43.769+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:43.770+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:05:43.770+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:05:43.770+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:05:43.771+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:05:43.771+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:43.771+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:53.738+0000] {spark_submit.py:495} INFO - 24/11/02 08:05:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:05:53.739+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:05:53.739+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:05:53.739+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:05:53.740+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:05:53.715Z",
[2024-11-02T08:05:53.740+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:05:53.740+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:53.740+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:53.740+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:53.740+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:05:53.741+0000] {spark_submit.py:495} INFO - "latestOffset" : 11,
[2024-11-02T08:05:53.741+0000] {spark_submit.py:495} INFO - "triggerExecution" : 11
[2024-11-02T08:05:53.741+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:53.741+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:05:53.741+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:05:53.741+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:05:53.742+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:05:53.742+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:53.742+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:53.742+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:53.742+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:53.742+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:05:53.743+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:53.743+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:53.743+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:53.743+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:53.743+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:05:53.743+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:05:53.744+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:05:53.744+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:53.744+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:05:53.744+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:05:53.744+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:05:53.744+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:05:53.745+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:05:53.745+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:05:53.745+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:05:53.745+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:05:53.745+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:53.745+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:05:53.746+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:05:53.746+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:05:53.746+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:05:53.746+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:05:53.746+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:03.775+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:03.776+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:03.788+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:03.789+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:03.789+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:03.721Z",
[2024-11-02T08:06:03.789+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:06:03.789+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:06:03.789+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:06:03.790+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:06:03.790+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:03.814+0000] {spark_submit.py:495} INFO - "latestOffset" : 37,
[2024-11-02T08:06:03.820+0000] {spark_submit.py:495} INFO - "triggerExecution" : 37
[2024-11-02T08:06:03.841+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:03.849+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:03.849+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:03.849+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:03.849+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:03.849+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:03.850+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:03.850+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:03.850+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:03.850+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:03.850+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:03.850+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:03.851+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:03.851+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:03.866+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:03.866+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:03.891+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:03.892+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:03.892+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:03.926+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:06:03.935+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:06:03.936+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:06:03.944+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:03.945+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:03.956+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:03.963+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:03.965+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:03.965+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:03.966+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:03.966+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:03.967+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:06:03.967+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:03.967+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:13.942+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:13.943+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:13.943+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:13.944+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:13.944+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:13.768Z",
[2024-11-02T08:06:13.944+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:06:13.945+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:06:13.945+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:06:13.945+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:06:13.945+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:13.946+0000] {spark_submit.py:495} INFO - "latestOffset" : 71,
[2024-11-02T08:06:13.946+0000] {spark_submit.py:495} INFO - "triggerExecution" : 71
[2024-11-02T08:06:13.946+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:13.947+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:13.947+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:13.947+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:13.947+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:13.948+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:13.948+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:13.948+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:13.949+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:13.949+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:13.949+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:13.949+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:13.950+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:13.950+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:13.950+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:13.951+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:13.952+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:13.952+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:13.953+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:13.953+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:06:13.954+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:06:13.954+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:06:13.954+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:13.955+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:13.956+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:13.956+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:13.957+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:13.957+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:13.958+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:13.958+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:13.959+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:06:13.959+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:13.960+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:19.442+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/19 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.19.6c6643d2-fc73-4f92-8415-f3be6a044485.tmp
[2024-11-02T08:06:21.017+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.19.6c6643d2-fc73-4f92-8415-f3be6a044485.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/19
[2024-11-02T08:06:21.017+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1730534778850,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:06:21.129+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:21.132+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:21.182+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:21.222+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:21.478+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:21.601+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:22.090+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:06:22.146+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:06:22.183+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:06:22.214+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:06:22.242+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:06:22.306+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:06:22.316+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:06:22.347+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 23.3 KiB, free 434.4 MiB)
[2024-11-02T08:06:22.591+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.4 MiB)
[2024-11-02T08:06:22.623+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:23.189+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:06:23.190+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:06:23.190+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-11-02T08:06:23.190+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:22 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:06:24.642+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:24 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:27.044+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 4145 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:06:27.047+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-11-02T08:06:27.140+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 4.825 s
[2024-11-02T08:06:27.142+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:06:27.143+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-11-02T08:06:27.143+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 5.034606 s
[2024-11-02T08:06:27.144+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:06:27.145+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:27.145+0000] {spark_submit.py:495} INFO - Batch: 19
[2024-11-02T08:06:27.145+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:28.222+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:28.227+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:06:28.238+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:28.242+0000] {spark_submit.py:495} INFO - |1        |29.08      |50.35   |2024-11-02 08:06:18|
[2024-11-02T08:06:28.252+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:28.257+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:06:28.262+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:06:28.605+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/19 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.19.66e8e5c2-d2e8-4634-995d-c24314c8c538.tmp
[2024-11-02T08:06:29.620+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.19.66e8e5c2-d2e8-4634-995d-c24314c8c538.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/19
[2024-11-02T08:06:29.672+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:29 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:29.674+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:29.674+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:29.674+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:29.675+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:18.253Z",
[2024-11-02T08:06:29.675+0000] {spark_submit.py:495} INFO - "batchId" : 19,
[2024-11-02T08:06:29.675+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-02T08:06:29.676+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 6.097560975609756,
[2024-11-02T08:06:29.676+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.08801267382503081,
[2024-11-02T08:06:29.676+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:29.677+0000] {spark_submit.py:495} INFO - "addBatch" : 7088,
[2024-11-02T08:06:29.677+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1387,
[2024-11-02T08:06:29.693+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:06:29.701+0000] {spark_submit.py:495} INFO - "latestOffset" : 597,
[2024-11-02T08:06:29.708+0000] {spark_submit.py:495} INFO - "queryPlanning" : 123,
[2024-11-02T08:06:29.718+0000] {spark_submit.py:495} INFO - "triggerExecution" : 11362,
[2024-11-02T08:06:29.719+0000] {spark_submit.py:495} INFO - "walCommit" : 2166
[2024-11-02T08:06:29.755+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:29.757+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:29.757+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:29.757+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:29.757+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:29.757+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:29.758+0000] {spark_submit.py:495} INFO - "0" : 6181
[2024-11-02T08:06:29.758+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:29.758+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:29.758+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:29.758+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:29.758+0000] {spark_submit.py:495} INFO - "0" : 6182
[2024-11-02T08:06:29.759+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:29.759+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:29.759+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:29.759+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:29.759+0000] {spark_submit.py:495} INFO - "0" : 6182
[2024-11-02T08:06:29.760+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:29.760+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:29.760+0000] {spark_submit.py:495} INFO - "numInputRows" : 1,
[2024-11-02T08:06:29.760+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 6.097560975609756,
[2024-11-02T08:06:29.760+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.08801267382503081,
[2024-11-02T08:06:29.760+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:29.761+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:29.761+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:29.761+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:29.761+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:29.761+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:29.762+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:29.762+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:29.762+0000] {spark_submit.py:495} INFO - "numOutputRows" : 1
[2024-11-02T08:06:29.762+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:29.762+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:30.048+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/20 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.20.ffa39491-2ed3-4fcc-8b7a-7c897bcab442.tmp
[2024-11-02T08:06:31.896+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.20.ffa39491-2ed3-4fcc-8b7a-7c897bcab442.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/20
[2024-11-02T08:06:31.918+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:31 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1730534789737,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:06:32.043+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:32.076+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:32.179+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:32.179+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:32.235+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:32.245+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:32.560+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:06:32.560+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:06:32.572+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:06:32.602+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:06:32.602+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:06:32.603+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:06:32.603+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[84] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:06:32.603+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:06:32.817+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:32 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:06:33.236+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:33.286+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:33.287+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:33.320+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:06:33.336+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[84] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:06:33.337+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-11-02T08:06:33.439+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:06:33.740+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:33 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:34.840+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 1415 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:06:34.841+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-11-02T08:06:34.914+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 2.291 s
[2024-11-02T08:06:34.915+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:06:34.915+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-11-02T08:06:34.916+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 2.342496 s
[2024-11-02T08:06:34.916+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:06:34.916+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:34.916+0000] {spark_submit.py:495} INFO - Batch: 20
[2024-11-02T08:06:34.916+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:35.172+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:35.180+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:06:35.180+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:35.189+0000] {spark_submit.py:495} INFO - |2        |29.07      |50.49   |2024-11-02 08:06:19|
[2024-11-02T08:06:35.189+0000] {spark_submit.py:495} INFO - |3        |29.08      |50.72   |2024-11-02 08:06:20|
[2024-11-02T08:06:35.191+0000] {spark_submit.py:495} INFO - |4        |29.42      |50.68   |2024-11-02 08:06:21|
[2024-11-02T08:06:35.191+0000] {spark_submit.py:495} INFO - |5        |29.86      |50.89   |2024-11-02 08:06:23|
[2024-11-02T08:06:35.192+0000] {spark_submit.py:495} INFO - |1        |29.49      |50.99   |2024-11-02 08:06:24|
[2024-11-02T08:06:35.193+0000] {spark_submit.py:495} INFO - |2        |29.27      |50.7    |2024-11-02 08:06:26|
[2024-11-02T08:06:35.194+0000] {spark_submit.py:495} INFO - |3        |29.26      |50.26   |2024-11-02 08:06:27|
[2024-11-02T08:06:35.194+0000] {spark_submit.py:495} INFO - |4        |29.95      |50.41   |2024-11-02 08:06:28|
[2024-11-02T08:06:35.195+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:35.202+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:06:35.204+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:06:35.459+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/20 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.20.98b6f4b4-01b1-4e5e-98fa-89f66302bce6.tmp
[2024-11-02T08:06:36.065+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.20.98b6f4b4-01b1-4e5e-98fa-89f66302bce6.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/20
[2024-11-02T08:06:36.075+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:36.079+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:36.094+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:36.095+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:36.095+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:29.672Z",
[2024-11-02T08:06:36.095+0000] {spark_submit.py:495} INFO - "batchId" : 20,
[2024-11-02T08:06:36.096+0000] {spark_submit.py:495} INFO - "numInputRows" : 8,
[2024-11-02T08:06:36.096+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.700586741395919,
[2024-11-02T08:06:36.096+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2519561815336464,
[2024-11-02T08:06:36.096+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:36.096+0000] {spark_submit.py:495} INFO - "addBatch" : 3015,
[2024-11-02T08:06:36.097+0000] {spark_submit.py:495} INFO - "commitOffsets" : 903,
[2024-11-02T08:06:36.097+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:06:36.097+0000] {spark_submit.py:495} INFO - "latestOffset" : 65,
[2024-11-02T08:06:36.097+0000] {spark_submit.py:495} INFO - "queryPlanning" : 243,
[2024-11-02T08:06:36.098+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6390,
[2024-11-02T08:06:36.098+0000] {spark_submit.py:495} INFO - "walCommit" : 2163
[2024-11-02T08:06:36.098+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:36.098+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:36.099+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:36.099+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:36.099+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:36.099+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:36.100+0000] {spark_submit.py:495} INFO - "0" : 6182
[2024-11-02T08:06:36.100+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:36.100+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:36.100+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:36.100+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:36.101+0000] {spark_submit.py:495} INFO - "0" : 6190
[2024-11-02T08:06:36.101+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:36.101+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:36.101+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:36.101+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:36.102+0000] {spark_submit.py:495} INFO - "0" : 6190
[2024-11-02T08:06:36.102+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:36.102+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:36.102+0000] {spark_submit.py:495} INFO - "numInputRows" : 8,
[2024-11-02T08:06:36.103+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.700586741395919,
[2024-11-02T08:06:36.103+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2519561815336464,
[2024-11-02T08:06:36.103+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:36.103+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:36.103+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:36.103+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:36.104+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:36.104+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:36.104+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:36.104+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:36.104+0000] {spark_submit.py:495} INFO - "numOutputRows" : 8
[2024-11-02T08:06:36.104+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:36.105+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:36.554+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/21 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.21.52a70a19-cc50-42b1-9204-2f113dc6d0fb.tmp
[2024-11-02T08:06:37.131+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.21.52a70a19-cc50-42b1-9204-2f113dc6d0fb.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/21
[2024-11-02T08:06:37.177+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1730534796259,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:06:37.228+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:37.253+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:37.309+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:37.332+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:37.420+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:37.464+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:37.850+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:06:37.851+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:06:37.953+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:06:37.978+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:06:37.978+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:06:37.978+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:06:37.984+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:37 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[88] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:06:38.010+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:06:38.342+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:06:38.355+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:38.363+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:06:38.418+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[88] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:06:38.497+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-11-02T08:06:38.498+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:38.775+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:38.806+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:38 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:06:39.288+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:39.963+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 1174 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:06:39.967+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-11-02T08:06:39.988+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 1.981 s
[2024-11-02T08:06:39.988+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:06:39.989+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-11-02T08:06:39.990+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 2.229641 s
[2024-11-02T08:06:39.994+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:06:39.995+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:40.002+0000] {spark_submit.py:495} INFO - Batch: 21
[2024-11-02T08:06:40.005+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:40.231+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:40.248+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:06:40.251+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:40.252+0000] {spark_submit.py:495} INFO - |5        |29.7       |50.22   |2024-11-02 08:06:29|
[2024-11-02T08:06:40.253+0000] {spark_submit.py:495} INFO - |1        |29.51      |50.78   |2024-11-02 08:06:31|
[2024-11-02T08:06:40.253+0000] {spark_submit.py:495} INFO - |2        |29.74      |50.85   |2024-11-02 08:06:32|
[2024-11-02T08:06:40.253+0000] {spark_submit.py:495} INFO - |3        |29.17      |50.37   |2024-11-02 08:06:33|
[2024-11-02T08:06:40.254+0000] {spark_submit.py:495} INFO - |4        |29.41      |50.91   |2024-11-02 08:06:34|
[2024-11-02T08:06:40.254+0000] {spark_submit.py:495} INFO - |5        |29.75      |50.13   |2024-11-02 08:06:35|
[2024-11-02T08:06:40.255+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:40.271+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:06:40.321+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:06:40.377+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/21 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.21.0df26813-0c6d-4796-b9c6-c2bc880398e8.tmp
[2024-11-02T08:06:41.114+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.21.0df26813-0c6d-4796-b9c6-c2bc880398e8.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/21
[2024-11-02T08:06:41.123+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:41 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:41.124+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:41.125+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:41.126+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:41.126+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:36.126Z",
[2024-11-02T08:06:41.126+0000] {spark_submit.py:495} INFO - "batchId" : 21,
[2024-11-02T08:06:41.127+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:06:41.127+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9296560272699101,
[2024-11-02T08:06:41.127+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2036108324974923,
[2024-11-02T08:06:41.127+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:41.127+0000] {spark_submit.py:495} INFO - "addBatch" : 2966,
[2024-11-02T08:06:41.128+0000] {spark_submit.py:495} INFO - "commitOffsets" : 857,
[2024-11-02T08:06:41.128+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:06:41.128+0000] {spark_submit.py:495} INFO - "latestOffset" : 133,
[2024-11-02T08:06:41.128+0000] {spark_submit.py:495} INFO - "queryPlanning" : 117,
[2024-11-02T08:06:41.129+0000] {spark_submit.py:495} INFO - "triggerExecution" : 4985,
[2024-11-02T08:06:41.129+0000] {spark_submit.py:495} INFO - "walCommit" : 863
[2024-11-02T08:06:41.129+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:41.129+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:41.129+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:41.130+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:41.130+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:41.130+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:41.130+0000] {spark_submit.py:495} INFO - "0" : 6190
[2024-11-02T08:06:41.131+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:41.131+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:41.131+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:41.131+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:41.131+0000] {spark_submit.py:495} INFO - "0" : 6196
[2024-11-02T08:06:41.132+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:41.132+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:41.132+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:41.133+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:41.133+0000] {spark_submit.py:495} INFO - "0" : 6196
[2024-11-02T08:06:41.133+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:41.133+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:41.134+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:06:41.134+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9296560272699101,
[2024-11-02T08:06:41.134+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.2036108324974923,
[2024-11-02T08:06:41.134+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:41.134+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:41.135+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:41.135+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:41.135+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:41.135+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:41.135+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:41.136+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:41.136+0000] {spark_submit.py:495} INFO - "numOutputRows" : 6
[2024-11-02T08:06:41.136+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:41.136+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:41.422+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/22 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.22.27a82120-966d-432d-b9c0-128d56ba2c96.tmp
[2024-11-02T08:06:42.276+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.22.27a82120-966d-432d-b9c0-128d56ba2c96.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/22
[2024-11-02T08:06:42.285+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1730534801210,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:06:42.310+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:42.334+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:42.584+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:42.622+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:42.682+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:42.701+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:42.856+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:06:42.864+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:06:42.920+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:06:42.921+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:06:42.921+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:06:42.921+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:06:43.055+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:42 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:06:43.120+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:06:43.366+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:06:43.521+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:43.522+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:06:43.522+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:06:43.522+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-11-02T08:06:43.523+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:43.831+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:06:43.835+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:43 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:44.205+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:44 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:45.360+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 1686 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:06:45.435+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-11-02T08:06:45.437+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 2.444 s
[2024-11-02T08:06:45.438+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:06:45.438+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2024-11-02T08:06:45.438+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 2.527445 s
[2024-11-02T08:06:45.439+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:06:45.439+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:45.439+0000] {spark_submit.py:495} INFO - Batch: 22
[2024-11-02T08:06:45.506+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:45.787+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:45.788+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:06:45.788+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:45.789+0000] {spark_submit.py:495} INFO - |1        |29.8       |50.71   |2024-11-02 08:06:36|
[2024-11-02T08:06:45.789+0000] {spark_submit.py:495} INFO - |2        |29.14      |50.05   |2024-11-02 08:06:37|
[2024-11-02T08:06:45.789+0000] {spark_submit.py:495} INFO - |3        |29.2       |50.03   |2024-11-02 08:06:38|
[2024-11-02T08:06:45.789+0000] {spark_submit.py:495} INFO - |4        |29.93      |50.71   |2024-11-02 08:06:39|
[2024-11-02T08:06:45.790+0000] {spark_submit.py:495} INFO - |5        |29.48      |50.08   |2024-11-02 08:06:40|
[2024-11-02T08:06:45.790+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:45.790+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:06:45.790+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:06:46.067+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/22 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.22.2ac3edba-305a-4d82-bb4d-5c1898cca5f1.tmp
[2024-11-02T08:06:47.852+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.22.2ac3edba-305a-4d82-bb4d-5c1898cca5f1.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/22
[2024-11-02T08:06:48.012+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:48 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:48.023+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:48.052+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:48.052+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:48.053+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:41.121Z",
[2024-11-02T08:06:48.053+0000] {spark_submit.py:495} INFO - "batchId" : 22,
[2024-11-02T08:06:48.054+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:06:48.054+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.001001001001001,
[2024-11-02T08:06:48.054+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7408504963698326,
[2024-11-02T08:06:48.055+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:48.055+0000] {spark_submit.py:495} INFO - "addBatch" : 3329,
[2024-11-02T08:06:48.056+0000] {spark_submit.py:495} INFO - "commitOffsets" : 2086,
[2024-11-02T08:06:48.056+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:06:48.057+0000] {spark_submit.py:495} INFO - "latestOffset" : 89,
[2024-11-02T08:06:48.057+0000] {spark_submit.py:495} INFO - "queryPlanning" : 199,
[2024-11-02T08:06:48.057+0000] {spark_submit.py:495} INFO - "triggerExecution" : 6749,
[2024-11-02T08:06:48.058+0000] {spark_submit.py:495} INFO - "walCommit" : 1044
[2024-11-02T08:06:48.059+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:48.059+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:48.060+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:48.061+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:48.061+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:48.061+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:48.062+0000] {spark_submit.py:495} INFO - "0" : 6196
[2024-11-02T08:06:48.062+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:48.062+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:48.062+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:48.063+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:48.063+0000] {spark_submit.py:495} INFO - "0" : 6201
[2024-11-02T08:06:48.064+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:48.064+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:48.065+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:48.065+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:48.066+0000] {spark_submit.py:495} INFO - "0" : 6201
[2024-11-02T08:06:48.066+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:48.066+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:48.067+0000] {spark_submit.py:495} INFO - "numInputRows" : 5,
[2024-11-02T08:06:48.067+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 1.001001001001001,
[2024-11-02T08:06:48.067+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7408504963698326,
[2024-11-02T08:06:48.067+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:48.068+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:48.068+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:48.068+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:48.069+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:48.069+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:48.069+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:48.070+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:48.070+0000] {spark_submit.py:495} INFO - "numOutputRows" : 5
[2024-11-02T08:06:48.071+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:48.071+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:48.836+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/23 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.23.4ac64dc6-a16e-4bd8-b109-8b683a97d536.tmp
[2024-11-02T08:06:49.778+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.23.4ac64dc6-a16e-4bd8-b109-8b683a97d536.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/23
[2024-11-02T08:06:49.784+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:49 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1730534808487,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:06:49.925+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:49.984+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:50.379+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:50.649+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:50.704+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:50.714+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:50.827+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:06:50.980+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:06:50.986+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:06:50.990+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:06:50.999+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:06:51.003+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:06:51.004+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:50 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:06:51.113+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:06:51.265+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:06:51.357+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:51.382+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:06:51.382+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:06:51.382+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2024-11-02T08:06:51.414+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:51.442+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:51.539+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:51 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:06:52.175+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:52 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:53.186+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 1739 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:06:53.187+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-11-02T08:06:53.286+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 2.355 s
[2024-11-02T08:06:53.286+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:06:53.286+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2024-11-02T08:06:53.286+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 2.375808 s
[2024-11-02T08:06:53.287+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:06:53.302+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:53.303+0000] {spark_submit.py:495} INFO - Batch: 23
[2024-11-02T08:06:53.303+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:06:53.359+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:53.363+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:06:53.374+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:53.374+0000] {spark_submit.py:495} INFO - |1        |29.96      |50.68   |2024-11-02 08:06:41|
[2024-11-02T08:06:53.374+0000] {spark_submit.py:495} INFO - |2        |29.41      |50.53   |2024-11-02 08:06:42|
[2024-11-02T08:06:53.374+0000] {spark_submit.py:495} INFO - |3        |29.75      |50.49   |2024-11-02 08:06:43|
[2024-11-02T08:06:53.375+0000] {spark_submit.py:495} INFO - |4        |29.61      |51.0    |2024-11-02 08:06:45|
[2024-11-02T08:06:53.375+0000] {spark_submit.py:495} INFO - |5        |29.41      |50.32   |2024-11-02 08:06:46|
[2024-11-02T08:06:53.375+0000] {spark_submit.py:495} INFO - |1        |29.83      |50.41   |2024-11-02 08:06:47|
[2024-11-02T08:06:53.375+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:06:53.375+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:06:53.375+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:06:53.816+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/23 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.23.60c1778c-2629-4264-8d82-d2010d2f8e75.tmp
[2024-11-02T08:06:56.033+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.23.60c1778c-2629-4264-8d82-d2010d2f8e75.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/23
[2024-11-02T08:06:56.103+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:06:56.104+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:06:56.104+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:06:56.104+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:06:56.104+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:48.011Z",
[2024-11-02T08:06:56.105+0000] {spark_submit.py:495} INFO - "batchId" : 23,
[2024-11-02T08:06:56.105+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:06:56.106+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8708272859216256,
[2024-11-02T08:06:56.106+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7513148009015778,
[2024-11-02T08:06:56.107+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:06:56.107+0000] {spark_submit.py:495} INFO - "addBatch" : 3375,
[2024-11-02T08:06:56.107+0000] {spark_submit.py:495} INFO - "commitOffsets" : 2584,
[2024-11-02T08:06:56.108+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:06:56.108+0000] {spark_submit.py:495} INFO - "latestOffset" : 476,
[2024-11-02T08:06:56.108+0000] {spark_submit.py:495} INFO - "queryPlanning" : 251,
[2024-11-02T08:06:56.109+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7986,
[2024-11-02T08:06:56.109+0000] {spark_submit.py:495} INFO - "walCommit" : 1299
[2024-11-02T08:06:56.110+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:56.110+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:06:56.110+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:06:56.111+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:06:56.111+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:06:56.112+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:56.112+0000] {spark_submit.py:495} INFO - "0" : 6201
[2024-11-02T08:06:56.112+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:56.113+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:56.113+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:06:56.114+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:56.114+0000] {spark_submit.py:495} INFO - "0" : 6207
[2024-11-02T08:06:56.114+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:56.115+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:56.115+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:06:56.115+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:06:56.116+0000] {spark_submit.py:495} INFO - "0" : 6207
[2024-11-02T08:06:56.116+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:56.116+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:06:56.117+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:06:56.117+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8708272859216256,
[2024-11-02T08:06:56.117+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.7513148009015778,
[2024-11-02T08:06:56.118+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:06:56.118+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:06:56.118+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:06:56.119+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:06:56.119+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:56.119+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:06:56.120+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:06:56.120+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:06:56.121+0000] {spark_submit.py:495} INFO - "numOutputRows" : 6
[2024-11-02T08:06:56.121+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:56.121+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:06:56.452+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/24 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.24.5e1b478c-9dee-46fd-b459-534701dc35aa.tmp
[2024-11-02T08:06:57.554+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.24.5e1b478c-9dee-46fd-b459-534701dc35aa.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/24
[2024-11-02T08:06:57.593+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1730534816122,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:06:57.636+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:57.638+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:57.666+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:57.687+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:57.783+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:57.837+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:06:58.435+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:06:58.517+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:06:58.572+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:06:58.694+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:06:58.717+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:06:58.740+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:06:58.773+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[100] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:06:58.788+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:06:59.007+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:06:59.015+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:59.021+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:58 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:59.082+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:59 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:06:59.136+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:59 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:06:59.155+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[100] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:06:59.156+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:59 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-11-02T08:06:59.156+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:59 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:06:59.952+0000] {spark_submit.py:495} INFO - 24/11/02 08:06:59 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:03.065+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 3911 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:07:03.066+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-11-02T08:07:03.180+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 4.355 s
[2024-11-02T08:07:03.181+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:07:03.181+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-11-02T08:07:03.219+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 4.736754 s
[2024-11-02T08:07:03.235+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:07:03.252+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:03.253+0000] {spark_submit.py:495} INFO - Batch: 24
[2024-11-02T08:07:03.254+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:03.739+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:03.754+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:07:03.759+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:03.762+0000] {spark_submit.py:495} INFO - |2        |29.45      |50.82   |2024-11-02 08:06:49|
[2024-11-02T08:07:03.767+0000] {spark_submit.py:495} INFO - |3        |29.29      |50.38   |2024-11-02 08:06:50|
[2024-11-02T08:07:03.771+0000] {spark_submit.py:495} INFO - |4        |29.2       |50.81   |2024-11-02 08:06:51|
[2024-11-02T08:07:03.780+0000] {spark_submit.py:495} INFO - |5        |29.66      |50.27   |2024-11-02 08:06:52|
[2024-11-02T08:07:03.781+0000] {spark_submit.py:495} INFO - |1        |29.26      |50.48   |2024-11-02 08:06:53|
[2024-11-02T08:07:03.781+0000] {spark_submit.py:495} INFO - |2        |29.73      |50.31   |2024-11-02 08:06:55|
[2024-11-02T08:07:03.781+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:03.782+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:07:03.799+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:07:04.240+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/24 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.24.47fc3edc-bc54-4efb-8d04-d03cedd68fdb.tmp
[2024-11-02T08:07:07.442+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.24.47fc3edc-bc54-4efb-8d04-d03cedd68fdb.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/24
[2024-11-02T08:07:07.457+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:07 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:07:07.457+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:07:07.458+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:07:07.458+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:07:07.458+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:06:56.042Z",
[2024-11-02T08:07:07.458+0000] {spark_submit.py:495} INFO - "batchId" : 24,
[2024-11-02T08:07:07.459+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:07:07.459+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7471049682480388,
[2024-11-02T08:07:07.459+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.5265467310223783,
[2024-11-02T08:07:07.459+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:07:07.459+0000] {spark_submit.py:495} INFO - "addBatch" : 6105,
[2024-11-02T08:07:07.460+0000] {spark_submit.py:495} INFO - "commitOffsets" : 3692,
[2024-11-02T08:07:07.460+0000] {spark_submit.py:495} INFO - "getBatch" : 1,
[2024-11-02T08:07:07.460+0000] {spark_submit.py:495} INFO - "latestOffset" : 80,
[2024-11-02T08:07:07.460+0000] {spark_submit.py:495} INFO - "queryPlanning" : 56,
[2024-11-02T08:07:07.460+0000] {spark_submit.py:495} INFO - "triggerExecution" : 11395,
[2024-11-02T08:07:07.461+0000] {spark_submit.py:495} INFO - "walCommit" : 1461
[2024-11-02T08:07:07.461+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:07.461+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:07:07.461+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:07:07.462+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:07:07.462+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:07:07.462+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:07.462+0000] {spark_submit.py:495} INFO - "0" : 6207
[2024-11-02T08:07:07.462+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:07.463+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:07.463+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:07:07.463+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:07.464+0000] {spark_submit.py:495} INFO - "0" : 6213
[2024-11-02T08:07:07.464+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:07.464+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:07.464+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:07:07.465+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:07.465+0000] {spark_submit.py:495} INFO - "0" : 6213
[2024-11-02T08:07:07.465+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:07.466+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:07.572+0000] {spark_submit.py:495} INFO - "numInputRows" : 6,
[2024-11-02T08:07:07.575+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.7471049682480388,
[2024-11-02T08:07:07.576+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.5265467310223783,
[2024-11-02T08:07:07.577+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:07:07.577+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:07:07.578+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:07:07.579+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:07:07.579+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:07.580+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:07:07.580+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:07:07.580+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:07:07.581+0000] {spark_submit.py:495} INFO - "numOutputRows" : 6
[2024-11-02T08:07:07.581+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:07.581+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:08.210+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/25 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.25.f9b63aaa-2629-4c3d-877c-34f414b56ef5.tmp
[2024-11-02T08:07:10.055+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.25.f9b63aaa-2629-4c3d-877c-34f414b56ef5.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/25
[2024-11-02T08:07:10.056+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1730534827748,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:07:10.177+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:10.371+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:10.488+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:10.632+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:10.709+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:10.909+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:11.072+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:07:11.079+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:07:11.118+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:07:11.204+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:07:11.234+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:07:11.236+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:07:11.260+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:07:11.402+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:07:11.528+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:07:11.676+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:11.676+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:07:11.677+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:07:11.677+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-11-02T08:07:11.705+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:11 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:07:12.190+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:12 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:13.488+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 1827 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:07:13.489+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-11-02T08:07:13.528+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 2.228 s
[2024-11-02T08:07:13.536+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:07:13.587+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-11-02T08:07:13.587+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 2.461406 s
[2024-11-02T08:07:13.587+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:07:13.587+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:13.588+0000] {spark_submit.py:495} INFO - Batch: 25
[2024-11-02T08:07:13.588+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:13.895+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:13.904+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:07:13.917+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:13.924+0000] {spark_submit.py:495} INFO - |3        |29.89      |50.32   |2024-11-02 08:06:56|
[2024-11-02T08:07:13.959+0000] {spark_submit.py:495} INFO - |4        |29.8       |50.81   |2024-11-02 08:06:57|
[2024-11-02T08:07:13.960+0000] {spark_submit.py:495} INFO - |5        |29.62      |50.29   |2024-11-02 08:06:58|
[2024-11-02T08:07:13.960+0000] {spark_submit.py:495} INFO - |1        |29.06      |50.14   |2024-11-02 08:06:59|
[2024-11-02T08:07:13.982+0000] {spark_submit.py:495} INFO - |2        |29.16      |50.1    |2024-11-02 08:07:01|
[2024-11-02T08:07:13.983+0000] {spark_submit.py:495} INFO - |3        |29.57      |50.42   |2024-11-02 08:07:02|
[2024-11-02T08:07:13.983+0000] {spark_submit.py:495} INFO - |4        |29.38      |50.92   |2024-11-02 08:07:03|
[2024-11-02T08:07:13.983+0000] {spark_submit.py:495} INFO - |5        |29.67      |50.1    |2024-11-02 08:07:04|
[2024-11-02T08:07:13.984+0000] {spark_submit.py:495} INFO - |1        |29.25      |50.19   |2024-11-02 08:07:05|
[2024-11-02T08:07:13.984+0000] {spark_submit.py:495} INFO - |2        |29.64      |50.63   |2024-11-02 08:07:07|
[2024-11-02T08:07:13.996+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:13.996+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:07:13.996+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:07:14.421+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/25 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.25.508fa3ff-89f3-4b0f-8d31-aa961611c195.tmp
[2024-11-02T08:07:15.023+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.25.508fa3ff-89f3-4b0f-8d31-aa961611c195.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/25
[2024-11-02T08:07:15.047+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:07:15.048+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:07:15.048+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:07:15.049+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:07:15.049+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:07:07.549Z",
[2024-11-02T08:07:15.049+0000] {spark_submit.py:495} INFO - "batchId" : 25,
[2024-11-02T08:07:15.068+0000] {spark_submit.py:495} INFO - "numInputRows" : 10,
[2024-11-02T08:07:15.068+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8690362388111584,
[2024-11-02T08:07:15.068+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3365410318096764,
[2024-11-02T08:07:15.069+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:07:15.069+0000] {spark_submit.py:495} INFO - "addBatch" : 3446,
[2024-11-02T08:07:15.069+0000] {spark_submit.py:495} INFO - "commitOffsets" : 1120,
[2024-11-02T08:07:15.069+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:07:15.069+0000] {spark_submit.py:495} INFO - "latestOffset" : 199,
[2024-11-02T08:07:15.070+0000] {spark_submit.py:495} INFO - "queryPlanning" : 398,
[2024-11-02T08:07:15.070+0000] {spark_submit.py:495} INFO - "triggerExecution" : 7482,
[2024-11-02T08:07:15.070+0000] {spark_submit.py:495} INFO - "walCommit" : 2305
[2024-11-02T08:07:15.070+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:15.070+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:07:15.071+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:07:15.071+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:07:15.071+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:07:15.071+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:15.072+0000] {spark_submit.py:495} INFO - "0" : 6213
[2024-11-02T08:07:15.072+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:15.072+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:15.072+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:07:15.073+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:15.073+0000] {spark_submit.py:495} INFO - "0" : 6223
[2024-11-02T08:07:15.073+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:15.073+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:15.073+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:07:15.074+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:15.074+0000] {spark_submit.py:495} INFO - "0" : 6223
[2024-11-02T08:07:15.074+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:15.074+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:15.074+0000] {spark_submit.py:495} INFO - "numInputRows" : 10,
[2024-11-02T08:07:15.075+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.8690362388111584,
[2024-11-02T08:07:15.075+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 1.3365410318096764,
[2024-11-02T08:07:15.075+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:07:15.075+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:07:15.075+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:07:15.076+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:07:15.076+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:15.076+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:07:15.076+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:07:15.077+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:07:15.077+0000] {spark_submit.py:495} INFO - "numOutputRows" : 10
[2024-11-02T08:07:15.077+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:15.077+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:15.631+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/26 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.26.ece9153f-c47e-4720-b020-7dd105cca0be.tmp
[2024-11-02T08:07:17.142+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.26.ece9153f-c47e-4720-b020-7dd105cca0be.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/26
[2024-11-02T08:07:17.171+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1730534835234,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:07:17.281+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:17.369+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:17.507+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:17.537+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:17.706+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:17.720+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:17.856+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:07:17.858+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:07:17.898+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:07:17.906+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:07:17.906+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:07:17.907+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:07:17.932+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[108] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:07:17.995+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:17 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:07:18.116+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:07:18.163+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:18.166+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:07:18.166+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[108] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:07:18.166+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:18.183+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2024-11-02T08:07:18.198+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:18.237+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:07:18.426+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:18.638+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:18.665+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:18 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:20.091+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 1898 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:07:20.160+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 2.224 s
[2024-11-02T08:07:20.188+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:07:20.189+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-11-02T08:07:20.229+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2024-11-02T08:07:20.231+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 2.347020 s
[2024-11-02T08:07:20.231+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:07:20.386+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:20.418+0000] {spark_submit.py:495} INFO - Batch: 26
[2024-11-02T08:07:20.419+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:21.754+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:21.763+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:07:21.792+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:21.827+0000] {spark_submit.py:495} INFO - |3        |29.99      |50.42   |2024-11-02 08:07:08|
[2024-11-02T08:07:21.828+0000] {spark_submit.py:495} INFO - |4        |29.24      |50.89   |2024-11-02 08:07:09|
[2024-11-02T08:07:21.829+0000] {spark_submit.py:495} INFO - |5        |29.11      |50.38   |2024-11-02 08:07:10|
[2024-11-02T08:07:21.947+0000] {spark_submit.py:495} INFO - |1        |29.71      |50.78   |2024-11-02 08:07:11|
[2024-11-02T08:07:22.019+0000] {spark_submit.py:495} INFO - |2        |29.62      |50.03   |2024-11-02 08:07:12|
[2024-11-02T08:07:22.019+0000] {spark_submit.py:495} INFO - |3        |29.12      |50.74   |2024-11-02 08:07:14|
[2024-11-02T08:07:22.020+0000] {spark_submit.py:495} INFO - |4        |29.84      |50.67   |2024-11-02 08:07:15|
[2024-11-02T08:07:22.021+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:22.054+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:07:22.056+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:07:22.262+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/26 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.26.f6bbd724-c1ca-4bdc-bbf4-626595654b15.tmp
[2024-11-02T08:07:23.725+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.26.f6bbd724-c1ca-4bdc-bbf4-626595654b15.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/26
[2024-11-02T08:07:23.780+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:07:23.781+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:07:23.781+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:07:23.782+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:07:23.799+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:07:15.040Z",
[2024-11-02T08:07:23.800+0000] {spark_submit.py:495} INFO - "batchId" : 26,
[2024-11-02T08:07:23.801+0000] {spark_submit.py:495} INFO - "numInputRows" : 7,
[2024-11-02T08:07:23.801+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9344546789480711,
[2024-11-02T08:07:23.801+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8064516129032259,
[2024-11-02T08:07:23.801+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:07:23.802+0000] {spark_submit.py:495} INFO - "addBatch" : 4261,
[2024-11-02T08:07:23.802+0000] {spark_submit.py:495} INFO - "commitOffsets" : 2082,
[2024-11-02T08:07:23.820+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:07:23.821+0000] {spark_submit.py:495} INFO - "latestOffset" : 194,
[2024-11-02T08:07:23.821+0000] {spark_submit.py:495} INFO - "queryPlanning" : 199,
[2024-11-02T08:07:23.821+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8680,
[2024-11-02T08:07:23.821+0000] {spark_submit.py:495} INFO - "walCommit" : 1919
[2024-11-02T08:07:23.821+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:23.829+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:07:23.829+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:07:23.830+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:07:23.830+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:07:23.830+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:23.830+0000] {spark_submit.py:495} INFO - "0" : 6223
[2024-11-02T08:07:23.830+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:23.831+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:23.831+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:07:23.846+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:23.846+0000] {spark_submit.py:495} INFO - "0" : 6230
[2024-11-02T08:07:23.847+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:23.847+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:23.847+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:07:23.847+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:23.848+0000] {spark_submit.py:495} INFO - "0" : 6230
[2024-11-02T08:07:23.848+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:23.848+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:23.848+0000] {spark_submit.py:495} INFO - "numInputRows" : 7,
[2024-11-02T08:07:23.849+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.9344546789480711,
[2024-11-02T08:07:23.849+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.8064516129032259,
[2024-11-02T08:07:23.849+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:07:23.850+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:07:23.850+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:07:23.850+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:07:23.850+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:23.851+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:07:23.851+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:07:23.851+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:07:23.851+0000] {spark_submit.py:495} INFO - "numOutputRows" : 7
[2024-11-02T08:07:23.852+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:23.852+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:24.362+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/27 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.27.a4bca61a-34a1-4c59-965d-eae40b5cdbfc.tmp
[2024-11-02T08:07:26.166+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/.27.a4bca61a-34a1-4c59-965d-eae40b5cdbfc.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/offsets/27
[2024-11-02T08:07:26.170+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:26 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1730534843924,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-11-02T08:07:26.371+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:26.470+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:26.663+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:26.767+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:27.270+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:27.404+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-11-02T08:07:27.566+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.
[2024-11-02T08:07:27.576+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-11-02T08:07:27.661+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-02T08:07:27.690+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
[2024-11-02T08:07:27.696+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO DAGScheduler: Parents of final stage: List()
[2024-11-02T08:07:27.697+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO DAGScheduler: Missing parents: List()
[2024-11-02T08:07:27.697+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[112] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-02T08:07:27.739+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 23.3 KiB, free 434.3 MiB)
[2024-11-02T08:07:27.782+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 434.3 MiB)
[2024-11-02T08:07:27.783+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 7d12808d40db:43395 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:27.877+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
[2024-11-02T08:07:27.987+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[112] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-02T08:07:28.001+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-11-02T08:07:28.001+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:27 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (172.22.0.4, executor 0, partition 0, PROCESS_LOCAL, 8333 bytes)
[2024-11-02T08:07:28.869+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:28 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.22.0.4:37367 (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:07:29.332+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 1366 ms on 172.22.0.4 (executor 0) (1/1)
[2024-11-02T08:07:29.333+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-11-02T08:07:29.366+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 1.699 s
[2024-11-02T08:07:29.372+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-02T08:07:29.392+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-11-02T08:07:29.413+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 1.821797 s
[2024-11-02T08:07:29.416+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.
[2024-11-02T08:07:29.417+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:29.417+0000] {spark_submit.py:495} INFO - Batch: 27
[2024-11-02T08:07:29.417+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2024-11-02T08:07:29.893+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:29.905+0000] {spark_submit.py:495} INFO - |sensor_id|temperature|humidity|timestamp          |
[2024-11-02T08:07:29.936+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:29.937+0000] {spark_submit.py:495} INFO - |5        |29.64      |50.41   |2024-11-02 08:07:16|
[2024-11-02T08:07:29.938+0000] {spark_submit.py:495} INFO - |1        |29.54      |50.42   |2024-11-02 08:07:17|
[2024-11-02T08:07:29.938+0000] {spark_submit.py:495} INFO - +---------+-----------+--------+-------------------+
[2024-11-02T08:07:29.938+0000] {spark_submit.py:495} INFO - 
[2024-11-02T08:07:29.939+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=false]] committed.
[2024-11-02T08:07:30.582+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/27 using temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.27.a3330554-876a-49a7-a8e2-a2287b09a027.tmp
[2024-11-02T08:07:32.567+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/.27.a3330554-876a-49a7-a8e2-a2287b09a027.tmp to file:/tmp/temporary-a3e43ddc-ab02-4e0f-9d73-541c67281f9c/commits/27
[2024-11-02T08:07:32.648+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:32 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:07:32.648+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:07:32.648+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:07:32.649+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:07:32.649+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:07:23.770Z",
[2024-11-02T08:07:32.649+0000] {spark_submit.py:495} INFO - "batchId" : 27,
[2024-11-02T08:07:32.649+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-02T08:07:32.649+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.2290950744558992,
[2024-11-02T08:07:32.650+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.22750540325332727,
[2024-11-02T08:07:32.650+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:07:32.650+0000] {spark_submit.py:495} INFO - "addBatch" : 3346,
[2024-11-02T08:07:32.650+0000] {spark_submit.py:495} INFO - "commitOffsets" : 2636,
[2024-11-02T08:07:32.650+0000] {spark_submit.py:495} INFO - "getBatch" : 0,
[2024-11-02T08:07:32.651+0000] {spark_submit.py:495} INFO - "latestOffset" : 154,
[2024-11-02T08:07:32.651+0000] {spark_submit.py:495} INFO - "queryPlanning" : 429,
[2024-11-02T08:07:32.651+0000] {spark_submit.py:495} INFO - "triggerExecution" : 8791,
[2024-11-02T08:07:32.651+0000] {spark_submit.py:495} INFO - "walCommit" : 2223
[2024-11-02T08:07:32.651+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:32.652+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:07:32.652+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:07:32.652+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:07:32.652+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:07:32.652+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:32.652+0000] {spark_submit.py:495} INFO - "0" : 6230
[2024-11-02T08:07:32.653+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:32.653+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:32.653+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:07:32.653+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:32.653+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:32.654+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:32.654+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:32.654+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:07:32.654+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:32.654+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:32.655+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:32.655+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:32.655+0000] {spark_submit.py:495} INFO - "numInputRows" : 2,
[2024-11-02T08:07:32.655+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.2290950744558992,
[2024-11-02T08:07:32.655+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.22750540325332727,
[2024-11-02T08:07:32.656+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:07:32.656+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:07:32.656+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:07:32.656+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:07:32.656+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:32.657+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:07:32.657+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:07:32.657+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:07:32.657+0000] {spark_submit.py:495} INFO - "numOutputRows" : 2
[2024-11-02T08:07:32.657+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:32.658+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:42.771+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:07:42.923+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:07:42.959+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:07:43.033+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:07:43.034+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:07:41.914Z",
[2024-11-02T08:07:43.034+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:07:43.035+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:07:43.203+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:07:43.204+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:07:43.204+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:07:43.205+0000] {spark_submit.py:495} INFO - "latestOffset" : 714,
[2024-11-02T08:07:43.206+0000] {spark_submit.py:495} INFO - "triggerExecution" : 744
[2024-11-02T08:07:43.207+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:43.207+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:07:43.208+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:07:43.291+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:07:43.292+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:07:43.345+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:43.345+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:43.346+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:43.347+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:43.348+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:07:43.348+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:43.349+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:43.350+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:43.350+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:43.351+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:07:43.351+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:43.446+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:43.447+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:43.447+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:43.448+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:07:43.448+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:07:43.449+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:07:43.449+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:07:43.449+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:07:43.450+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:07:43.450+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:07:43.450+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:43.450+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:07:43.451+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:07:43.451+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:07:43.451+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:07:43.452+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:43.452+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:52.738+0000] {spark_submit.py:495} INFO - 24/11/02 08:07:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:07:52.758+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:07:52.760+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:07:52.761+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:07:52.780+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:07:52.295Z",
[2024-11-02T08:07:52.785+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:07:52.786+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:07:52.787+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:07:52.788+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:07:52.789+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:07:52.789+0000] {spark_submit.py:495} INFO - "latestOffset" : 435,
[2024-11-02T08:07:52.809+0000] {spark_submit.py:495} INFO - "triggerExecution" : 435
[2024-11-02T08:07:52.810+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:52.819+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:07:52.819+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:07:52.820+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:07:52.820+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:07:52.820+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:52.820+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:52.821+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:52.821+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:52.821+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:07:52.821+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:52.837+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:52.838+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:52.838+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:52.839+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:07:52.839+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:07:52.839+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:07:52.839+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:52.840+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:07:52.840+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:07:52.840+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:07:52.846+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:07:52.847+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:07:52.847+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:07:52.847+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:07:52.847+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:07:52.848+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:52.848+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:07:52.848+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:07:52.848+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:07:52.849+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:07:52.849+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:07:52.849+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:02.758+0000] {spark_submit.py:495} INFO - 24/11/02 08:08:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:08:02.759+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:08:02.764+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:08:02.765+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:08:02.773+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:08:02.731Z",
[2024-11-02T08:08:02.774+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:08:02.777+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:02.777+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:02.777+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:02.778+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:08:02.778+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-02T08:08:02.778+0000] {spark_submit.py:495} INFO - "triggerExecution" : 16
[2024-11-02T08:08:02.778+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:02.783+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:08:02.784+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:08:02.785+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:08:02.785+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:08:02.785+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:02.788+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:02.789+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:02.790+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:02.798+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:08:02.802+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:02.803+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:02.803+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:02.803+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:02.803+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:08:02.804+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:02.804+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:02.804+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:02.805+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:02.805+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:02.805+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:02.806+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:02.812+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:08:02.816+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:08:02.818+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:08:02.819+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:08:02.819+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:02.823+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:08:02.824+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:08:02.824+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:08:02.824+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:08:02.824+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:02.824+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:12.796+0000] {spark_submit.py:495} INFO - 24/11/02 08:08:12 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:08:12.797+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:08:12.797+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:08:12.801+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:08:12.804+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:08:12.733Z",
[2024-11-02T08:08:12.805+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:08:12.805+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:12.805+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:12.805+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:12.805+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:08:12.806+0000] {spark_submit.py:495} INFO - "latestOffset" : 57,
[2024-11-02T08:08:12.806+0000] {spark_submit.py:495} INFO - "triggerExecution" : 57
[2024-11-02T08:08:12.806+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:12.806+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:08:12.816+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:08:12.816+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:08:12.816+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:08:12.817+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:12.817+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:12.817+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:12.817+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:12.818+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:08:12.818+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:12.818+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:12.818+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:12.819+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:12.819+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:08:12.826+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:12.831+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:12.837+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:12.841+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:12.842+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:12.842+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:12.842+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:12.842+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:08:12.843+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:08:12.843+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:08:12.843+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:08:12.843+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:12.843+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:08:12.844+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:08:12.844+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:08:12.863+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:08:12.864+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:12.865+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:22.897+0000] {spark_submit.py:495} INFO - 24/11/02 08:08:22 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:08:22.919+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:08:22.929+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:08:22.930+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:08:22.930+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:08:22.729Z",
[2024-11-02T08:08:22.930+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:08:22.930+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:22.931+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:22.931+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:22.931+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:08:22.931+0000] {spark_submit.py:495} INFO - "latestOffset" : 149,
[2024-11-02T08:08:22.931+0000] {spark_submit.py:495} INFO - "triggerExecution" : 149
[2024-11-02T08:08:22.931+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:22.932+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:08:22.932+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:08:22.932+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:08:22.932+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:08:22.932+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:22.932+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:22.933+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:22.933+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:22.933+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:08:22.933+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:22.933+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:22.933+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:22.934+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:22.934+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:08:22.934+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:22.934+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:22.934+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:22.934+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:22.935+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:22.935+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:22.935+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:22.935+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:08:22.935+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:08:22.936+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:08:22.936+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:08:22.960+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:22.963+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:08:22.963+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:08:22.977+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:08:22.980+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:08:22.980+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:22.980+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:32.900+0000] {spark_submit.py:495} INFO - 24/11/02 08:08:32 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:08:32.901+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:08:32.902+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:08:32.902+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:08:32.903+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:08:32.865Z",
[2024-11-02T08:08:32.903+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:08:32.903+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:32.903+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:32.903+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:32.904+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:08:32.904+0000] {spark_submit.py:495} INFO - "latestOffset" : 22,
[2024-11-02T08:08:32.904+0000] {spark_submit.py:495} INFO - "triggerExecution" : 22
[2024-11-02T08:08:32.905+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:32.906+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:08:32.907+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:08:32.908+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:08:32.908+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:08:32.908+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:32.913+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:32.914+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:32.914+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:32.914+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:08:32.914+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:32.914+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:32.915+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:32.915+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:32.915+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:08:32.915+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:32.915+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:32.915+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:32.916+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:32.916+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:32.916+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:32.916+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:32.916+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:08:32.916+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:08:32.917+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:08:32.917+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:08:32.917+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:32.917+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:08:32.917+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:08:32.917+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:08:32.918+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:08:32.927+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:32.928+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:42.932+0000] {spark_submit.py:495} INFO - 24/11/02 08:08:42 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:08:42.934+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:08:42.936+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:08:42.936+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:08:42.936+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:08:42.890Z",
[2024-11-02T08:08:42.936+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:08:42.936+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:42.937+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:42.937+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:42.937+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:08:42.937+0000] {spark_submit.py:495} INFO - "latestOffset" : 27,
[2024-11-02T08:08:42.937+0000] {spark_submit.py:495} INFO - "triggerExecution" : 33
[2024-11-02T08:08:42.937+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:42.938+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:08:42.938+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:08:42.938+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:08:42.938+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:08:42.938+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:42.938+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:42.939+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:42.940+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:08:42.940+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:42.940+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:42.940+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:42.940+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:42.941+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:42.941+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:42.941+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:42.941+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:08:42.941+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:08:42.941+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:08:42.942+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:08:42.942+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:42.942+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:08:42.942+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:08:42.942+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:08:42.943+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:08:42.943+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:42.943+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:52.975+0000] {spark_submit.py:495} INFO - 24/11/02 08:08:52 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:08:52.976+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:08:52.976+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:08:52.977+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:08:52.977+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:08:52.918Z",
[2024-11-02T08:08:52.977+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:08:52.977+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:52.977+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:52.978+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:52.978+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:08:52.978+0000] {spark_submit.py:495} INFO - "latestOffset" : 40,
[2024-11-02T08:08:52.978+0000] {spark_submit.py:495} INFO - "triggerExecution" : 40
[2024-11-02T08:08:52.978+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:52.979+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:08:52.979+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:08:52.979+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:08:52.980+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:08:52.980+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:52.980+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:52.980+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:52.980+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:52.981+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:08:52.981+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:52.981+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:52.981+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:52.981+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:52.982+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:08:52.982+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:08:52.982+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:08:52.982+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:52.982+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:08:52.983+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:08:52.983+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:08:52.983+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:08:52.983+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:08:52.983+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:08:52.984+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:08:52.984+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:08:52.984+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:52.984+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:08:52.984+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:08:52.985+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:08:52.985+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:08:52.985+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:08:52.985+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:03.015+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:02 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:09:03.021+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:09:03.025+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:09:03.027+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:09:03.027+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:09:02.803Z",
[2024-11-02T08:09:03.028+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:09:03.028+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:03.028+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:03.028+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:03.029+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:09:03.029+0000] {spark_submit.py:495} INFO - "latestOffset" : 170,
[2024-11-02T08:09:03.047+0000] {spark_submit.py:495} INFO - "triggerExecution" : 170
[2024-11-02T08:09:03.048+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:03.050+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:09:03.051+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:09:03.051+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:09:03.053+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:09:03.053+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:03.054+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:03.057+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:03.060+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:03.061+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:09:03.061+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:03.061+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:03.061+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:03.061+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:03.062+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:09:03.069+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:03.070+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:03.070+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:03.074+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:03.075+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:03.075+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:03.075+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:03.075+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:09:03.075+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:09:03.076+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:09:03.076+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:09:03.076+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:03.076+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:09:03.076+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:09:03.076+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:09:03.077+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:09:03.077+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:03.077+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:05.887+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:05 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:09:05.897+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:05 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:09:06.231+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:06 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 7d12808d40db:43395 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:09:06.240+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:06 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.22.0.4:37367 in memory (size: 11.0 KiB, free: 434.4 MiB)
[2024-11-02T08:09:13.028+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:09:13.029+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:09:13.029+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:09:13.050+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:09:13.055+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:09:12.974Z",
[2024-11-02T08:09:13.055+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:09:13.065+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:13.074+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:13.078+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:13.099+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:09:13.100+0000] {spark_submit.py:495} INFO - "latestOffset" : 47,
[2024-11-02T08:09:13.100+0000] {spark_submit.py:495} INFO - "triggerExecution" : 47
[2024-11-02T08:09:13.100+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:13.100+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:09:13.101+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:09:13.101+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:09:13.101+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:09:13.101+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:13.101+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:13.102+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:13.102+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:13.102+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:09:13.102+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:13.102+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:13.126+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:13.130+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:13.131+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:09:13.131+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:13.131+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:13.131+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:13.131+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:13.132+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:13.132+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:13.132+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:13.132+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:09:13.132+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:09:13.133+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:09:13.133+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:09:13.133+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:13.133+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:09:13.134+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:09:13.134+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:09:13.134+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:09:13.134+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:13.134+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:23.443+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:09:23.468+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:09:23.468+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:09:23.469+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:09:23.469+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:09:22.991Z",
[2024-11-02T08:09:23.469+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:09:23.469+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:23.469+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:23.470+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:23.470+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:09:23.470+0000] {spark_submit.py:495} INFO - "latestOffset" : 416,
[2024-11-02T08:09:23.470+0000] {spark_submit.py:495} INFO - "triggerExecution" : 416
[2024-11-02T08:09:23.471+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:23.471+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:09:23.471+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:09:23.471+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:09:23.471+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:09:23.472+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:23.472+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:23.472+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:23.472+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:23.472+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:09:23.473+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:23.473+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:23.473+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:23.473+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:23.473+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:09:23.474+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:23.474+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:23.474+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:23.474+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:23.474+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:23.475+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:23.475+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:23.475+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:09:23.475+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:09:23.475+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:09:23.476+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:09:23.476+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:23.476+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:09:23.476+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:09:23.476+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:09:23.476+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:09:23.477+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:23.477+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:33.430+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:09:33.430+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:09:33.431+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:09:33.431+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:09:33.431+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:09:33.377Z",
[2024-11-02T08:09:33.431+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:09:33.432+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:33.432+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:33.450+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:33.451+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:09:33.451+0000] {spark_submit.py:495} INFO - "latestOffset" : 49,
[2024-11-02T08:09:33.451+0000] {spark_submit.py:495} INFO - "triggerExecution" : 49
[2024-11-02T08:09:33.451+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:33.452+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:09:33.452+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:09:33.452+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:09:33.452+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:09:33.452+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:33.453+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:33.453+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:33.454+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:33.454+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:09:33.467+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:33.478+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:33.478+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:33.478+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:33.478+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:09:33.478+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:33.479+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:33.479+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:33.479+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:33.479+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:33.479+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:33.479+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:33.480+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:09:33.480+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:09:33.480+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:09:33.480+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:09:33.480+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:33.480+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:09:33.481+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:09:33.481+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:09:33.481+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:09:33.481+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:33.481+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:43.567+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:09:43.567+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:09:43.568+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:09:43.568+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:09:43.568+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:09:43.430Z",
[2024-11-02T08:09:43.568+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:09:43.569+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:43.569+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:43.569+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:43.569+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:09:43.570+0000] {spark_submit.py:495} INFO - "latestOffset" : 56,
[2024-11-02T08:09:43.570+0000] {spark_submit.py:495} INFO - "triggerExecution" : 59
[2024-11-02T08:09:43.570+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:43.570+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:09:43.570+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:09:43.570+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:09:43.571+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:09:43.571+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:43.571+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:43.571+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:43.571+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:43.572+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:09:43.572+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:43.572+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:43.572+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:43.572+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:43.572+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:09:43.573+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:43.573+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:43.573+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:43.573+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:43.573+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:43.573+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:43.574+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:43.574+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:09:43.574+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:09:43.574+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:09:43.574+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:09:43.574+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:43.575+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:09:43.575+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:09:43.575+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:09:43.575+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:09:43.575+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:43.576+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:53.564+0000] {spark_submit.py:495} INFO - 24/11/02 08:09:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:09:53.568+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:09:53.569+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:09:53.569+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:09:53.570+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:09:53.490Z",
[2024-11-02T08:09:53.593+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:09:53.596+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:53.596+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:53.596+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:53.596+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:09:53.597+0000] {spark_submit.py:495} INFO - "latestOffset" : 64,
[2024-11-02T08:09:53.597+0000] {spark_submit.py:495} INFO - "triggerExecution" : 64
[2024-11-02T08:09:53.597+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:53.597+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:09:53.597+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:09:53.598+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:09:53.598+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:09:53.598+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:53.598+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:53.598+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:53.599+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:53.599+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:09:53.599+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:53.599+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:53.599+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:53.600+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:53.600+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:09:53.600+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:09:53.600+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:09:53.600+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:53.601+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:09:53.601+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:09:53.601+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:09:53.601+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:09:53.601+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:09:53.602+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:09:53.602+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:09:53.602+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:09:53.602+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:53.602+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:09:53.602+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:09:53.603+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:09:53.603+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:09:53.603+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:09:53.603+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:03.584+0000] {spark_submit.py:495} INFO - 24/11/02 08:10:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:10:03.585+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:10:03.589+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:10:03.592+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:10:03.592+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:10:03.563Z",
[2024-11-02T08:10:03.592+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:10:03.593+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:03.593+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:03.593+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:03.602+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:10:03.603+0000] {spark_submit.py:495} INFO - "latestOffset" : 15,
[2024-11-02T08:10:03.603+0000] {spark_submit.py:495} INFO - "triggerExecution" : 15
[2024-11-02T08:10:03.603+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:03.603+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:10:03.604+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:10:03.604+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:10:03.604+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:10:03.614+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:03.614+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:03.615+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:03.615+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:03.615+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:10:03.615+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:03.615+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:03.616+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:03.616+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:03.616+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:10:03.616+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:03.616+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:03.616+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:03.617+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:03.617+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:03.618+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:03.619+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:03.619+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:10:03.619+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:10:03.619+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:10:03.620+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:10:03.620+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:03.620+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:10:03.625+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:10:03.626+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:10:03.627+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:10:03.628+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:03.630+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:13.635+0000] {spark_submit.py:495} INFO - 24/11/02 08:10:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:10:13.644+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:10:13.648+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:10:13.648+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:10:13.662+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:10:13.556Z",
[2024-11-02T08:10:13.667+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:10:13.679+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:13.681+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:13.681+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:13.684+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:10:13.684+0000] {spark_submit.py:495} INFO - "latestOffset" : 38,
[2024-11-02T08:10:13.685+0000] {spark_submit.py:495} INFO - "triggerExecution" : 38
[2024-11-02T08:10:13.685+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:13.688+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:10:13.689+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:10:13.691+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:10:13.691+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:10:13.691+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:13.691+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:13.691+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:13.692+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:13.692+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:10:13.692+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:13.692+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:13.692+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:13.692+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:13.693+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:10:13.693+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:13.693+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:13.693+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:13.693+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:13.693+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:10:13.694+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:13.695+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:10:13.695+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:10:13.695+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:10:13.695+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:10:13.695+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:13.695+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:23.598+0000] {spark_submit.py:495} INFO - 24/11/02 08:10:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:10:23.606+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:10:23.607+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:10:23.609+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:10:23.610+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:10:23.528Z",
[2024-11-02T08:10:23.610+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:10:23.611+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:23.611+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:23.611+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:23.611+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:10:23.612+0000] {spark_submit.py:495} INFO - "latestOffset" : 66,
[2024-11-02T08:10:23.612+0000] {spark_submit.py:495} INFO - "triggerExecution" : 66
[2024-11-02T08:10:23.612+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:23.612+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:10:23.613+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:10:23.613+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:10:23.613+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:10:23.613+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:23.614+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:23.614+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:23.630+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:23.631+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:10:23.631+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:23.632+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:23.632+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:23.633+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:23.633+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:10:23.633+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:23.633+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:23.634+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:23.634+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:23.634+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:23.635+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:23.651+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:23.660+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:10:23.664+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:10:23.667+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:10:23.667+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:10:23.667+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:23.668+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:10:23.668+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:10:23.668+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:10:23.669+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:10:23.669+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:23.669+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:33.701+0000] {spark_submit.py:495} INFO - 24/11/02 08:10:33 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:10:33.706+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:10:33.710+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:10:33.717+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:10:33.717+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:10:33.533Z",
[2024-11-02T08:10:33.717+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:10:33.717+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:33.718+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:33.718+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:33.718+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:10:33.718+0000] {spark_submit.py:495} INFO - "latestOffset" : 151,
[2024-11-02T08:10:33.718+0000] {spark_submit.py:495} INFO - "triggerExecution" : 152
[2024-11-02T08:10:33.719+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:33.719+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:10:33.719+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:10:33.719+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:10:33.719+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:10:33.720+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:33.720+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:33.720+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:33.720+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:33.720+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:10:33.721+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:33.721+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:33.721+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:33.721+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:33.751+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:10:33.751+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:33.751+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:33.751+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:33.752+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:33.752+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:33.752+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:33.752+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:33.752+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:10:33.752+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:10:33.753+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:10:33.754+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:33.754+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:43.703+0000] {spark_submit.py:495} INFO - 24/11/02 08:10:43 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:10:43.708+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:10:43.708+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:10:43.709+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:10:43.709+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:10:43.676Z",
[2024-11-02T08:10:43.709+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:10:43.709+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:43.709+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:43.710+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:43.710+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:10:43.710+0000] {spark_submit.py:495} INFO - "latestOffset" : 24,
[2024-11-02T08:10:43.710+0000] {spark_submit.py:495} INFO - "triggerExecution" : 24
[2024-11-02T08:10:43.710+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:43.711+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:10:43.711+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:10:43.711+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:10:43.711+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:10:43.711+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:43.711+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:43.712+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:43.712+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:43.712+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:10:43.712+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:43.712+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:43.712+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:43.713+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:43.713+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:10:43.713+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:43.713+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:43.713+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:43.713+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:43.714+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:43.714+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:43.714+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:43.714+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:10:43.714+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:10:43.715+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:10:43.715+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:10:43.716+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:43.716+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:10:43.716+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:10:43.716+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:10:43.716+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:10:43.717+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:43.717+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:53.720+0000] {spark_submit.py:495} INFO - 24/11/02 08:10:53 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:10:53.721+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:10:53.721+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:10:53.721+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:10:53.721+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:10:53.681Z",
[2024-11-02T08:10:53.722+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:10:53.722+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:53.722+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:53.722+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:53.723+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:10:53.723+0000] {spark_submit.py:495} INFO - "latestOffset" : 32,
[2024-11-02T08:10:53.723+0000] {spark_submit.py:495} INFO - "triggerExecution" : 33
[2024-11-02T08:10:53.723+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:53.723+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:10:53.731+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:10:53.737+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:10:53.774+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:10:53.783+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:53.784+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:53.784+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:53.784+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:53.788+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:10:53.788+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:53.789+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:53.789+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:53.790+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:53.790+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:10:53.796+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:10:53.797+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:10:53.806+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:53.806+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:10:53.807+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:10:53.807+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:10:53.807+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:10:53.812+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:10:53.813+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:10:53.816+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:10:53.816+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:10:53.817+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:53.826+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:10:53.827+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:10:53.828+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:10:53.828+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:10:53.828+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:10:53.829+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:03.906+0000] {spark_submit.py:495} INFO - 24/11/02 08:11:03 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:11:03.908+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:11:03.941+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:11:03.964+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:11:03.973+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:11:03.668Z",
[2024-11-02T08:11:03.973+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:11:03.974+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:03.979+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:03.980+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:03.980+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:11:03.980+0000] {spark_submit.py:495} INFO - "latestOffset" : 235,
[2024-11-02T08:11:03.992+0000] {spark_submit.py:495} INFO - "triggerExecution" : 235
[2024-11-02T08:11:03.992+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:03.993+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:11:03.993+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:11:03.993+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:11:03.993+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:11:03.994+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:03.994+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:03.994+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:03.995+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:04.005+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:11:04.005+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:04.006+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:04.006+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:04.006+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:04.006+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:11:04.006+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:04.007+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:04.007+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:04.007+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:04.007+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:04.008+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:04.008+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:04.008+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:11:04.008+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:11:04.009+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:11:04.009+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:11:04.009+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:04.010+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:11:04.010+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:11:04.010+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:11:04.010+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:11:04.011+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:04.011+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:13.987+0000] {spark_submit.py:495} INFO - 24/11/02 08:11:13 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:11:13.988+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:11:13.988+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:11:13.988+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:11:13.988+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:11:13.910Z",
[2024-11-02T08:11:13.988+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:11:13.989+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:13.989+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:13.989+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:13.989+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:11:13.989+0000] {spark_submit.py:495} INFO - "latestOffset" : 47,
[2024-11-02T08:11:13.990+0000] {spark_submit.py:495} INFO - "triggerExecution" : 47
[2024-11-02T08:11:13.990+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:13.990+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:11:13.990+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:11:13.990+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:11:13.990+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:11:13.991+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:13.991+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:13.991+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:13.991+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:13.991+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:11:13.991+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:13.992+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:13.992+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:14.032+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:14.034+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:11:14.035+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:14.035+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:14.035+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:14.036+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:14.036+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:14.036+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:14.036+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:14.036+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:11:14.037+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:11:14.037+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:11:14.037+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:11:14.037+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:14.038+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:11:14.038+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:11:14.038+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:11:14.038+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:11:14.038+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:14.039+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:23.981+0000] {spark_submit.py:495} INFO - 24/11/02 08:11:23 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:11:23.983+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:11:23.984+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:11:23.984+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:11:23.984+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:11:23.963Z",
[2024-11-02T08:11:23.985+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:11:23.985+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:23.985+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:23.985+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:23.985+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:11:23.985+0000] {spark_submit.py:495} INFO - "latestOffset" : 16,
[2024-11-02T08:11:23.986+0000] {spark_submit.py:495} INFO - "triggerExecution" : 16
[2024-11-02T08:11:23.986+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:23.986+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:11:23.986+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:11:23.986+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:11:23.986+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:11:23.987+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:23.987+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:23.987+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:23.987+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:23.987+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:11:23.987+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:23.988+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:23.989+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:23.989+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:23.989+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:23.989+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:23.989+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:11:23.989+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:11:23.990+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:11:23.990+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:11:23.990+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:23.990+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:11:23.990+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:11:23.990+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:11:23.991+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:11:23.991+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:23.991+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:34.061+0000] {spark_submit.py:495} INFO - 24/11/02 08:11:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:11:34.062+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:11:34.063+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:11:34.063+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:11:34.063+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:11:34.035Z",
[2024-11-02T08:11:34.064+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:11:34.064+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:34.064+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:34.064+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:34.064+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:11:34.065+0000] {spark_submit.py:495} INFO - "latestOffset" : 24,
[2024-11-02T08:11:34.065+0000] {spark_submit.py:495} INFO - "triggerExecution" : 24
[2024-11-02T08:11:34.065+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:34.065+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:11:34.065+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:11:34.065+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:11:34.066+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:11:34.066+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:34.066+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:34.066+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:34.066+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:34.067+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:11:34.067+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:34.067+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:34.067+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:34.067+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:34.067+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:11:34.068+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:34.068+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:34.068+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:34.068+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:34.068+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:34.068+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:34.069+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:34.069+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:11:34.069+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:11:34.069+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:11:34.069+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:11:34.070+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:34.070+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:11:34.070+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:11:34.070+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:11:34.071+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:11:34.071+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:34.071+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:44.149+0000] {spark_submit.py:495} INFO - 24/11/02 08:11:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:11:44.150+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:11:44.151+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:11:44.151+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:11:44.151+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:11:44.073Z",
[2024-11-02T08:11:44.151+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:11:44.151+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:44.152+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:44.152+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:44.152+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:11:44.152+0000] {spark_submit.py:495} INFO - "latestOffset" : 66,
[2024-11-02T08:11:44.152+0000] {spark_submit.py:495} INFO - "triggerExecution" : 66
[2024-11-02T08:11:44.152+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:44.153+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:11:44.153+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:11:44.153+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:11:44.153+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:11:44.153+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:44.153+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:44.154+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:44.154+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:44.154+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:11:44.154+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:44.154+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:44.154+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:44.155+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:44.155+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:11:44.155+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:44.155+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:44.155+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:44.155+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:44.156+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:44.156+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:44.156+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:44.156+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:11:44.156+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:11:44.156+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:11:44.157+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:11:44.157+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:44.157+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:11:44.157+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:11:44.157+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:11:44.157+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:11:44.158+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:44.158+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:54.158+0000] {spark_submit.py:495} INFO - 24/11/02 08:11:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:11:54.163+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:11:54.163+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:11:54.163+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:11:54.164+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:11:54.120Z",
[2024-11-02T08:11:54.164+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:11:54.164+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:54.164+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:54.165+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:54.165+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:11:54.165+0000] {spark_submit.py:495} INFO - "latestOffset" : 35,
[2024-11-02T08:11:54.165+0000] {spark_submit.py:495} INFO - "triggerExecution" : 37
[2024-11-02T08:11:54.166+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:54.166+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:11:54.166+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:11:54.166+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:11:54.167+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:11:54.167+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:54.167+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:54.167+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:54.168+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:54.168+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:11:54.168+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:54.168+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:54.168+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:54.168+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:54.169+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:11:54.169+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:11:54.169+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:11:54.169+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:54.169+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:11:54.170+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:11:54.170+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:11:54.170+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:11:54.170+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:11:54.171+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:11:54.171+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:11:54.171+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:11:54.171+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:54.171+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:11:54.172+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:11:54.172+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:11:54.172+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:11:54.173+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:11:54.173+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:04.161+0000] {spark_submit.py:495} INFO - 24/11/02 08:12:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:12:04.164+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:12:04.166+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:12:04.166+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:12:04.170+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:12:04.129Z",
[2024-11-02T08:12:04.171+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:12:04.172+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:04.172+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:04.172+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:04.172+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:12:04.180+0000] {spark_submit.py:495} INFO - "latestOffset" : 27,
[2024-11-02T08:12:04.183+0000] {spark_submit.py:495} INFO - "triggerExecution" : 28
[2024-11-02T08:12:04.183+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:04.184+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:12:04.186+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:12:04.186+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:12:04.187+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:12:04.191+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:04.197+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:04.199+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:04.201+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:04.202+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:12:04.202+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:04.202+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:04.203+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:04.203+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:04.203+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:12:04.203+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:04.203+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:04.211+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:04.213+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:04.213+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:04.215+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:04.215+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:04.215+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:12:04.216+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:12:04.216+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:12:04.216+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:12:04.216+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:04.216+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:12:04.217+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:12:04.217+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:12:04.217+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:12:04.217+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:04.217+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:14.184+0000] {spark_submit.py:495} INFO - 24/11/02 08:12:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:12:14.195+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:12:14.196+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:12:14.200+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:12:14.200+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:12:14.146Z",
[2024-11-02T08:12:14.200+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:12:14.200+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:14.200+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:14.201+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:14.201+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:12:14.201+0000] {spark_submit.py:495} INFO - "latestOffset" : 33,
[2024-11-02T08:12:14.201+0000] {spark_submit.py:495} INFO - "triggerExecution" : 33
[2024-11-02T08:12:14.201+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:14.202+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:12:14.202+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:12:14.202+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:12:14.202+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:12:14.219+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:14.220+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:14.228+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:14.228+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:14.228+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:12:14.228+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:14.229+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:14.229+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:14.229+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:14.230+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:12:14.230+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:14.230+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:14.230+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:14.230+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:14.230+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:14.231+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:14.231+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:14.231+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:12:14.231+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:12:14.231+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:12:14.231+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:12:14.232+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:14.241+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:12:14.244+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:12:14.245+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:12:14.245+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:12:14.245+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:14.248+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:24.217+0000] {spark_submit.py:495} INFO - 24/11/02 08:12:24 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:12:24.218+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:12:24.222+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:12:24.223+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:12:24.234+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:12:24.174Z",
[2024-11-02T08:12:24.234+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:12:24.234+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:24.234+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:24.234+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:24.234+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:12:24.235+0000] {spark_submit.py:495} INFO - "latestOffset" : 41,
[2024-11-02T08:12:24.235+0000] {spark_submit.py:495} INFO - "triggerExecution" : 42
[2024-11-02T08:12:24.235+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:24.235+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:12:24.235+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:12:24.235+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:12:24.236+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:12:24.236+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:24.236+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:24.236+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:24.236+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:24.236+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:24.237+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:24.238+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:24.238+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:24.238+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:24.249+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:24.250+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:24.250+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:12:24.251+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:12:24.252+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:12:24.253+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:12:24.254+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:24.255+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:12:24.256+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:12:24.257+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:12:24.258+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:12:24.259+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:24.260+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:34.407+0000] {spark_submit.py:495} INFO - 24/11/02 08:12:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:12:34.408+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:12:34.410+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:12:34.410+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:12:34.411+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:12:34.259Z",
[2024-11-02T08:12:34.411+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:12:34.412+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:34.412+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:34.413+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:34.413+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:12:34.420+0000] {spark_submit.py:495} INFO - "latestOffset" : 92,
[2024-11-02T08:12:34.422+0000] {spark_submit.py:495} INFO - "triggerExecution" : 92
[2024-11-02T08:12:34.423+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:34.423+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:12:34.430+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:12:34.430+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:12:34.431+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:12:34.431+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:34.431+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:34.432+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:34.432+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:34.432+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:12:34.433+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:34.433+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:34.433+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:34.433+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:34.434+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:12:34.434+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:34.435+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:34.435+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:34.435+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:34.435+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:34.436+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:34.436+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:34.436+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:12:34.437+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:12:34.437+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:12:34.437+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:12:34.437+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:34.438+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:12:34.438+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:12:34.438+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:12:34.438+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:12:34.438+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:34.439+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:44.419+0000] {spark_submit.py:495} INFO - 24/11/02 08:12:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:12:44.420+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:12:44.420+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:12:44.421+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:12:44.429+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:12:44.377Z",
[2024-11-02T08:12:44.430+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:12:44.430+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:44.435+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:44.435+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:44.438+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:12:44.438+0000] {spark_submit.py:495} INFO - "latestOffset" : 39,
[2024-11-02T08:12:44.442+0000] {spark_submit.py:495} INFO - "triggerExecution" : 39
[2024-11-02T08:12:44.444+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:44.444+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:12:44.445+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:12:44.445+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:12:44.446+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:12:44.446+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:44.447+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:44.447+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:44.448+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:44.449+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:12:44.454+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:44.456+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:44.460+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:44.460+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:44.460+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:12:44.460+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:44.461+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:44.461+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:44.461+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:44.462+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:44.462+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:44.462+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:44.463+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:12:44.464+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:12:44.478+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:12:44.479+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:12:44.483+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:44.483+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:12:44.484+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:12:44.485+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:12:44.486+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:12:44.486+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:44.487+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:54.548+0000] {spark_submit.py:495} INFO - 24/11/02 08:12:54 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:12:54.561+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:12:54.562+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:12:54.590+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:12:54.616+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:12:54.386Z",
[2024-11-02T08:12:54.645+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:12:54.645+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:54.652+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:54.653+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:54.661+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:12:54.661+0000] {spark_submit.py:495} INFO - "latestOffset" : 157,
[2024-11-02T08:12:54.662+0000] {spark_submit.py:495} INFO - "triggerExecution" : 157
[2024-11-02T08:12:54.678+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:54.679+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:12:54.679+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:12:54.679+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:12:54.680+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:12:54.680+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:54.704+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:54.704+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:54.712+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:54.713+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:12:54.713+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:54.713+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:54.714+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:54.714+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:54.714+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:12:54.714+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:12:54.714+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:12:54.714+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:54.715+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:12:54.715+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:12:54.715+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:12:54.715+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:12:54.723+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:12:54.725+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:12:54.726+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:12:54.726+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:12:54.726+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:54.727+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:12:54.727+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:12:54.727+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:12:54.727+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:12:54.727+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:12:54.728+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:04.576+0000] {spark_submit.py:495} INFO - 24/11/02 08:13:04 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:13:04.580+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:13:04.580+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:13:04.580+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:13:04.581+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:13:04.542Z",
[2024-11-02T08:13:04.581+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:13:04.581+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:04.581+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:04.581+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:04.581+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:13:04.582+0000] {spark_submit.py:495} INFO - "latestOffset" : 28,
[2024-11-02T08:13:04.582+0000] {spark_submit.py:495} INFO - "triggerExecution" : 28
[2024-11-02T08:13:04.582+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:04.582+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:13:04.582+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:13:04.583+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:13:04.583+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:13:04.583+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:04.583+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:04.583+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:04.584+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:04.584+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:13:04.584+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:04.584+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:04.584+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:04.585+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:04.585+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:13:04.585+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:04.585+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:04.585+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:04.586+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:04.586+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:04.586+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:04.586+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:04.586+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:13:04.587+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:13:04.587+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:13:04.587+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:13:04.587+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:04.587+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:13:04.587+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:13:04.588+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:13:04.588+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:13:04.588+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:04.588+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:14.639+0000] {spark_submit.py:495} INFO - 24/11/02 08:13:14 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:13:14.640+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:13:14.640+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:13:14.641+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:13:14.641+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:13:14.573Z",
[2024-11-02T08:13:14.641+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:13:14.641+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:14.641+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:14.641+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:14.642+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:13:14.642+0000] {spark_submit.py:495} INFO - "latestOffset" : 39,
[2024-11-02T08:13:14.642+0000] {spark_submit.py:495} INFO - "triggerExecution" : 39
[2024-11-02T08:13:14.642+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:14.642+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:13:14.643+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:13:14.643+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:13:14.643+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:13:14.643+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:14.643+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:14.643+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:14.644+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:14.644+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:13:14.644+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:14.644+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:14.656+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:14.656+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:14.656+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:13:14.661+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:14.662+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:14.662+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:14.663+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:14.663+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:14.663+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:14.663+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:14.663+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:13:14.663+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:13:14.664+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:13:14.664+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:13:14.664+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:14.664+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:13:14.664+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:13:14.665+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:13:14.665+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:13:14.665+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:14.665+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:24.632+0000] {spark_submit.py:495} INFO - 24/11/02 08:13:24 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:13:24.635+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:13:24.635+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:13:24.635+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:13:24.636+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:13:24.608Z",
[2024-11-02T08:13:24.636+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:13:24.638+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:24.639+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:24.639+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:24.639+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:13:24.639+0000] {spark_submit.py:495} INFO - "latestOffset" : 20,
[2024-11-02T08:13:24.639+0000] {spark_submit.py:495} INFO - "triggerExecution" : 20
[2024-11-02T08:13:24.640+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:24.640+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:13:24.640+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:13:24.640+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:13:24.640+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:13:24.641+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:24.641+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:24.641+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:24.641+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:24.641+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:13:24.642+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:24.642+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:24.642+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:24.642+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:24.642+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:13:24.643+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:24.643+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:24.643+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:24.643+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:24.643+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:24.644+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:24.644+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:24.644+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:13:24.644+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:13:24.644+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:13:24.644+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:13:24.645+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:24.645+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:13:24.645+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:13:24.645+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:13:24.645+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:13:24.646+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:24.646+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:34.691+0000] {spark_submit.py:495} INFO - 24/11/02 08:13:34 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:13:34.692+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:13:34.692+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:13:34.693+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:13:34.693+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:13:34.637Z",
[2024-11-02T08:13:34.693+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:13:34.694+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:34.694+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:34.694+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:34.695+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:13:34.695+0000] {spark_submit.py:495} INFO - "latestOffset" : 49,
[2024-11-02T08:13:34.710+0000] {spark_submit.py:495} INFO - "triggerExecution" : 49
[2024-11-02T08:13:34.711+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:34.711+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:13:34.712+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:13:34.712+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:13:34.712+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:13:34.713+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:34.713+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:34.717+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:34.727+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:34.740+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:13:34.741+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:34.741+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:34.742+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:34.743+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:34.748+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:13:34.748+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:34.748+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:34.749+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:34.749+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:34.749+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:34.749+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:34.800+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:34.801+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:13:34.801+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:13:34.801+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:13:34.801+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:13:34.812+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:34.813+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:13:34.813+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:13:34.813+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:13:34.813+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:13:34.813+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:34.814+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:44.803+0000] {spark_submit.py:495} INFO - 24/11/02 08:13:44 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:13:44.824+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:13:44.825+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:13:44.828+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:13:44.829+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:13:44.540Z",
[2024-11-02T08:13:44.829+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:13:44.830+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:44.831+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:44.831+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:44.832+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:13:44.833+0000] {spark_submit.py:495} INFO - "latestOffset" : 150,
[2024-11-02T08:13:44.833+0000] {spark_submit.py:495} INFO - "triggerExecution" : 150
[2024-11-02T08:13:44.834+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:44.834+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:13:44.835+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:13:44.835+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:13:44.836+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:13:44.836+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:44.837+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:44.837+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:44.838+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:44.838+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:13:44.838+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:44.839+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:44.839+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:44.839+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:44.839+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:13:44.840+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:44.840+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:44.840+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:44.840+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:44.840+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:44.841+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:44.841+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:44.841+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:13:44.842+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:13:44.842+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:13:44.843+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:13:44.843+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:44.895+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:13:44.895+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:13:44.896+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:13:44.896+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:13:44.896+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:44.897+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:55.276+0000] {spark_submit.py:495} INFO - 24/11/02 08:13:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:13:55.276+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:13:55.277+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:13:55.277+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:13:55.277+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:13:54.479Z",
[2024-11-02T08:13:55.308+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:13:55.326+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:55.326+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:55.345+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:55.345+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:13:55.346+0000] {spark_submit.py:495} INFO - "latestOffset" : 779,
[2024-11-02T08:13:55.346+0000] {spark_submit.py:495} INFO - "triggerExecution" : 779
[2024-11-02T08:13:55.346+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:55.346+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:13:55.347+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:13:55.392+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:13:55.393+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:13:55.420+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:55.421+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:55.421+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:55.422+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:55.422+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:13:55.477+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:55.477+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:55.478+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:55.547+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:55.547+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:13:55.548+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:13:55.548+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:13:55.590+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:55.590+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:13:55.591+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:13:55.591+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:13:55.591+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:13:55.592+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:13:55.611+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:13:55.612+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:13:55.612+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:13:55.612+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:55.613+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:13:55.613+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:13:55.613+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:13:55.614+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:13:55.711+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:13:55.823+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:05.275+0000] {spark_submit.py:495} INFO - 24/11/02 08:14:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:14:05.277+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:14:05.278+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:14:05.282+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:14:05.282+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:14:05.192Z",
[2024-11-02T08:14:05.282+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:14:05.283+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:05.283+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:05.283+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:05.283+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:14:05.284+0000] {spark_submit.py:495} INFO - "latestOffset" : 81,
[2024-11-02T08:14:05.288+0000] {spark_submit.py:495} INFO - "triggerExecution" : 81
[2024-11-02T08:14:05.301+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:05.303+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:14:05.303+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:14:05.304+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:14:05.304+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:14:05.304+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:05.304+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:05.304+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:05.304+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:05.305+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:14:05.305+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:05.305+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:05.305+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:05.305+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:05.306+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:14:05.306+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:05.306+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:05.306+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:05.306+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:05.307+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:05.307+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:05.307+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:05.307+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:14:05.307+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:14:05.307+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:14:05.308+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:14:05.308+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:05.308+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:14:05.308+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:14:05.309+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:14:05.309+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:14:05.310+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:05.310+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:15.340+0000] {spark_submit.py:495} INFO - 24/11/02 08:14:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:14:15.349+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:14:15.349+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:14:15.352+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:14:15.354+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:14:15.264Z",
[2024-11-02T08:14:15.355+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:14:15.355+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:15.355+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:15.355+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:15.355+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:14:15.356+0000] {spark_submit.py:495} INFO - "latestOffset" : 72,
[2024-11-02T08:14:15.356+0000] {spark_submit.py:495} INFO - "triggerExecution" : 72
[2024-11-02T08:14:15.356+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:15.356+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:14:15.356+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:14:15.357+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:14:15.357+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:14:15.357+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:15.357+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:15.357+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:15.357+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:15.358+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:14:15.358+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:15.358+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:15.358+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:15.358+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:15.358+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:14:15.359+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:15.359+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:15.359+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:15.359+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:15.359+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:15.360+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:15.360+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:15.360+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:14:15.368+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:14:15.369+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:14:15.369+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:14:15.369+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:15.370+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:14:15.370+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:14:15.379+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:14:15.379+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:14:15.379+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:15.379+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:25.360+0000] {spark_submit.py:495} INFO - 24/11/02 08:14:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:14:25.362+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:14:25.362+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:14:25.363+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:14:25.363+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:14:25.347Z",
[2024-11-02T08:14:25.363+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:14:25.364+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:25.364+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:25.364+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:25.364+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:14:25.364+0000] {spark_submit.py:495} INFO - "latestOffset" : 9,
[2024-11-02T08:14:25.364+0000] {spark_submit.py:495} INFO - "triggerExecution" : 9
[2024-11-02T08:14:25.365+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:25.365+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:14:25.365+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:14:25.366+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:14:25.366+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:14:25.366+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:25.366+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:25.366+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:25.366+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:25.367+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:14:25.367+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:25.367+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:25.367+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:25.367+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:25.368+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:14:25.368+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:25.368+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:25.369+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:25.369+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:25.370+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:25.370+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:25.370+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:25.371+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:14:25.371+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:14:25.371+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:14:25.371+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:14:25.371+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:25.372+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:14:25.372+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:14:25.372+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:14:25.372+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:14:25.372+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:25.373+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:35.632+0000] {spark_submit.py:495} INFO - 24/11/02 08:14:35 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:14:35.652+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:14:35.669+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:14:35.669+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:14:35.670+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:14:35.021Z",
[2024-11-02T08:14:35.670+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:14:35.670+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:35.670+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:35.671+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:35.671+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:14:35.671+0000] {spark_submit.py:495} INFO - "latestOffset" : 609,
[2024-11-02T08:14:35.671+0000] {spark_submit.py:495} INFO - "triggerExecution" : 609
[2024-11-02T08:14:35.672+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:35.672+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:14:35.672+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:14:35.672+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:14:35.673+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:14:35.673+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:35.719+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:35.720+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:35.720+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:35.720+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:14:35.720+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:35.721+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:35.721+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:35.721+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:35.721+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:14:35.721+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:35.722+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:35.722+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:35.722+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:35.722+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:35.722+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:35.723+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:35.723+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:14:35.723+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:14:35.723+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:14:35.723+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:14:35.724+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:35.724+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:14:35.724+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:14:35.724+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:14:35.725+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:14:35.725+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:35.725+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:45.635+0000] {spark_submit.py:495} INFO - 24/11/02 08:14:45 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:14:45.642+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:14:45.643+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:14:45.643+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:14:45.644+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:14:45.614Z",
[2024-11-02T08:14:45.644+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:14:45.644+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:45.645+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:45.645+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:45.645+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:14:45.645+0000] {spark_submit.py:495} INFO - "latestOffset" : 18,
[2024-11-02T08:14:45.645+0000] {spark_submit.py:495} INFO - "triggerExecution" : 18
[2024-11-02T08:14:45.646+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:45.646+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:14:45.646+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:14:45.646+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:14:45.646+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:14:45.647+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:45.647+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:45.647+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:45.647+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:45.647+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:14:45.648+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:45.648+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:45.648+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:45.648+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:45.648+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:14:45.649+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:45.649+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:45.649+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:45.649+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:45.649+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:45.650+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:45.650+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:45.650+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:14:45.650+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:14:45.650+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:14:45.651+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:14:45.651+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:45.651+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:14:45.651+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:14:45.651+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:14:45.652+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:14:45.652+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:45.652+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:55.725+0000] {spark_submit.py:495} INFO - 24/11/02 08:14:55 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:14:55.726+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:14:55.726+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:14:55.727+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:14:55.727+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:14:55.541Z",
[2024-11-02T08:14:55.727+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:14:55.727+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:55.727+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:55.728+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:55.733+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:14:55.733+0000] {spark_submit.py:495} INFO - "latestOffset" : 176,
[2024-11-02T08:14:55.733+0000] {spark_submit.py:495} INFO - "triggerExecution" : 176
[2024-11-02T08:14:55.733+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:55.734+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:14:55.734+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:14:55.734+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:14:55.734+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:14:55.734+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:55.734+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:55.743+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:55.743+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:55.744+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:14:55.744+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:55.744+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:55.744+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:55.744+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:55.744+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:14:55.747+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:14:55.747+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:14:55.747+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:55.747+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:14:55.747+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:14:55.752+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:14:55.752+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:14:55.753+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:14:55.753+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:14:55.753+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:14:55.762+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:14:55.765+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:55.766+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:14:55.768+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:14:55.768+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:14:55.768+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:14:55.769+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:14:55.769+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:05.726+0000] {spark_submit.py:495} INFO - 24/11/02 08:15:05 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:15:05.727+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:15:05.727+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:15:05.727+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:15:05.728+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:15:05.700Z",
[2024-11-02T08:15:05.728+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:15:05.728+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:05.729+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:05.729+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:05.739+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:15:05.739+0000] {spark_submit.py:495} INFO - "latestOffset" : 21,
[2024-11-02T08:15:05.740+0000] {spark_submit.py:495} INFO - "triggerExecution" : 21
[2024-11-02T08:15:05.755+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:05.756+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:15:05.756+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:15:05.756+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:15:05.757+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:15:05.757+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:05.757+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:05.757+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:05.757+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:05.758+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:15:05.758+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:05.758+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:05.758+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:05.759+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:05.759+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:15:05.759+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:05.759+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:05.759+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:05.760+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:05.760+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:05.760+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:05.760+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:05.760+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:15:05.760+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:15:05.761+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:15:05.761+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:15:05.761+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:05.761+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:15:05.761+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:15:05.762+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:15:05.762+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:15:05.762+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:05.762+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:15.805+0000] {spark_submit.py:495} INFO - 24/11/02 08:15:15 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:15:15.812+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:15:15.812+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:15:15.813+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:15:15.832+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:15:15.629Z",
[2024-11-02T08:15:15.841+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:15:15.846+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:15.865+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:15.867+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:15.868+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:15:15.868+0000] {spark_submit.py:495} INFO - "latestOffset" : 166,
[2024-11-02T08:15:15.868+0000] {spark_submit.py:495} INFO - "triggerExecution" : 166
[2024-11-02T08:15:15.877+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:15.882+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:15:15.888+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:15:15.890+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:15:15.891+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:15:15.893+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:15.894+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:15.895+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:15.895+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:15.895+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:15:15.895+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:15.896+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:15.896+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:15.896+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:15.896+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:15:15.897+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:15.897+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:15.897+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:15.897+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:15.898+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:15.898+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:15.905+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:15.905+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:15:15.908+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:15:15.908+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:15:15.908+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:15:15.909+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:15.911+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:15:15.911+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:15:15.913+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:15:15.914+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:15:15.915+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:15.915+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:25.899+0000] {spark_submit.py:495} INFO - 24/11/02 08:15:25 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:15:25.995+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:15:25.996+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:15:25.996+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:15:26.042+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:15:25.543Z",
[2024-11-02T08:15:26.042+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:15:26.043+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:26.043+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:26.043+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:26.043+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:15:26.044+0000] {spark_submit.py:495} INFO - "latestOffset" : 256,
[2024-11-02T08:15:26.044+0000] {spark_submit.py:495} INFO - "triggerExecution" : 256
[2024-11-02T08:15:26.044+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:26.044+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:15:26.045+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:15:26.045+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:15:26.045+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:15:26.045+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:26.045+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:26.083+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:26.086+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:26.086+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:15:26.087+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:26.087+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:26.087+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:26.087+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:26.087+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:15:26.088+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:26.088+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:26.088+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:26.088+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:26.089+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:26.089+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:26.089+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:26.108+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:15:26.113+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:15:26.122+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:15:26.123+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:15:26.125+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:26.130+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:15:26.130+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:15:26.131+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:15:26.131+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:15:26.131+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:26.132+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:36.119+0000] {spark_submit.py:495} INFO - 24/11/02 08:15:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:15:36.134+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:15:36.135+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:15:36.135+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:15:36.136+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:15:35.787Z",
[2024-11-02T08:15:36.136+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:15:36.143+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:36.143+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:36.144+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:36.144+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:15:36.144+0000] {spark_submit.py:495} INFO - "latestOffset" : 318,
[2024-11-02T08:15:36.144+0000] {spark_submit.py:495} INFO - "triggerExecution" : 318
[2024-11-02T08:15:36.145+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:36.145+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:15:36.145+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:15:36.145+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:15:36.146+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:15:36.146+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:36.146+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:36.146+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:36.146+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:36.147+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:15:36.174+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:36.174+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:36.174+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:36.175+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:36.175+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:15:36.175+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:36.175+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:36.175+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:36.176+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:36.176+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:36.176+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:36.176+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:36.177+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:15:36.193+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:15:36.193+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:15:36.193+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:15:36.243+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:36.243+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:15:36.265+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:15:36.308+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:15:36.309+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:15:36.309+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:36.310+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:46.149+0000] {spark_submit.py:495} INFO - 24/11/02 08:15:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:15:46.150+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:15:46.150+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:15:46.151+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:15:46.151+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:15:46.119Z",
[2024-11-02T08:15:46.151+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:15:46.151+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:46.151+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:46.152+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:46.152+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:15:46.152+0000] {spark_submit.py:495} INFO - "latestOffset" : 22,
[2024-11-02T08:15:46.152+0000] {spark_submit.py:495} INFO - "triggerExecution" : 23
[2024-11-02T08:15:46.152+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:46.152+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:15:46.153+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:15:46.153+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:15:46.153+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:15:46.153+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:46.153+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:46.154+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:46.154+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:46.154+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:15:46.154+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:46.154+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:46.154+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:46.155+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:46.155+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:15:46.155+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:46.155+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:46.155+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:46.155+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:46.156+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:46.156+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:46.156+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:46.156+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:15:46.156+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:15:46.157+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:15:46.157+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:15:46.157+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:46.157+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:15:46.157+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:15:46.158+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:15:46.158+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:15:46.158+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:46.158+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:56.445+0000] {spark_submit.py:495} INFO - 24/11/02 08:15:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:15:56.469+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:15:56.471+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:15:56.486+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:15:56.486+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:15:56.172Z",
[2024-11-02T08:15:56.486+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:15:56.487+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:56.487+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:56.487+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:56.487+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:15:56.498+0000] {spark_submit.py:495} INFO - "latestOffset" : 240,
[2024-11-02T08:15:56.503+0000] {spark_submit.py:495} INFO - "triggerExecution" : 240
[2024-11-02T08:15:56.515+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:56.515+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:15:56.516+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:15:56.516+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:15:56.516+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:15:56.516+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:56.516+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:56.517+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:56.517+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:56.517+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:15:56.517+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:56.517+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:56.517+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:56.518+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:56.518+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:15:56.518+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:15:56.518+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:15:56.518+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:56.519+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:15:56.519+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:15:56.519+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:15:56.519+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:15:56.519+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:15:56.520+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:15:56.520+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:15:56.520+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:15:56.520+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:56.520+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:15:56.548+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:15:56.549+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:15:56.549+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:15:56.549+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:15:56.550+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:06.447+0000] {spark_submit.py:495} INFO - 24/11/02 08:16:06 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:16:06.451+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:16:06.452+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:16:06.452+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:16:06.453+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:16:06.402Z",
[2024-11-02T08:16:06.453+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:16:06.453+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:06.453+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:06.453+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:06.454+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:16:06.454+0000] {spark_submit.py:495} INFO - "latestOffset" : 42,
[2024-11-02T08:16:06.465+0000] {spark_submit.py:495} INFO - "triggerExecution" : 43
[2024-11-02T08:16:06.469+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:06.470+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:16:06.482+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:16:06.486+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:16:06.486+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:16:06.486+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:06.486+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:06.487+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:06.487+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:06.487+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:16:06.488+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:06.499+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:06.500+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:06.507+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:06.507+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:16:06.507+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:06.508+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:06.508+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:06.508+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:06.508+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:06.509+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:06.509+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:06.509+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:16:06.509+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:16:06.509+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:16:06.510+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:16:06.510+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:06.510+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:16:06.510+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:16:06.511+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:16:06.511+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:16:06.511+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:06.525+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:16.466+0000] {spark_submit.py:495} INFO - 24/11/02 08:16:16 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:16:16.467+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:16:16.467+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:16:16.467+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:16:16.468+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:16:16.434Z",
[2024-11-02T08:16:16.468+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:16:16.468+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:16.468+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:16.469+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:16.469+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:16:16.469+0000] {spark_submit.py:495} INFO - "latestOffset" : 31,
[2024-11-02T08:16:16.469+0000] {spark_submit.py:495} INFO - "triggerExecution" : 31
[2024-11-02T08:16:16.469+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:16.470+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:16:16.470+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:16:16.470+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:16:16.470+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:16:16.470+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:16.471+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:16.471+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:16.471+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:16.471+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:16:16.471+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:16.472+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:16.472+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:16.472+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:16.472+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:16:16.472+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:16.473+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:16.473+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:16.473+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:16.473+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:16.478+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:16.480+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:16.480+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:16:16.480+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:16:16.480+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:16:16.481+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:16:16.481+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:16.481+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:16:16.481+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:16:16.481+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:16:16.482+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:16:16.482+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:16.482+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:26.508+0000] {spark_submit.py:495} INFO - 24/11/02 08:16:26 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:16:26.509+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:16:26.510+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:16:26.510+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:16:26.510+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:16:26.471Z",
[2024-11-02T08:16:26.510+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:16:26.510+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:26.511+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:26.511+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:26.511+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:16:26.511+0000] {spark_submit.py:495} INFO - "latestOffset" : 34,
[2024-11-02T08:16:26.512+0000] {spark_submit.py:495} INFO - "triggerExecution" : 34
[2024-11-02T08:16:26.512+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:26.512+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:16:26.512+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:16:26.512+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:16:26.513+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:16:26.513+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:26.513+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:26.513+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:26.513+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:26.514+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:16:26.514+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:26.514+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:26.514+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:26.514+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:26.515+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:16:26.515+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:26.515+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:26.515+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:26.521+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:26.523+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:26.523+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:26.523+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:26.523+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:16:26.523+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:16:26.523+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:16:26.524+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:26.525+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:36.605+0000] {spark_submit.py:495} INFO - 24/11/02 08:16:36 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:16:36.615+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:16:36.616+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:16:36.617+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:16:36.618+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:16:36.553Z",
[2024-11-02T08:16:36.618+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:16:36.618+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:36.619+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:36.619+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:36.619+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:16:36.619+0000] {spark_submit.py:495} INFO - "latestOffset" : 46,
[2024-11-02T08:16:36.620+0000] {spark_submit.py:495} INFO - "triggerExecution" : 46
[2024-11-02T08:16:36.620+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:36.620+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:16:36.621+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:16:36.627+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:16:36.628+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:16:36.629+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:36.629+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:36.629+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:36.629+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:36.630+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:16:36.630+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:36.630+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:36.630+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:36.630+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:36.631+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:16:36.631+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:36.631+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:36.631+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:36.631+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:36.632+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:36.632+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:36.632+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:36.632+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:16:36.633+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:16:36.633+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:16:36.633+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:16:36.633+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:36.633+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:16:36.633+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:16:36.634+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:16:36.634+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:16:36.634+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:36.634+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:46.681+0000] {spark_submit.py:495} INFO - 24/11/02 08:16:46 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:16:46.682+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:16:46.683+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:16:46.683+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:16:46.684+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:16:46.608Z",
[2024-11-02T08:16:46.684+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:16:46.685+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:46.685+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:46.686+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:46.686+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:16:46.686+0000] {spark_submit.py:495} INFO - "latestOffset" : 55,
[2024-11-02T08:16:46.687+0000] {spark_submit.py:495} INFO - "triggerExecution" : 56
[2024-11-02T08:16:46.687+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:46.688+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:16:46.688+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:16:46.689+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:16:46.689+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:16:46.689+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:46.690+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:46.690+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:46.690+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:46.691+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:16:46.691+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:46.691+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:46.692+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:46.692+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:46.692+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:16:46.693+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:46.693+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:46.693+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:46.694+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:46.694+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:46.694+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:46.695+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:46.695+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:16:46.695+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:16:46.695+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:16:46.695+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:16:46.696+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:46.696+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:16:46.696+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:16:46.697+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:16:46.697+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:16:46.697+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:46.698+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:56.705+0000] {spark_submit.py:495} INFO - 24/11/02 08:16:56 INFO MicroBatchExecution: Streaming query made progress: {
[2024-11-02T08:16:56.706+0000] {spark_submit.py:495} INFO - "id" : "1fd99432-c7e5-4ad2-abdf-c2441802f585",
[2024-11-02T08:16:56.709+0000] {spark_submit.py:495} INFO - "runId" : "79fe2973-457e-4920-95f8-9a2fd4db54de",
[2024-11-02T08:16:56.709+0000] {spark_submit.py:495} INFO - "name" : null,
[2024-11-02T08:16:56.709+0000] {spark_submit.py:495} INFO - "timestamp" : "2024-11-02T08:16:56.656Z",
[2024-11-02T08:16:56.710+0000] {spark_submit.py:495} INFO - "batchId" : 28,
[2024-11-02T08:16:56.710+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:56.710+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:56.711+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:56.711+0000] {spark_submit.py:495} INFO - "durationMs" : {
[2024-11-02T08:16:56.711+0000] {spark_submit.py:495} INFO - "latestOffset" : 45,
[2024-11-02T08:16:56.711+0000] {spark_submit.py:495} INFO - "triggerExecution" : 45
[2024-11-02T08:16:56.712+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:56.712+0000] {spark_submit.py:495} INFO - "stateOperators" : [ ],
[2024-11-02T08:16:56.712+0000] {spark_submit.py:495} INFO - "sources" : [ {
[2024-11-02T08:16:56.712+0000] {spark_submit.py:495} INFO - "description" : "KafkaV2[Subscribe[raw_data]]",
[2024-11-02T08:16:56.712+0000] {spark_submit.py:495} INFO - "startOffset" : {
[2024-11-02T08:16:56.713+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:56.713+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:56.713+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:56.713+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:56.714+0000] {spark_submit.py:495} INFO - "endOffset" : {
[2024-11-02T08:16:56.714+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:56.714+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:56.714+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:56.715+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:56.715+0000] {spark_submit.py:495} INFO - "latestOffset" : {
[2024-11-02T08:16:56.715+0000] {spark_submit.py:495} INFO - "raw_data" : {
[2024-11-02T08:16:56.715+0000] {spark_submit.py:495} INFO - "0" : 6232
[2024-11-02T08:16:56.716+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:56.716+0000] {spark_submit.py:495} INFO - },
[2024-11-02T08:16:56.716+0000] {spark_submit.py:495} INFO - "numInputRows" : 0,
[2024-11-02T08:16:56.716+0000] {spark_submit.py:495} INFO - "inputRowsPerSecond" : 0.0,
[2024-11-02T08:16:56.717+0000] {spark_submit.py:495} INFO - "processedRowsPerSecond" : 0.0,
[2024-11-02T08:16:56.717+0000] {spark_submit.py:495} INFO - "metrics" : {
[2024-11-02T08:16:56.717+0000] {spark_submit.py:495} INFO - "avgOffsetsBehindLatest" : "0.0",
[2024-11-02T08:16:56.717+0000] {spark_submit.py:495} INFO - "maxOffsetsBehindLatest" : "0",
[2024-11-02T08:16:56.717+0000] {spark_submit.py:495} INFO - "minOffsetsBehindLatest" : "0"
[2024-11-02T08:16:56.718+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:56.718+0000] {spark_submit.py:495} INFO - } ],
[2024-11-02T08:16:56.718+0000] {spark_submit.py:495} INFO - "sink" : {
[2024-11-02T08:16:56.718+0000] {spark_submit.py:495} INFO - "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2a5fba2a",
[2024-11-02T08:16:56.719+0000] {spark_submit.py:495} INFO - "numOutputRows" : 0
[2024-11-02T08:16:56.719+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:16:56.719+0000] {spark_submit.py:495} INFO - }
[2024-11-02T08:17:05.719+0000] {local_task_job_runner.py:313} WARNING - State of this instance has been externally set to success. Terminating instance.
[2024-11-02T08:17:05.748+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-11-02T08:17:05.884+0000] {process_utils.py:132} INFO - Sending 15 to group 2249. PIDs of all processes in the group: [2250, 2256, 2334, 2249]
[2024-11-02T08:17:05.898+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 2249
[2024-11-02T08:17:05.902+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-11-02T08:17:05.912+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2024-11-02T08:17:05.927+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-11-02T08:17:06.425+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=2256, status='terminated', started='07:49:23') (2256) terminated with exit code None
[2024-11-02T08:17:06.517+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=2249, status='terminated', exitcode=0, started='07:49:21') (2249) terminated with exit code 0
[2024-11-02T08:17:06.518+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=2334, status='terminated', started='07:50:28') (2334) terminated with exit code None
[2024-11-02T08:17:06.537+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=2250, status='terminated', started='07:49:22') (2250) terminated with exit code None
