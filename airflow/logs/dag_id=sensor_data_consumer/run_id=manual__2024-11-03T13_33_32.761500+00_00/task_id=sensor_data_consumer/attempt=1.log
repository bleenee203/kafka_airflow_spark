[2024-11-03T13:34:01.667+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-11-03T13:34:02.127+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-03T13:33:32.761500+00:00 [queued]>
[2024-11-03T13:34:02.222+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-03T13:33:32.761500+00:00 [queued]>
[2024-11-03T13:34:02.224+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-11-03T13:34:02.323+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): sensor_data_consumer> on 2024-11-03 13:33:32.761500+00:00
[2024-11-03T13:34:02.346+0000] {standard_task_runner.py:64} INFO - Started process 4324 to run task
[2024-11-03T13:34:02.385+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sensor_data_consumer', 'sensor_data_consumer', 'manual__2024-11-03T13:33:32.761500+00:00', '--job-id', '420', '--raw', '--subdir', 'DAGS_FOLDER/***_consumer.py', '--cfg-path', '/tmp/tmpncgj7guv']
[2024-11-03T13:34:02.412+0000] {standard_task_runner.py:91} INFO - Job 420: Subtask sensor_data_consumer
[2024-11-03T13:34:02.795+0000] {task_command.py:426} INFO - Running <TaskInstance: sensor_data_consumer.sensor_data_consumer manual__2024-11-03T13:33:32.761500+00:00 [running]> on host 7d9160448bdc
[2024-11-03T13:34:03.632+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Bich Ly' AIRFLOW_CTX_DAG_ID='sensor_data_consumer' AIRFLOW_CTX_TASK_ID='sensor_data_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-11-03T13:33:32.761500+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-03T13:33:32.761500+00:00'
[2024-11-03T13:34:03.637+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-11-03T13:34:03.787+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2024-11-03T13:34:03.803+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --conf spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop-3.2.1/etc/hadoop --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.2 --name KafkaSparkHDFS /opt/***/dags/spark_streaming_job.py
[2024-11-03T13:34:42.612+0000] {spark_submit.py:495} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-03T13:34:43.449+0000] {spark_submit.py:495} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-03T13:34:43.518+0000] {spark_submit.py:495} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-03T13:34:43.538+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency
[2024-11-03T13:34:43.556+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-11-03T13:34:43.557+0000] {spark_submit.py:495} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-e7e9bbe3-be8f-4451-bddf-70d8875d3b2a;1.0
[2024-11-03T13:34:43.557+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-03T13:34:45.608+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 in central
[2024-11-03T13:34:46.625+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 in central
[2024-11-03T13:34:47.017+0000] {spark_submit.py:495} INFO - found org.apache.kafka#kafka-clients;3.3.2 in central
[2024-11-03T13:34:47.399+0000] {spark_submit.py:495} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-11-03T13:34:47.775+0000] {spark_submit.py:495} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-11-03T13:34:47.903+0000] {spark_submit.py:495} INFO - found org.slf4j#slf4j-api;2.0.6 in central
[2024-11-03T13:34:48.346+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-11-03T13:34:48.569+0000] {spark_submit.py:495} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-11-03T13:34:48.882+0000] {spark_submit.py:495} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-11-03T13:34:49.015+0000] {spark_submit.py:495} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-11-03T13:34:49.126+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 in central
[2024-11-03T13:34:49.391+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-11-03T13:34:49.896+0000] {spark_submit.py:495} INFO - :: resolution report :: resolve 6014ms :: artifacts dl 398ms
[2024-11-03T13:34:49.897+0000] {spark_submit.py:495} INFO - :: modules in use:
[2024-11-03T13:34:49.897+0000] {spark_submit.py:495} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2024-11-03T13:34:49.898+0000] {spark_submit.py:495} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2024-11-03T13:34:49.898+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2024-11-03T13:34:49.898+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2024-11-03T13:34:49.899+0000] {spark_submit.py:495} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-11-03T13:34:49.899+0000] {spark_submit.py:495} INFO - org.apache.kafka#kafka-clients;3.3.2 from central in [default]
[2024-11-03T13:34:49.899+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-03T13:34:49.993+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-streaming-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-03T13:34:49.993+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.2 from central in [default]
[2024-11-03T13:34:49.994+0000] {spark_submit.py:495} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
[2024-11-03T13:34:49.994+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;2.0.6 from central in [default]
[2024-11-03T13:34:49.995+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-11-03T13:34:49.995+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-03T13:34:49.996+0000] {spark_submit.py:495} INFO - |                  |            modules            ||   artifacts   |
[2024-11-03T13:34:49.996+0000] {spark_submit.py:495} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-03T13:34:49.997+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-03T13:34:49.997+0000] {spark_submit.py:495} INFO - |      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
[2024-11-03T13:34:49.997+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2024-11-03T13:34:50.093+0000] {spark_submit.py:495} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-e7e9bbe3-be8f-4451-bddf-70d8875d3b2a
[2024-11-03T13:34:50.094+0000] {spark_submit.py:495} INFO - confs: [default]
[2024-11-03T13:34:50.299+0000] {spark_submit.py:495} INFO - 0 artifacts copied, 12 already retrieved (0kB/181ms)
[2024-11-03T13:34:55.102+0000] {spark_submit.py:495} INFO - 24/11/03 13:34:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-03T13:35:14.463+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:14 INFO SparkContext: Running Spark version 3.4.2
[2024-11-03T13:35:14.925+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:14 INFO ResourceUtils: ==============================================================
[2024-11-03T13:35:14.926+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:14 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-03T13:35:14.926+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:14 INFO ResourceUtils: ==============================================================
[2024-11-03T13:35:14.926+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:14 INFO SparkContext: Submitted application: KafkaSparkStreaming
[2024-11-03T13:35:15.954+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-03T13:35:16.016+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:16 INFO ResourceProfile: Limiting resource is cpu
[2024-11-03T13:35:16.027+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-03T13:35:17.078+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:16 INFO SecurityManager: Changing view acls to: ***
[2024-11-03T13:35:17.087+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:17 INFO SecurityManager: Changing modify acls to: ***
[2024-11-03T13:35:17.088+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:17 INFO SecurityManager: Changing view acls groups to:
[2024-11-03T13:35:17.089+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:17 INFO SecurityManager: Changing modify acls groups to:
[2024-11-03T13:35:17.089+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-03T13:35:22.431+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:22 INFO Utils: Successfully started service 'sparkDriver' on port 39311.
[2024-11-03T13:35:24.934+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:24 INFO SparkEnv: Registering MapOutputTracker
[2024-11-03T13:35:25.947+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:25 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-03T13:35:26.699+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-03T13:35:26.711+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-03T13:35:26.852+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-03T13:35:27.987+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3d5d4fa4-5f98-4ffb-a53c-d88211241b74
[2024-11-03T13:35:28.772+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-03T13:35:29.304+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-03T13:35:33.371+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:33 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-03T13:35:35.061+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-03T13:35:38.263+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:38 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-03T13:35:40.805+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:40 INFO TransportClientFactory: Successfully created connection to spark-master/172.20.0.4:7077 after 1809 ms (0 ms spent in bootstraps)
[2024-11-03T13:35:44.009+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:43 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241103133542-0003
[2024-11-03T13:35:44.152+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241103133542-0003/0 on worker-20241103112459-172.20.0.7-41257 (172.20.0.7:41257) with 1 core(s)
[2024-11-03T13:35:44.154+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20241103133542-0003/0 on hostPort 172.20.0.7:41257 with 1 core(s), 1024.0 MiB RAM
[2024-11-03T13:35:44.370+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46013.
[2024-11-03T13:35:44.477+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:44 INFO NettyBlockTransferService: Server created on 7d9160448bdc:46013
[2024-11-03T13:35:44.618+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-03T13:35:45.429+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7d9160448bdc, 46013, None)
[2024-11-03T13:35:45.894+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:45 INFO BlockManagerMasterEndpoint: Registering block manager 7d9160448bdc:46013 with 434.4 MiB RAM, BlockManagerId(driver, 7d9160448bdc, 46013, None)
[2024-11-03T13:35:46.059+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7d9160448bdc, 46013, None)
[2024-11-03T13:35:46.158+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7d9160448bdc, 46013, None)
[2024-11-03T13:35:46.288+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241103133542-0003/0 is now RUNNING
[2024-11-03T13:35:51.244+0000] {spark_submit.py:495} INFO - 24/11/03 13:35:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-03T13:36:02.704+0000] {spark_submit.py:495} INFO - 24/11/03 13:36:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-03T13:36:02.884+0000] {spark_submit.py:495} INFO - 24/11/03 13:36:02 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-03T13:38:21.225+0000] {spark_submit.py:495} INFO - 24/11/03 13:38:21 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.7:44078) with ID 0,  ResourceProfileId 0
[2024-11-03T13:38:26.806+0000] {spark_submit.py:495} INFO - 24/11/03 13:38:26 INFO AsyncEventQueue: Process of event SparkListenerExecutorAdded(1730641102163,0,org.apache.spark.scheduler.cluster.ExecutorData@5ea06a05) by listener AppStatusListener took 1.326889123s.
[2024-11-03T13:38:39.186+0000] {spark_submit.py:495} INFO - 24/11/03 13:38:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.7:32775 with 434.4 MiB RAM, BlockManagerId(0, 172.20.0.7, 32775, None)
[2024-11-03T13:40:44.061+0000] {spark_submit.py:495} INFO - done
[2024-11-03T13:40:52.097+0000] {spark_submit.py:495} INFO - root
[2024-11-03T13:40:52.656+0000] {spark_submit.py:495} INFO - |-- sensor_id: integer (nullable = true)
[2024-11-03T13:40:52.657+0000] {spark_submit.py:495} INFO - |-- temperature: float (nullable = true)
[2024-11-03T13:40:52.657+0000] {spark_submit.py:495} INFO - |-- humidity: float (nullable = true)
[2024-11-03T13:40:52.657+0000] {spark_submit.py:495} INFO - |-- timestamp: string (nullable = true)
[2024-11-03T13:40:52.657+0000] {spark_submit.py:495} INFO - 
[2024-11-03T13:41:00.732+0000] {spark_submit.py:495} INFO - 24/11/03 13:41:00 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-11-03T13:41:02.104+0000] {spark_submit.py:495} INFO - 24/11/03 13:41:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-af0538a1-75dc-47ba-80d7-97a7a5ecfb1e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-11-03T13:41:05.014+0000] {spark_submit.py:495} INFO - 24/11/03 13:41:04 INFO ResolveWriteToStream: Checkpoint root /tmp/temporary-af0538a1-75dc-47ba-80d7-97a7a5ecfb1e resolved to file:/tmp/temporary-af0538a1-75dc-47ba-80d7-97a7a5ecfb1e.
[2024-11-03T13:41:05.021+0000] {spark_submit.py:495} INFO - 24/11/03 13:41:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               